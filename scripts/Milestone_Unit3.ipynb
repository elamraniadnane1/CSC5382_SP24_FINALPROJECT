{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution - (c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -lflow (c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -lflow (c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (4.38.2)\n",
      "Requirement already satisfied: torch in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (2.2.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (1.5.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (1.2.2)\n",
      "Requirement already satisfied: mlflow in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (2.11.3)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (3.8.3)\n",
      "Requirement already satisfied: seaborn in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: wordcloud in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (1.9.3)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (2.15.0)\n",
      "Requirement already satisfied: tensorflow-data-validation in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (1.14.0)\n",
      "Requirement already satisfied: zenml in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (0.56.2)\n",
      "Requirement already satisfied: tweepy in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (4.14.0)\n",
      "Requirement already satisfied: cassandra-driver in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (3.29.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (0.21.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch) (4.10.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch) (2024.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas) (2022.7.1)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scikit-learn) (1.12.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scikit-learn) (3.3.0)\n",
      "Requirement already satisfied: click<9,>=7.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from mlflow) (8.1.3)\n",
      "Requirement already satisfied: cloudpickle<4 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from mlflow) (2.2.1)\n",
      "Requirement already satisfied: entrypoints<1 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from mlflow) (0.4)\n",
      "Requirement already satisfied: gitpython<4,>=3.1.9 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from mlflow) (3.1.42)\n",
      "Requirement already satisfied: protobuf<5,>=3.12.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from mlflow) (3.20.3)\n",
      "Requirement already satisfied: importlib-metadata!=4.7.0,<8,>=3.7.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from mlflow) (6.11.0)\n",
      "Requirement already satisfied: sqlparse<1,>=0.4.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from mlflow) (0.4.4)\n",
      "Requirement already satisfied: alembic!=1.10.0,<2 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from mlflow) (1.8.1)\n",
      "Requirement already satisfied: docker<8,>=4.0.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from mlflow) (6.1.3)\n",
      "Requirement already satisfied: Flask<4 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from mlflow) (2.3.3)\n",
      "Requirement already satisfied: querystring-parser<2 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from mlflow) (1.2.4)\n",
      "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from mlflow) (1.4.41)\n",
      "Requirement already satisfied: pyarrow<16,>=4.0.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from mlflow) (10.0.1)\n",
      "Requirement already satisfied: markdown<4,>=3.3 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from mlflow) (3.5.2)\n",
      "Requirement already satisfied: graphene<4 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from mlflow) (3.3)\n",
      "Requirement already satisfied: waitress<4 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from mlflow) (2.1.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib) (4.49.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib) (5.13.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.15.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (2.15.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (24.3.7)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (3.10.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.2.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (49.2.1)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.62.1)\n",
      "Requirement already satisfied: tensorboard<2.16,>=2.15 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.15.2)\n",
      "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.15.0)\n",
      "Requirement already satisfied: keras<2.16,>=2.15.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.15.0)\n",
      "Requirement already satisfied: apache-beam<3,>=2.47 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (2.54.0)\n",
      "Requirement already satisfied: pyfarmhash<0.4,>=0.2.2 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-data-validation) (0.3.2)\n",
      "Requirement already satisfied: tensorflow-metadata<1.15,>=1.14.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-data-validation) (1.14.0)\n",
      "Requirement already satisfied: tfx-bsl<1.15,>=1.14.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-data-validation) (1.14.0)\n",
      "Requirement already satisfied: azure-mgmt-resource>=21.0.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from zenml) (23.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers torch pandas scikit-learn mlflow matplotlib seaborn wordcloud tensorflow tensorflow-data-validation zenml tweepy cassandra-driver\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bcrypt==4.0.1 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from zenml) (4.0.1)\n",
      "Requirement already satisfied: click-params<0.4.0,>=0.3.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from zenml) (0.3.0)\n",
      "Requirement already satisfied: distro<2.0.0,>=1.6.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from zenml) (1.9.0)\n",
      "Requirement already satisfied: httplib2<0.20,>=0.19.1 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from zenml) (0.19.1)\n",
      "Requirement already satisfied: passlib<1.8.0,>=1.7.4 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from passlib[bcrypt]<1.8.0,>=1.7.4->zenml) (1.7.4)\n",
      "Requirement already satisfied: psutil>=5.0.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from zenml) (5.9.8)\n",
      "Requirement already satisfied: pydantic<1.11,>=1.9.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from zenml) (1.10.14)\n",
      "Requirement already satisfied: pymysql<1.1.0,>=1.0.2 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from zenml) (1.0.3)\n",
      "Requirement already satisfied: rich>=12.0.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from rich[jupyter]>=12.0.0->zenml) (13.7.1)\n",
      "Requirement already satisfied: sqlalchemy_utils==0.38.3 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from zenml) (0.38.3)\n",
      "Requirement already satisfied: sqlmodel==0.0.8 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from zenml) (0.0.8)\n",
      "Requirement already satisfied: sqlalchemy2-stubs in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sqlmodel==0.0.8->zenml) (0.0.2a38)\n",
      "Requirement already satisfied: oauthlib<4,>=3.2.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tweepy) (3.2.2)\n",
      "Requirement already satisfied: requests-oauthlib<2,>=1.2.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tweepy) (1.4.0)\n",
      "Requirement already satisfied: geomet<0.3,>=0.1 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from cassandra-driver) (0.2.1.post1)\n",
      "Requirement already satisfied: Mako in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from alembic!=1.10.0,<2->mlflow) (1.3.2)\n",
      "Requirement already satisfied: crcmod<2.0,>=1.7 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (1.7)\n",
      "Requirement already satisfied: orjson<4,>=3.9.7 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (3.10.0)\n",
      "Requirement already satisfied: dill<0.3.2,>=0.3.1.1 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (0.3.1.1)\n",
      "Requirement already satisfied: fastavro<2,>=0.23.6 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (1.9.4)\n",
      "Requirement already satisfied: fasteners<1.0,>=0.3 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (0.19)\n",
      "Requirement already satisfied: hdfs<3.0.0,>=2.1.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (2.7.3)\n",
      "Requirement already satisfied: js2py<1,>=0.74 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (0.74)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.0.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (4.21.1)\n",
      "Requirement already satisfied: jsonpickle<4.0.0,>=3.0.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (3.0.3)\n",
      "Requirement already satisfied: objsize<0.8.0,>=0.6.1 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (0.7.0)\n",
      "Requirement already satisfied: pymongo<5.0.0,>=3.8.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (4.6.2)\n",
      "Requirement already satisfied: proto-plus<2,>=1.7.1 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (1.23.0)\n",
      "Requirement already satisfied: pydot<2,>=1.2.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (1.4.2)\n",
      "Requirement already satisfied: zstandard<1,>=0.18.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (0.22.0)\n",
      "Requirement already satisfied: pyarrow-hotfix<1 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (0.6)\n",
      "Requirement already satisfied: cachetools<6,>=3.1.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (5.3.3)\n",
      "Requirement already satisfied: google-api-core<3,>=2.0.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (2.17.1)\n",
      "Requirement already satisfied: google-apitools<0.5.32,>=0.5.31 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (0.5.31)\n",
      "Requirement already satisfied: google-auth<3,>=1.18.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (2.28.2)\n",
      "Requirement already satisfied: google-auth-httplib2<0.2.0,>=0.1.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (0.1.1)\n",
      "Requirement already satisfied: google-cloud-datastore<3,>=2.0.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (2.19.0)\n",
      "Requirement already satisfied: google-cloud-pubsub<3,>=2.1.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (2.20.2)\n",
      "Requirement already satisfied: google-cloud-pubsublite<2,>=1.2.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (1.9.0)\n",
      "Requirement already satisfied: google-cloud-storage<3,>=2.14.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (2.15.0)\n",
      "Requirement already satisfied: google-cloud-bigquery<4,>=2.0.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (3.19.0)\n",
      "Requirement already satisfied: google-cloud-bigquery-storage<3,>=2.6.3 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (2.24.0)\n",
      "Requirement already satisfied: google-cloud-core<3,>=2.0.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (2.4.1)\n",
      "Requirement already satisfied: google-cloud-bigtable<3,>=2.19.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (2.23.0)\n",
      "Requirement already satisfied: google-cloud-spanner<4,>=3.0.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (3.44.0)\n",
      "Requirement already satisfied: google-cloud-dlp<4,>=3.0.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (3.16.0)\n",
      "Requirement already satisfied: google-cloud-language<3,>=2.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (2.13.3)\n",
      "Requirement already satisfied: google-cloud-videointelligence<3,>=2.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (2.13.3)\n",
      "Requirement already satisfied: google-cloud-vision<4,>=2 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (3.7.2)\n",
      "Requirement already satisfied: google-cloud-recommendations-ai<0.11.0,>=0.1.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (0.10.10)\n",
      "Requirement already satisfied: google-cloud-aiplatform<2.0,>=1.26.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (1.44.0)\n",
      "Requirement already satisfied: isodate<1.0.0,>=0.6.1 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from azure-mgmt-resource>=21.0.0->zenml) (0.6.1)\n",
      "Requirement already satisfied: azure-common~=1.1 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from azure-mgmt-resource>=21.0.0->zenml) (1.1.28)\n",
      "Requirement already satisfied: azure-mgmt-core<2.0.0,>=1.3.2 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from azure-mgmt-resource>=21.0.0->zenml) (1.4.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from click<9,>=7.0->mlflow) (0.4.6)\n",
      "Requirement already satisfied: validators<0.19,>=0.18 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from click-params<0.4.0,>=0.3.0->zenml) (0.18.2)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from docker<8,>=4.0.0->mlflow) (2.2.1)\n",
      "Requirement already satisfied: websocket-client>=0.32.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from docker<8,>=4.0.0->mlflow) (1.7.0)\n",
      "Requirement already satisfied: pywin32>=304 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from docker<8,>=4.0.0->mlflow) (306)\n",
      "Requirement already satisfied: Werkzeug>=2.3.7 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from Flask<4->mlflow) (3.0.1)\n",
      "Requirement already satisfied: itsdangerous>=2.1.2 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from Flask<4->mlflow) (2.1.2)\n",
      "Requirement already satisfied: blinker>=1.6.2 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from Flask<4->mlflow) (1.7.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from gitpython<4,>=3.1.9->mlflow) (4.0.11)\n",
      "Requirement already satisfied: graphql-core<3.3,>=3.1 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from graphene<4->mlflow) (3.2.3)\n",
      "Requirement already satisfied: graphql-relay<3.3,>=3.1 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from graphene<4->mlflow) (3.2.0)\n",
      "Requirement already satisfied: aniso8601<10,>=8 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from graphene<4->mlflow) (9.0.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from importlib-metadata!=4.7.0,<8,>=3.7.0->mlflow) (3.18.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from rich>=12.0.0->rich[jupyter]>=12.0.0->zenml) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from rich>=12.0.0->rich[jupyter]>=12.0.0->zenml) (2.17.2)\n",
      "Requirement already satisfied: ipywidgets<9,>=7.5.1 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from rich[jupyter]>=12.0.0->zenml) (8.1.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sqlalchemy<3,>=1.4.0->mlflow) (3.0.3)\n",
      "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-metadata<1.15,>=1.14.0->tensorflow-data-validation) (1.63.0)\n",
      "Requirement already satisfied: google-api-python-client<2,>=1.7.11 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tfx-bsl<1.15,>=1.14.0->tensorflow-data-validation) (1.12.11)\n",
      "Requirement already satisfied: tensorflow-serving-api<3,>=2.13.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tfx-bsl<1.15,>=1.14.0->tensorflow-data-validation) (2.14.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.15.0->tensorflow) (0.43.0)\n",
      "Requirement already satisfied: azure-core<2.0.0,>=1.26.2 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from azure-mgmt-core<2.0.0,>=1.3.2->azure-mgmt-resource>=21.0.0->zenml) (1.30.1)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow) (5.0.1)\n",
      "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from google-api-python-client<2,>=1.7.11->tfx-bsl<1.15,>=1.14.0->tensorflow-data-validation) (3.0.1)\n",
      "Requirement already satisfied: oauth2client>=1.4.12 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from google-apitools<0.5.32,>=0.5.31->apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (4.1.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from google-auth<3,>=1.18.0->apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from google-auth<3,>=1.18.0->apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (4.9)\n",
      "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (1.12.3)\n",
      "Requirement already satisfied: shapely<3.0.0dev in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (2.0.3)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from google-cloud-bigquery<4,>=2.0.0->apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (2.7.0)\n",
      "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from google-cloud-bigtable<3,>=2.19.0->apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (0.13.0)\n",
      "Requirement already satisfied: grpcio-status>=1.33.2 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from google-cloud-pubsub<3,>=2.1.0->apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (1.48.2)\n",
      "Requirement already satisfied: overrides<8.0.0,>=6.0.1 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from google-cloud-pubsublite<2,>=1.2.0->apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (7.7.0)\n",
      "Requirement already satisfied: deprecated>=1.2.14 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from google-cloud-spanner<4,>=3.0.0->apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (1.2.14)\n",
      "Requirement already satisfied: grpc-interceptor>=0.15.4 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from google-cloud-spanner<4,>=3.0.0->apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (0.15.4)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from google-cloud-storage<3,>=2.14.0->apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (1.5.0)\n",
      "Requirement already satisfied: docopt in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from hdfs<3.0.0,>=2.1.0->apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (0.6.2)\n",
      "Requirement already satisfied: comm>=0.1.3 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from ipywidgets<9,>=7.5.1->rich[jupyter]>=12.0.0->zenml) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from ipywidgets<9,>=7.5.1->rich[jupyter]>=12.0.0->zenml) (8.18.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from ipywidgets<9,>=7.5.1->rich[jupyter]>=12.0.0->zenml) (5.14.2)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.10 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from ipywidgets<9,>=7.5.1->rich[jupyter]>=12.0.0->zenml) (4.0.10)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.10 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from ipywidgets<9,>=7.5.1->rich[jupyter]>=12.0.0->zenml) (3.0.10)\n",
      "Requirement already satisfied: tzlocal>=1.2 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from js2py<1,>=0.74->apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (5.2)\n",
      "Requirement already satisfied: pyjsparser>=2.5.1 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from js2py<1,>=0.74->apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (2.7.1)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from jsonschema<5.0.0,>=4.0.0->apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (23.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from jsonschema<5.0.0,>=4.0.0->apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from jsonschema<5.0.0,>=4.0.0->apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (0.33.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from jsonschema<5.0.0,>=4.0.0->apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (0.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=12.0.0->rich[jupyter]>=12.0.0->zenml) (0.1.2)\n",
      "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pymongo<5.0.0,>=3.8.0->apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (2.6.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (1.2.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: decorator>=3.4.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from validators<0.19,>=0.18->click-params<0.4.0,>=0.3.0->zenml) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from ipython>=6.1.0->ipywidgets<9,>=7.5.1->rich[jupyter]>=12.0.0->zenml) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from ipython>=6.1.0->ipywidgets<9,>=7.5.1->rich[jupyter]>=12.0.0->zenml) (0.1.6)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from ipython>=6.1.0->ipywidgets<9,>=7.5.1->rich[jupyter]>=12.0.0->zenml) (3.0.36)\n",
      "Requirement already satisfied: stack-data in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from ipython>=6.1.0->ipywidgets<9,>=7.5.1->rich[jupyter]>=12.0.0->zenml) (0.6.3)\n",
      "Requirement already satisfied: exceptiongroup in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from ipython>=6.1.0->ipywidgets<9,>=7.5.1->rich[jupyter]>=12.0.0->zenml) (1.2.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.7 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from oauth2client>=1.4.12->google-apitools<0.5.32,>=0.5.31->apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (0.5.1)\n",
      "Requirement already satisfied: tzdata in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tzlocal>=1.2->js2py<1,>=0.74->apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (2024.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets<9,>=7.5.1->rich[jupyter]>=12.0.0->zenml) (0.8.3)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=6.1.0->ipywidgets<9,>=7.5.1->rich[jupyter]>=12.0.0->zenml) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets<9,>=7.5.1->rich[jupyter]>=12.0.0->zenml) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets<9,>=7.5.1->rich[jupyter]>=12.0.0->zenml) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets<9,>=7.5.1->rich[jupyter]>=12.0.0->zenml) (0.2.2)\n",
      "WARNING:tensorflow:From c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tfx_bsl\\types\\common_types.py:26: The name tf.SparseTensorValue is deprecated. Please use tf.compat.v1.SparseTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tfx_bsl\\types\\common_types.py:27: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# General imports\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import warnings\n",
    "# Suppress deprecated warnings\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning, module=\".*tensorflow.*\")\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import mlflow\n",
    "import numpy as np\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import os\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "from transformers import BertTokenizer\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow_data_validation as tfdv\n",
    "import mlflow\n",
    "import mlflow.tensorflow\n",
    "# Machine Learning imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification, BertConfig\n",
    "import re\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "# ZenML imports\n",
    "from zenml.steps import step\n",
    "from zenml.pipelines import pipeline\n",
    "# For Twitter API (if needed for real-time data)\n",
    "import tweepy\n",
    "# Other imports you might need (depending on your exact requirements)\n",
    "import os\n",
    "os.environ['PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION'] = 'python'\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "os.environ['HF_HUB_DISABLE_SYMLINKS_WARNING'] = '1'\n",
    "import json\n",
    "import requests\n",
    "from cassandra.cluster import Cluster\n",
    "from cassandra.auth import PlainTextAuthProvider\n",
    "import uuid\n",
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "import pandas as pd\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: {'batch_size': 16, 'learning_rate': 2e-05, 'epochs': 2, 'weight_decay': 0.01, 'max_grad_norm': 1.0, 'lr_step_size': 1, 'lr_gamma': 0.1}\n"
     ]
    }
   ],
   "source": [
    "# Constants and Hyperparameters\n",
    "CSV_FILE_PATH = 'C:\\\\Users\\\\LENOVO\\\\Desktop\\\\CSC5356_SP24\\\\scripts\\\\dataset.csv'\n",
    "PRETRAINED_LM_PATH = 'C:\\\\Users\\\\LENOVO\\\\Desktop\\\\bert-election2020-twitter-stance-biden'\n",
    "HYPERPARAMS = {\n",
    "    \"batch_size\": 16,\n",
    "    \"learning_rate\": 2e-5,\n",
    "    \"epochs\": 2,\n",
    "    \"weight_decay\": 0.01,\n",
    "    'max_grad_norm': 1.0,\n",
    "    'lr_step_size': 1,\n",
    "    'lr_gamma': 0.1,\n",
    "    \n",
    "}\n",
    "print(\"Hyperparameters:\", HYPERPARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(CSV_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.set_index('tweet_id', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tweet_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1298098337946783745</th>\n",
       "      <td>@USER @USER and like you, they are all squish #gop weasels who will be in tears on election night.</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1296643755022397440</th>\n",
       "      <td>.@USER on covid-19: we'll put the politics aside. we'll take the muzzle off the experts so the the public gets the information they need and deserve. honest unvarnished truth. they can handle it. #demconvention @USER HTTP</td>\n",
       "      <td>FAVOR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1293907151501459462</th>\n",
       "      <td>@USER google: how to vote early in \"your state\" and get it done as soon as possible. we have a duty to help the u.s. postal service process these ballots and reduce the stress donald trump is intentionally placing on these public servants. \\n\\n#voteearly #voteblue #bidenharris2020</td>\n",
       "      <td>FAVOR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1239356229803225088</th>\n",
       "      <td>am i the only one that pictures biden this way all the time? #demdebate HTTP</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1298801129984266241</th>\n",
       "      <td>@USER was it the infanticide lie the nun dropped on us that was helpful?\\n#rnc2020</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                          text  \\\n",
       "tweet_id                                                                                                                                                                                                                                                                                                         \n",
       "1298098337946783745                                                                                                                                                                                         @USER @USER and like you, they are all squish #gop weasels who will be in tears on election night.   \n",
       "1296643755022397440                                                              .@USER on covid-19: we'll put the politics aside. we'll take the muzzle off the experts so the the public gets the information they need and deserve. honest unvarnished truth. they can handle it. #demconvention @USER HTTP   \n",
       "1293907151501459462  @USER google: how to vote early in \"your state\" and get it done as soon as possible. we have a duty to help the u.s. postal service process these ballots and reduce the stress donald trump is intentionally placing on these public servants. \\n\\n#voteearly #voteblue #bidenharris2020   \n",
       "1239356229803225088                                                                                                                                                                                                               am i the only one that pictures biden this way all the time? #demdebate HTTP   \n",
       "1298801129984266241                                                                                                                                                                                                         @USER was it the infanticide lie the nun dropped on us that was helpful?\\n#rnc2020   \n",
       "\n",
       "                     label  \n",
       "tweet_id                    \n",
       "1298098337946783745   NONE  \n",
       "1296643755022397440  FAVOR  \n",
       "1293907151501459462  FAVOR  \n",
       "1239356229803225088   NONE  \n",
       "1298801129984266241   NONE  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data.isnull().values.any():\n",
    "            null_counts = data.isnull().sum()\n",
    "            print(\"Warning: Null values found in the dataset.\")\n",
    "            print(f\"Null value counts by column:\\n{null_counts[null_counts > 0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_columns = {'text','label'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 375 entries, 1298098337946783745 to 1292574112263610369\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    375 non-null    object\n",
      " 1   label   375 non-null    object\n",
      "dtypes: object(2)\n",
      "memory usage: 8.8+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text     0\n",
       "label    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'listened to both these guys for 2 hours tonight. much of what i heard is more big government that promises free/free/free, open borders, killing the oil industry and more. thanks, but we went through 8 years of that with obama and many of us do not want to go back.... #demdebate'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.text.iloc[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NONE       157\n",
       "FAVOR      112\n",
       "AGAINST    106\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.countplot(data.label, palette='Spectral')\n",
    "plt.xlabel('Classes')\n",
    "plt.title('Class Distribution');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "        \"\"\"\n",
    "        Function to clean text data.\n",
    "\n",
    "        Args:\n",
    "            text (str): Text to be cleaned.\n",
    "\n",
    "        Returns:\n",
    "            str: Cleaned text.\n",
    "        \"\"\"\n",
    "        # Removing URLs\n",
    "        text = re.sub(r'http\\S+', '', text)\n",
    "        # Removing usernames and hashtags\n",
    "        text = re.sub(r'@\\S+|#\\S+', '', text)\n",
    "        # Removing special characters and numbers\n",
    "        text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
    "        # Converting to lowercase\n",
    "        text = text.lower().strip()\n",
    "        # Tokenize and rejoin the text to ensure clean tokenization\n",
    "        tokens = text.split()\n",
    "        return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_columns = {'text','label'}\n",
    "if not expected_columns.issubset(data.columns):\n",
    "    missing_cols = expected_columns - set(data.columns)\n",
    "    error_msg = f\"The dataframe is missing the following required columns: {', '.join(missing_cols)}\"\n",
    "    logging.error(error_msg)\n",
    "    raise ValueError(error_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing the data...\n"
     ]
    }
   ],
   "source": [
    "print(\"Preprocessing the data...\")\n",
    "missing_label_rows = data[data['label'].isna()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(subset=['label'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not missing_label_rows.empty:\n",
    "    print(f\"Rows with missing labels:\\n{missing_label_rows}\")\n",
    "# If the DataFrame is empty after dropping missing values, return it as is\n",
    "if data.empty:\n",
    "    print(\"The DataFrame is empty after preprocessing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode labels\n",
    "label_mapping = {'NONE': 0, 'FAVOR': 1, 'AGAINST': 2}\n",
    "data['label'] = data['label'].map(label_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tweet_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1298098337946783745</th>\n",
       "      <td>@USER @USER and like you, they are all squish #gop weasels who will be in tears on election night.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1296643755022397440</th>\n",
       "      <td>.@USER on covid-19: we'll put the politics aside. we'll take the muzzle off the experts so the the public gets the information they need and deserve. honest unvarnished truth. they can handle it. #demconvention @USER HTTP</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1293907151501459462</th>\n",
       "      <td>@USER google: how to vote early in \"your state\" and get it done as soon as possible. we have a duty to help the u.s. postal service process these ballots and reduce the stress donald trump is intentionally placing on these public servants. \\n\\n#voteearly #voteblue #bidenharris2020</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1239356229803225088</th>\n",
       "      <td>am i the only one that pictures biden this way all the time? #demdebate HTTP</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1298801129984266241</th>\n",
       "      <td>@USER was it the infanticide lie the nun dropped on us that was helpful?\\n#rnc2020</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1310982880865275907</th>\n",
       "      <td>@USER i think all the lids on the biden campaign days is them trying to find the best drug cocktail to allow biden to look sentient.  trial and error.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1302050245820506112</th>\n",
       "      <td>you know how they say anyone can be president? it's not true. trump tried it and he couldn't do it. he is literally unable to do the job. so maybe we should give it to someone else\\n\\n#trumpisaloser #biden2020</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1239344169518116864</th>\n",
       "      <td>biden is mostly campaigning for the general election, not debating bernie. #democraticdebate</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1295022959787151360</th>\n",
       "      <td>so jazzed to get to meet future u.s.rep @USER &amp;amp; u.s.sen. @USER at 11:30 today at stonebridge park in fayetteville. #2020election</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1311078278728306688</th>\n",
       "      <td>@USER and trump wont be able to creep around behind biden like he did to hillary. he has to stay in his own lane </td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1239394168939503617</th>\n",
       "      <td>listened to both these guys for 2 hours tonight. much of what i heard is more big government that promises free/free/free, open borders, killing the oil industry and more. thanks, but we went through 8 years of that with obama and many of us do not want to go back.... #demdebate</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1301991834110091265</th>\n",
       "      <td>@USER i'm sure we'll hear the usual blah blah blah hate trump blah blah blah racist trump blah blah russia blah. \\n#trumpnowmorethanever \\n#kag #maga</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1291065309063938048</th>\n",
       "      <td>joe #biden kept saying to the \\namerican people that he is\\nready to #debate president #trump \\nand accepted 3 #three debates \\ndoes joe #biden contemplate\\nhis intention to #refuse \\nand #avoid these debates\\non a #strategy using the #coronavirus \\nfor not wanting to travel \\n#prediction</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1293891434546855937</th>\n",
       "      <td>.@USER created this disastrous catastrophe; we all need to #voteblue to save lives ... u.s. reports highest number of covid-19 deaths in one day since mid-may HTTP</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1311123145462489088</th>\n",
       "      <td>joe biden is a massive racist . california was once red . think about before . our new generation needs to look back . i have a mind that goes back 100 years because of what my family and our books have taught me . what is a super predator , joe ???  #debates</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                  text  \\\n",
       "tweet_id                                                                                                                                                                                                                                                                                                                 \n",
       "1298098337946783745                                                                                                                                                                                                 @USER @USER and like you, they are all squish #gop weasels who will be in tears on election night.   \n",
       "1296643755022397440                                                                      .@USER on covid-19: we'll put the politics aside. we'll take the muzzle off the experts so the the public gets the information they need and deserve. honest unvarnished truth. they can handle it. #demconvention @USER HTTP   \n",
       "1293907151501459462          @USER google: how to vote early in \"your state\" and get it done as soon as possible. we have a duty to help the u.s. postal service process these ballots and reduce the stress donald trump is intentionally placing on these public servants. \\n\\n#voteearly #voteblue #bidenharris2020   \n",
       "1239356229803225088                                                                                                                                                                                                                       am i the only one that pictures biden this way all the time? #demdebate HTTP   \n",
       "1298801129984266241                                                                                                                                                                                                                 @USER was it the infanticide lie the nun dropped on us that was helpful?\\n#rnc2020   \n",
       "1310982880865275907                                                                                                                                           @USER i think all the lids on the biden campaign days is them trying to find the best drug cocktail to allow biden to look sentient.  trial and error.   \n",
       "1302050245820506112                                                                                  you know how they say anyone can be president? it's not true. trump tried it and he couldn't do it. he is literally unable to do the job. so maybe we should give it to someone else\\n\\n#trumpisaloser #biden2020   \n",
       "1239344169518116864                                                                                                                                                                                                       biden is mostly campaigning for the general election, not debating bernie. #democraticdebate   \n",
       "1295022959787151360                                                                                                                                                               so jazzed to get to meet future u.s.rep @USER &amp; u.s.sen. @USER at 11:30 today at stonebridge park in fayetteville. #2020election   \n",
       "1311078278728306688                                                                                                                                                                               @USER and trump wont be able to creep around behind biden like he did to hillary. he has to stay in his own lane    \n",
       "1239394168939503617            listened to both these guys for 2 hours tonight. much of what i heard is more big government that promises free/free/free, open borders, killing the oil industry and more. thanks, but we went through 8 years of that with obama and many of us do not want to go back.... #demdebate   \n",
       "1301991834110091265                                                                                                                                              @USER i'm sure we'll hear the usual blah blah blah hate trump blah blah blah racist trump blah blah russia blah. \\n#trumpnowmorethanever \\n#kag #maga   \n",
       "1291065309063938048  joe #biden kept saying to the \\namerican people that he is\\nready to #debate president #trump \\nand accepted 3 #three debates \\ndoes joe #biden contemplate\\nhis intention to #refuse \\nand #avoid these debates\\non a #strategy using the #coronavirus \\nfor not wanting to travel \\n#prediction   \n",
       "1293891434546855937                                                                                                                                .@USER created this disastrous catastrophe; we all need to #voteblue to save lives ... u.s. reports highest number of covid-19 deaths in one day since mid-may HTTP   \n",
       "1311123145462489088                               joe biden is a massive racist . california was once red . think about before . our new generation needs to look back . i have a mind that goes back 100 years because of what my family and our books have taught me . what is a super predator , joe ???  #debates   \n",
       "\n",
       "                     label  \n",
       "tweet_id                    \n",
       "1298098337946783745      0  \n",
       "1296643755022397440      1  \n",
       "1293907151501459462      1  \n",
       "1239356229803225088      0  \n",
       "1298801129984266241      0  \n",
       "1310982880865275907      2  \n",
       "1302050245820506112      1  \n",
       "1239344169518116864      0  \n",
       "1295022959787151360      0  \n",
       "1311078278728306688      1  \n",
       "1239394168939503617      0  \n",
       "1301991834110091265      2  \n",
       "1291065309063938048      0  \n",
       "1293891434546855937      1  \n",
       "1311123145462489088      2  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the encoding\n",
    "unique_labels = data['label'].unique()\n",
    "if set(unique_labels) != {0, 1, 2}:\n",
    "    error_msg = f\"Labels are not correctly mapped. Found unique labels: {unique_labels}\"\n",
    "    logging.error(error_msg)\n",
    "    raise ValueError(error_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "sns.histplot([len(s) for s in data.text], bins=100)\n",
    "plt.title('Sentence Length')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No missing values found in the dataset.\n"
     ]
    }
   ],
   "source": [
    "# Visualize Missing Values\n",
    "missing_values = data.isnull().sum()\n",
    "missing_percentage = (missing_values / len(data)) * 100\n",
    "missing_df = pd.DataFrame({'Feature': missing_values.index, 'MissingValues': missing_values, 'Percentage': missing_percentage})\n",
    "# Filter out features with no missing values\n",
    "missing_df = missing_df[missing_df['MissingValues'] > 0].sort_values('Percentage', ascending=False)\n",
    "# Plotting missing values (if there are any)\n",
    "if not missing_df.empty:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x='Percentage', y='Feature', data=missing_df)\n",
    "    plt.title('Percentage of Missing Values per Feature')\n",
    "    plt.xlabel('Percentage')\n",
    "    plt.ylabel('Feature')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No missing values found in the dataset.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;35mUsing categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\u001b[0m\n",
      "\u001b[1;35mUsing categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Data Distribution Visualization\n",
    "if 'label' in data.columns:\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.countplot(x='label', data=data)\n",
    "    plt.title('Distribution of Labels')\n",
    "    plt.xlabel('Label')\n",
    "    plt.ylabel('Count')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Label column not found, skipping label distribution visualization.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the size for your test and validation sets\n",
    "test_size = 0.2\n",
    "validation_size = 0.1\n",
    "\n",
    "# Initial split to separate out the test set\n",
    "train_val_data, test_data = train_test_split(data, test_size=test_size, random_state=42)\n",
    "\n",
    "# Calculate the adjusted validation size based on the remaining data after test split\n",
    "adjusted_validation_size = validation_size / (1 - test_size)\n",
    "\n",
    "# Split the remaining data into training and validation sets\n",
    "train_data, val_data = train_test_split(train_val_data, test_size=adjusted_validation_size, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stats = tfdv.generate_statistics_from_dataframe(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id='facets-iframe' width=\"100%\" height=\"500px\"></iframe>\n",
       "        <script>\n",
       "        facets_iframe = document.getElementById('facets-iframe');\n",
       "        facets_html = '<script src=\"https://cdnjs.cloudflare.com/ajax/libs/webcomponentsjs/1.3.3/webcomponents-lite.js\"><\\/script><link rel=\"import\" href=\"https://raw.githubusercontent.com/PAIR-code/facets/master/facets-dist/facets-jupyter.html\"><facets-overview proto-input=\"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\"></facets-overview>';\n",
       "        facets_iframe.srcdoc = facets_html;\n",
       "         facets_iframe.id = \"\";\n",
       "         setTimeout(() => {\n",
       "           facets_iframe.setAttribute('height', facets_iframe.contentWindow.document.body.offsetHeight + 'px')\n",
       "         }, 1500)\n",
       "         </script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tfdv.visualize_statistics(train_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_stats = tfdv.generate_statistics_from_dataframe(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id='facets-iframe' width=\"100%\" height=\"500px\"></iframe>\n",
       "        <script>\n",
       "        facets_iframe = document.getElementById('facets-iframe');\n",
       "        facets_html = '<script src=\"https://cdnjs.cloudflare.com/ajax/libs/webcomponentsjs/1.3.3/webcomponents-lite.js\"><\\/script><link rel=\"import\" href=\"https://raw.githubusercontent.com/PAIR-code/facets/master/facets-dist/facets-jupyter.html\"><facets-overview proto-input=\"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\"></facets-overview>';\n",
       "        facets_iframe.srcdoc = facets_html;\n",
       "         facets_iframe.id = \"\";\n",
       "         setTimeout(() => {\n",
       "           facets_iframe.setAttribute('height', facets_iframe.contentWindow.document.body.offsetHeight + 'px')\n",
       "         }, 1500)\n",
       "         </script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tfdv.visualize_statistics(val_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_stats = tfdv.generate_statistics_from_dataframe(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id='facets-iframe' width=\"100%\" height=\"500px\"></iframe>\n",
       "        <script>\n",
       "        facets_iframe = document.getElementById('facets-iframe');\n",
       "        facets_html = '<script src=\"https://cdnjs.cloudflare.com/ajax/libs/webcomponentsjs/1.3.3/webcomponents-lite.js\"><\\/script><link rel=\"import\" href=\"https://raw.githubusercontent.com/PAIR-code/facets/master/facets-dist/facets-jupyter.html\"><facets-overview proto-input=\"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\"></facets-overview>';\n",
       "        facets_iframe.srcdoc = facets_html;\n",
       "         facets_iframe.id = \"\";\n",
       "         setTimeout(() => {\n",
       "           facets_iframe.setAttribute('height', facets_iframe.contentWindow.document.body.offsetHeight + 'px')\n",
       "         }, 1500)\n",
       "         </script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tfdv.visualize_statistics(test_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema inferred from the testing data.\n"
     ]
    }
   ],
   "source": [
    "# Infer schema from training data\n",
    "schema = tfdv.infer_schema(statistics=train_stats)\n",
    "print(\"Schema inferred from the testing data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Presence</th>\n",
       "      <th>Valency</th>\n",
       "      <th>Domain</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Feature name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>'text'</th>\n",
       "      <td>BYTES</td>\n",
       "      <td>required</td>\n",
       "      <td></td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'label'</th>\n",
       "      <td>INT</td>\n",
       "      <td>required</td>\n",
       "      <td></td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'tweet_id'</th>\n",
       "      <td>INT</td>\n",
       "      <td>required</td>\n",
       "      <td></td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Type  Presence Valency Domain\n",
       "Feature name                                \n",
       "'text'        BYTES  required              -\n",
       "'label'         INT  required              -\n",
       "'tweet_id'      INT  required              -"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tfdv.display_schema(schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating Validation data...\n",
      "No anomalies detected in Validation data.\n",
      "Validating Test data...\n",
      "No anomalies detected in Test data.\n"
     ]
    }
   ],
   "source": [
    "for dataset_name, dataset_stats in [('Validation', val_stats), ('Test', test_stats)]:\n",
    "    print(f\"Validating {dataset_name} data...\")\n",
    "    anomalies = tfdv.validate_statistics(statistics=dataset_stats, schema=schema)\n",
    "    if anomalies.anomaly_info:\n",
    "        print(f\"Anomalies found in {dataset_name} data:\")\n",
    "        tfdv.display_anomalies(anomalies)\n",
    "    else:\n",
    "        print(f\"No anomalies detected in {dataset_name} data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h4 style=\"color:green;\">No anomalies found.</h4>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "anomalies = tfdv.validate_statistics(statistics=train_stats, schema=schema)\n",
    "tfdv.display_anomalies(anomalies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(PRETRAINED_LM_PATH)\n",
    "from cassandra.policies import DCAwareRoundRobinPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to the Cassandra Database...\n",
      "\u001b[33mDowngrading core protocol version from 66 to 65 for 127.0.0.1:9042. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version\u001b[0m\n",
      "\u001b[33mDowngrading core protocol version from 65 to 5 for 127.0.0.1:9042. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version\u001b[0m\n",
      "\u001b[1;35mUsing datacenter 'datacenter1' for DCAwareRoundRobinPolicy (via host '127.0.0.1:9042'); if incorrect, please specify a local_dc to the constructor, or limit contact points to local cluster nodes\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "print(\"Connecting to the Cassandra Database...\")\n",
    "cluster = Cluster(contact_points=['127.0.0.1'],port=9042,load_balancing_policy=DCAwareRoundRobinPolicy())\n",
    "session = cluster.connect()\n",
    "session.set_keyspace('keyspace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from transformers import BertTokenizer\n",
    "from cassandra.cluster import Cluster\n",
    "from cassandra.query import BatchStatement\n",
    "from cassandra import ConsistencyLevel\n",
    "import uuid\n",
    "import pandas as pd\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['text', 'label'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tokens</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>191bfb7f-905f-4231-8818-8c81f37645ae</td>\n",
       "      <td>[101, 1030, 5310, 1030, 5310, 1030, 5310, 1030, 5310, 1030, 5310, 2017, 2024, 2019, 7780, 2000, 2035, 1010, 2926, 2308, 2040, 2954, 10126, 2007, 2061, 2116, 2367, 15314, 999, 4067, 2017, 100, 1001, 17183, 8663, 15338, 3258, 1001, 7226, 2368, 8167, 6935, 11387, 11387, 102]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7037926b-c428-48f2-8af0-941a6b9b4be9</td>\n",
       "      <td>[101, 1030, 5310, 3533, 7226, 2368, 1521, 1055, 4700, 2086, 5443, 8398, 2015, 4700, 2706, 1012, 1998, 8398, 2347, 1521, 1056, 1037, 3761, 1010, 2085, 2008, 1521, 1055, 16436, 2005, 2017, 3533, 1012, 2009, 2003, 2054, 2009, 2003, 2138, 3533, 7226, 2368, 2003, 2040, 2002, 2003, 1012, 102]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>460c8904-e9f8-4a54-9c4e-a82373c9a933</td>\n",
       "      <td>[101, 1030, 5310, 2053, 1010, 2027, 2024, 2145, 2045, 1012, 2030, 2062, 2031, 2042, 6866, 1012, 9733, 2038, 3314, 2007, 2023, 2828, 1997, 4031, 2035, 1996, 2051, 1012, 3531, 2562, 7316, 2068, 1012, 1001, 3789, 16558, 5657, 102]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dcf84c07-dd40-4b60-b518-e2bd23e7b98a</td>\n",
       "      <td>[101, 1030, 5310, 1030, 5310, 1048, 2863, 2080, 1012, 1012, 1012, 2066, 2017, 2729, 2055, 2637, 1012, 1001, 8398, 11387, 11387, 1998, 2009, 2097, 2022, 1037, 4121, 2663, 999, 102]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>151f5d06-665b-4655-a94b-1fab6c8cd2df</td>\n",
       "      <td>[101, 1001, 3407, 18447, 11795, 3370, 27318, 5620, 10259, 1001, 3407, 25434, 16168, 10259, 1001, 3407, 18447, 11795, 3370, 27318, 2290, 10259, 1001, 28844, 25688, 4710, 1998, 1001, 2176, 5974, 29100, 2015, 2005, 1001, 6221, 24456, 2361, 100, 8299, 102]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id  \\\n",
       "0  191bfb7f-905f-4231-8818-8c81f37645ae   \n",
       "1  7037926b-c428-48f2-8af0-941a6b9b4be9   \n",
       "2  460c8904-e9f8-4a54-9c4e-a82373c9a933   \n",
       "3  dcf84c07-dd40-4b60-b518-e2bd23e7b98a   \n",
       "4  151f5d06-665b-4655-a94b-1fab6c8cd2df   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                            tokens  \\\n",
       "0                 [101, 1030, 5310, 1030, 5310, 1030, 5310, 1030, 5310, 1030, 5310, 2017, 2024, 2019, 7780, 2000, 2035, 1010, 2926, 2308, 2040, 2954, 10126, 2007, 2061, 2116, 2367, 15314, 999, 4067, 2017, 100, 1001, 17183, 8663, 15338, 3258, 1001, 7226, 2368, 8167, 6935, 11387, 11387, 102]   \n",
       "1  [101, 1030, 5310, 3533, 7226, 2368, 1521, 1055, 4700, 2086, 5443, 8398, 2015, 4700, 2706, 1012, 1998, 8398, 2347, 1521, 1056, 1037, 3761, 1010, 2085, 2008, 1521, 1055, 16436, 2005, 2017, 3533, 1012, 2009, 2003, 2054, 2009, 2003, 2138, 3533, 7226, 2368, 2003, 2040, 2002, 2003, 1012, 102]   \n",
       "2                                                              [101, 1030, 5310, 2053, 1010, 2027, 2024, 2145, 2045, 1012, 2030, 2062, 2031, 2042, 6866, 1012, 9733, 2038, 3314, 2007, 2023, 2828, 1997, 4031, 2035, 1996, 2051, 1012, 3531, 2562, 7316, 2068, 1012, 1001, 3789, 16558, 5657, 102]   \n",
       "3                                                                                                              [101, 1030, 5310, 1030, 5310, 1048, 2863, 2080, 1012, 1012, 1012, 2066, 2017, 2729, 2055, 2637, 1012, 1001, 8398, 11387, 11387, 1998, 2009, 2097, 2022, 1037, 4121, 2663, 999, 102]   \n",
       "4                                     [101, 1001, 3407, 18447, 11795, 3370, 27318, 5620, 10259, 1001, 3407, 25434, 16168, 10259, 1001, 3407, 18447, 11795, 3370, 27318, 2290, 10259, 1001, 28844, 25688, 4710, 1998, 1001, 2176, 5974, 29100, 2015, 2005, 1001, 6221, 24456, 2361, 100, 8299, 102]   \n",
       "\n",
       "  label  \n",
       "0     1  \n",
       "1     2  \n",
       "2     0  \n",
       "3     2  \n",
       "4     2  "
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mError preparing query:\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"cassandra\\cluster.py\", line 3128, in cassandra.cluster.Session.prepare\n",
      "  File \"cassandra\\cluster.py\", line 4956, in cassandra.cluster.ResponseFuture.result\n",
      "cassandra.InvalidRequest: Error from server: code=2200 [Invalid query] message=\"Undefined column name features in table \"keyspace\".features\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\"> </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> </span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">cassandra.cluster.Session.prepare</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">3131</span>                                                        <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">cassandra.cluster.Session.prepare</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">3128</span>                                                        <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">cassandra.cluster.ResponseFuture.result</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">4956</span>                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">InvalidRequest: </span>Error from server: <span style=\"color: #808000; text-decoration-color: #808000\">code</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2200</span> <span style=\"font-weight: bold\">[</span>Invalid query<span style=\"font-weight: bold\">]</span> <span style=\"color: #808000; text-decoration-color: #808000\">message</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"Undefined column name features in table </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\"</span>keyspace\".features\"\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m\u001b[0m\u001b[31m\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m\u001b[0m\u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m1\u001b[0m                                                                                    \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m in \u001b[92mcassandra.cluster.Session.prepare\u001b[0m:\u001b[94m3131\u001b[0m                                                        \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m in \u001b[92mcassandra.cluster.Session.prepare\u001b[0m:\u001b[94m3128\u001b[0m                                                        \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m in \u001b[92mcassandra.cluster.ResponseFuture.result\u001b[0m:\u001b[94m4956\u001b[0m                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m\n",
       "\u001b[1;91mInvalidRequest: \u001b[0mError from server: \u001b[33mcode\u001b[0m=\u001b[1;36m2200\u001b[0m \u001b[1m[\u001b[0mInvalid query\u001b[1m]\u001b[0m \u001b[33mmessage\u001b[0m=\u001b[32m\"Undefined\u001b[0m\u001b[32m column name features in table \u001b[0m\n",
       "\u001b[32m\"\u001b[0mkeyspace\".features\"\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "insert_statement = session.prepare('INSERT INTO \"keyspace\".features (id, features) VALUES (?, ?)')\n",
    "batch = BatchStatement(consistency_level=ConsistencyLevel.ONE)\n",
    "batch_size_limit = 16\n",
    "if 'tokens' not in data.columns:\n",
    "    data['tokens'] = None\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    row_id = uuid.uuid4()\n",
    "    tokens = tokenizer.encode(row['text'], add_special_tokens=True)\n",
    "    data.at[index, 'tokens'] = tokens\n",
    "    batch.add(insert_statement, (row_id, tokens))\n",
    "\n",
    "    if len(batch) >= batch_size_limit:\n",
    "        session.execute(batch)\n",
    "        batch.clear()\n",
    "\n",
    "if len(batch) > 0:\n",
    "    session.execute(batch)\n",
    "\n",
    "    print(\"Feature engineering and data insertion completed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Preparing the insert statement to include positional information\n",
    "#DESCRIBE TABLE \"keyspace\".features;\n",
    "def clean_text(text):\n",
    "    # Implement text cleaning here\n",
    "    text = text.replace('@USER', '')  # Remove @USER mentions\n",
    "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    # Add other cleaning steps as needed\n",
    "    return text\n",
    "# Function for text normalization\n",
    "def normalize_text(text):\n",
    "    # Implement text normalization steps here\n",
    "    # For example: lowercasing, handling mentions, hashtags, URLs\n",
    "    normalized_text = text.lower()\n",
    "    # Add more normalization steps as required\n",
    "    return normalized_text\n",
    "insert_statement = session.prepare(\n",
    "    'INSERT INTO \"keyspace\".features (id, tokens, label) VALUES (?, ?, ?)'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\"> </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> </span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #808000; text-decoration-color: #808000\">c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\indexes\\bas</span> <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #808000; text-decoration-color: #808000\">e.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">3802</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">get_loc</span>                                                                             <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3799 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>)                                                                         <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3800 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span>casted_key = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._maybe_cast_indexer(key)                                    <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3801 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">try</span>:                                                                          <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span>3802 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._engine.get_loc(casted_key)                                   <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3803 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">except</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">KeyError</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">as</span> err:                                                       <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3804 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">KeyError</span>(key) <span style=\"color: #0000ff; text-decoration-color: #0000ff\">from</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff; text-decoration: underline\">err</span>                                              <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3805 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">except</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">TypeError</span>:                                                             <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">pandas._libs.index.IndexEngine.get_loc</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">138</span>                                                    <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">pandas._libs.index.IndexEngine.get_loc</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">165</span>                                                    <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">pandas._libs.hashtable.PyObjectHashTable.get_item</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">5745</span>                                        <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">pandas._libs.hashtable.PyObjectHashTable.get_item</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">5753</span>                                        <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">KeyError: </span><span style=\"color: #008000; text-decoration-color: #008000\">'text'</span>\n",
       "\n",
       "<span style=\"font-style: italic\">The above exception was the direct cause of the following exception:</span>\n",
       "\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"> </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> </span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">11</span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #808000; text-decoration-color: #808000\">c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\series.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">9</span> <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">81</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">__getitem__</span>                                                                                <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 978 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._values[key]                                                      <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 979 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 980 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">elif</span> key_is_scalar:                                                               <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span> 981 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._get_value(key)                                                   <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 982 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 983 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> is_hashable(key):                                                              <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 984 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Otherwise index.get_value will raise InvalidIndexError</span>                      <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #808000; text-decoration-color: #808000\">c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\series.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span> <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">089</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_get_value</span>                                                                                <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1086 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._values[label]                                                    <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1087 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1088 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Similar to Index.get_value, but we do not fall back to positional</span>               <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span>1089 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span>loc = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.index.get_loc(label)                                                   <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1090 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.index._get_values_for_loc(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, loc, label)                           <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1091 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">   </span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1092 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">__setitem__</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, key, value) -&gt; <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>:                                            <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #808000; text-decoration-color: #808000\">c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\indexes\\bas</span> <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #808000; text-decoration-color: #808000\">e.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">3804</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">get_loc</span>                                                                             <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3801 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">try</span>:                                                                          <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3802 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._engine.get_loc(casted_key)                                   <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3803 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">except</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">KeyError</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">as</span> err:                                                       <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span>3804 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">KeyError</span>(key) <span style=\"color: #0000ff; text-decoration-color: #0000ff\">from</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff; text-decoration: underline\">err</span>                                              <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3805 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">except</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">TypeError</span>:                                                             <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3806 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># If we have a listlike key, _check_indexing_error will raise</span>             <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3807 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">#  InvalidIndexError. Otherwise we fall through and re-raise</span>              <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">KeyError: </span><span style=\"color: #008000; text-decoration-color: #008000\">'text'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m\u001b[0m\u001b[31m\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m\u001b[0m\u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[33mc:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\indexes\\bas\u001b[0m \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[33me.py\u001b[0m:\u001b[94m3802\u001b[0m in \u001b[92mget_loc\u001b[0m                                                                             \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m3799 \u001b[0m\u001b[2m            \u001b[0m)                                                                         \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m3800 \u001b[0m\u001b[2m         \u001b[0mcasted_key = \u001b[96mself\u001b[0m._maybe_cast_indexer(key)                                    \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m3801 \u001b[0m\u001b[2m         \u001b[0m\u001b[94mtry\u001b[0m:                                                                          \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[31m \u001b[0m3802 \u001b[2m            \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m._engine.get_loc(casted_key)                                   \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m3803 \u001b[0m\u001b[2m         \u001b[0m\u001b[94mexcept\u001b[0m \u001b[96mKeyError\u001b[0m \u001b[94mas\u001b[0m err:                                                       \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m3804 \u001b[0m\u001b[2m            \u001b[0m\u001b[94mraise\u001b[0m \u001b[96mKeyError\u001b[0m(key) \u001b[94mfrom\u001b[0m \u001b[4;96merr\u001b[0m                                              \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m3805 \u001b[0m\u001b[2m         \u001b[0m\u001b[94mexcept\u001b[0m \u001b[96mTypeError\u001b[0m:                                                             \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m in \u001b[92mpandas._libs.index.IndexEngine.get_loc\u001b[0m:\u001b[94m138\u001b[0m                                                    \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m in \u001b[92mpandas._libs.index.IndexEngine.get_loc\u001b[0m:\u001b[94m165\u001b[0m                                                    \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m in \u001b[92mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0m:\u001b[94m5745\u001b[0m                                        \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m in \u001b[92mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0m:\u001b[94m5753\u001b[0m                                        \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m\n",
       "\u001b[1;91mKeyError: \u001b[0m\u001b[32m'text'\u001b[0m\n",
       "\n",
       "\u001b[3mThe above exception was the direct cause of the following exception:\u001b[0m\n",
       "\n",
       "\u001b[31m\u001b[0m\u001b[31m\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m\u001b[0m\u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m11\u001b[0m                                                                                   \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[33mc:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m:\u001b[94m9\u001b[0m \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[94m81\u001b[0m in \u001b[92m__getitem__\u001b[0m                                                                                \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 978 \u001b[0m\u001b[2m         \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m._values[key]                                                      \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 979 \u001b[0m\u001b[2m      \u001b[0m                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 980 \u001b[0m\u001b[2m      \u001b[0m\u001b[94melif\u001b[0m key_is_scalar:                                                               \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[31m \u001b[0m 981 \u001b[2m         \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m._get_value(key)                                                   \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 982 \u001b[0m\u001b[2m      \u001b[0m                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 983 \u001b[0m\u001b[2m      \u001b[0m\u001b[94mif\u001b[0m is_hashable(key):                                                              \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 984 \u001b[0m\u001b[2m         \u001b[0m\u001b[2m# Otherwise index.get_value will raise InvalidIndexError\u001b[0m                      \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[33mc:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m:\u001b[94m1\u001b[0m \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[94m089\u001b[0m in \u001b[92m_get_value\u001b[0m                                                                                \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1086 \u001b[0m\u001b[2m         \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m._values[label]                                                    \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1087 \u001b[0m\u001b[2m      \u001b[0m                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1088 \u001b[0m\u001b[2m      \u001b[0m\u001b[2m# Similar to Index.get_value, but we do not fall back to positional\u001b[0m               \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[31m \u001b[0m1089 \u001b[2m      \u001b[0mloc = \u001b[96mself\u001b[0m.index.get_loc(label)                                                   \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1090 \u001b[0m\u001b[2m      \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m.index._get_values_for_loc(\u001b[96mself\u001b[0m, loc, label)                           \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1091 \u001b[0m\u001b[2m   \u001b[0m                                                                                      \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1092 \u001b[0m\u001b[2m   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92m__setitem__\u001b[0m(\u001b[96mself\u001b[0m, key, value) -> \u001b[94mNone\u001b[0m:                                            \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[33mc:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\indexes\\bas\u001b[0m \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[33me.py\u001b[0m:\u001b[94m3804\u001b[0m in \u001b[92mget_loc\u001b[0m                                                                             \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m3801 \u001b[0m\u001b[2m         \u001b[0m\u001b[94mtry\u001b[0m:                                                                          \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m3802 \u001b[0m\u001b[2m            \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m._engine.get_loc(casted_key)                                   \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m3803 \u001b[0m\u001b[2m         \u001b[0m\u001b[94mexcept\u001b[0m \u001b[96mKeyError\u001b[0m \u001b[94mas\u001b[0m err:                                                       \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[31m \u001b[0m3804 \u001b[2m            \u001b[0m\u001b[94mraise\u001b[0m \u001b[96mKeyError\u001b[0m(key) \u001b[94mfrom\u001b[0m \u001b[4;96merr\u001b[0m                                              \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m3805 \u001b[0m\u001b[2m         \u001b[0m\u001b[94mexcept\u001b[0m \u001b[96mTypeError\u001b[0m:                                                             \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m3806 \u001b[0m\u001b[2m            \u001b[0m\u001b[2m# If we have a listlike key, _check_indexing_error will raise\u001b[0m             \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m3807 \u001b[0m\u001b[2m            \u001b[0m\u001b[2m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[0m              \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m\n",
       "\u001b[1;91mKeyError: \u001b[0m\u001b[32m'text'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch = BatchStatement(consistency_level=ConsistencyLevel.ONE)\n",
    "batch_size_limit = 16\n",
    "\n",
    "if 'tokens' not in data.columns:\n",
    "    data['tokens'] = None\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    row_id = uuid.uuid4()\n",
    "\n",
    "    # Ensure tokens are correctly formatted as a list of integers\n",
    "    tokens = tokenizer.encode(row['text'], add_special_tokens=True)\n",
    "    if not all(isinstance(token, int) for token in tokens):\n",
    "        # Handle the case where tokens are not all integers\n",
    "        # This might involve converting or handling errors\n",
    "        pass\n",
    "\n",
    "    label = str(row['label'])  # Ensure label is a string\n",
    "\n",
    "    batch.add(insert_statement, (row_id, tokens, label))\n",
    "\n",
    "    if len(batch) >= batch_size_limit:\n",
    "        session.execute(batch)\n",
    "        batch.clear()\n",
    "\n",
    "if len(batch) > 0:\n",
    "    session.execute(batch)\n",
    "\n",
    "print(\"Feature engineering and data insertion completed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents of table features:\n",
      "Row(id=UUID('191bfb7f-905f-4231-8818-8c81f37645ae'), label='1', tokens=[101, 1030, 5310, 1030, 5310, 1030, 5310, 1030, 5310, 1030, 5310, 2017, 2024, 2019, 7780, 2000, 2035, 1010, 2926, 2308, 2040, 2954, 10126, 2007, 2061, 2116, 2367, 15314, 999, 4067, 2017, 100, 1001, 17183, 8663, 15338, 3258, 1001, 7226, 2368, 8167, 6935, 11387, 11387, 102])\n",
      "Row(id=UUID('7037926b-c428-48f2-8af0-941a6b9b4be9'), label='2', tokens=[101, 1030, 5310, 3533, 7226, 2368, 1521, 1055, 4700, 2086, 5443, 8398, 2015, 4700, 2706, 1012, 1998, 8398, 2347, 1521, 1056, 1037, 3761, 1010, 2085, 2008, 1521, 1055, 16436, 2005, 2017, 3533, 1012, 2009, 2003, 2054, 2009, 2003, 2138, 3533, 7226, 2368, 2003, 2040, 2002, 2003, 1012, 102])\n",
      "Row(id=UUID('460c8904-e9f8-4a54-9c4e-a82373c9a933'), label='0', tokens=[101, 1030, 5310, 2053, 1010, 2027, 2024, 2145, 2045, 1012, 2030, 2062, 2031, 2042, 6866, 1012, 9733, 2038, 3314, 2007, 2023, 2828, 1997, 4031, 2035, 1996, 2051, 1012, 3531, 2562, 7316, 2068, 1012, 1001, 3789, 16558, 5657, 102])\n",
      "Row(id=UUID('dcf84c07-dd40-4b60-b518-e2bd23e7b98a'), label='2', tokens=[101, 1030, 5310, 1030, 5310, 1048, 2863, 2080, 1012, 1012, 1012, 2066, 2017, 2729, 2055, 2637, 1012, 1001, 8398, 11387, 11387, 1998, 2009, 2097, 2022, 1037, 4121, 2663, 999, 102])\n",
      "Row(id=UUID('151f5d06-665b-4655-a94b-1fab6c8cd2df'), label='2', tokens=[101, 1001, 3407, 18447, 11795, 3370, 27318, 5620, 10259, 1001, 3407, 25434, 16168, 10259, 1001, 3407, 18447, 11795, 3370, 27318, 2290, 10259, 1001, 28844, 25688, 4710, 1998, 1001, 2176, 5974, 29100, 2015, 2005, 1001, 6221, 24456, 2361, 100, 8299, 102])\n",
      "Row(id=UUID('f1602f3b-1979-4be8-aed9-8515a97bd966'), label='0', tokens=[101, 1030, 5310, 1030, 5310, 1030, 5310, 13366, 2571, 6593, 2172, 1029, 1996, 12043, 4366, 2002, 1042, 1011, 13749, 4282, 7226, 2368, 2987, 1521, 1056, 2031, 2009, 1998, 8040, 11614, 2015, 2500, 1012, 8299, 102])\n",
      "Row(id=UUID('323a8e17-062d-4070-9a0b-5505ab7b7e92'), label='0', tokens=[101, 7226, 2368, 2003, 3262, 18524, 2005, 1996, 2236, 2602, 1010, 2025, 20767, 15941, 1012, 1001, 3537, 3207, 20179, 102])\n",
      "Row(id=UUID('33375db0-94f2-4276-a22f-9158924612a9'), label='0', tokens=[101, 1045, 6655, 2065, 1001, 3013, 3111, 2001, 5868, 2011, 11992, 7249, 1001, 4314, 2865, 2052, 2131, 2037, 6510, 19896, 1998, 24711, 3201, 4214, 2041, 1001, 21877, 3527, 21850, 6632, 2021, 1997, 2607, 2025, 2138, 1996, 2088, 2003, 2743, 2011, 1001, 5305, 11263, 10603, 2040, 2228, 1996, 23760, 3366, 2595, 8787, 3989, 1997, 2336, 2003, 1001, 9191, 1012, 6616, 2017, 12731, 10603, 102])\n",
      "Row(id=UUID('8ced4083-3244-466c-bd03-51d4675325e5'), label='1', tokens=[101, 1001, 10047, 22994, 2075, 29278, 5558, 2063, 2138, 1045, 1005, 1049, 5458, 1997, 3773, 2023, 22078, 7377, 5443, 1012, 9018, 10231, 2012, 1001, 1015, 1001, 22078, 7377, 1001, 3533, 17062, 2368, 1001, 8398, 10199, 2102, 7698, 102])\n",
      "Row(id=UUID('8153183f-98c0-4a17-ae71-89959eb89576'), label='0', tokens=[101, 1030, 5310, 6554, 13970, 27002, 2003, 2498, 2021, 1037, 8398, 25353, 3597, 21890, 3372, 1012, 1001, 29300, 2278, 11387, 11387, 1001, 29300, 21408, 2078, 15338, 3258, 11387, 11387, 102])\n",
      "Row(id=UUID('bd4e3dd9-3710-45df-b097-148a4cc14c37'), label='1', tokens=[101, 1030, 5310, 23156, 1012, 2085, 2425, 2149, 2153, 2129, 8398, 4342, 2010, 10800, 2013, 5924, 2061, 1057, 2134, 1005, 1056, 2342, 2000, 17727, 5243, 2818, 2032, 1010, 2030, 2129, 10556, 27313, 8953, 2097, 4047, 1037, 2450, 1005, 1055, 2157, 2000, 5454, 1012, 1001, 3789, 16558, 5657, 102])\n",
      "Row(id=UUID('09cdd8c6-9238-466a-896c-d42af60f31c4'), label='2', tokens=[101, 1030, 5310, 1030, 5310, 8112, 2015, 27157, 1012, 8112, 1521, 1055, 2637, 1012, 2908, 999, 4283, 2000, 8398, 999, 1001, 23848, 2050, 102])\n",
      "Row(id=UUID('640527f7-d418-42f9-917c-1641c88f88b6'), label='0', tokens=[101, 2129, 2097, 7226, 2368, 2079, 20767, 8398, 1029, 2057, 2356, 2216, 2040, 2031, 4320, 1996, 2280, 21210, 8299, 2035, 7226, 2368, 3791, 2000, 3342, 2003, 2008, 8398, 2196, 12237, 2006, 2391, 1998, 2002, 2097, 13366, 2571, 6593, 1010, 9772, 1010, 1998, 3046, 2000, 2689, 1996, 7984, 1997, 2151, 3395, 3160, 1012, 6293, 2000, 5896, 1012, 102])\n",
      "Row(id=UUID('a2b39333-7b78-42cf-b726-c1fd9e8fd03d'), label='1', tokens=[101, 3752, 1056, 28394, 3215, 2013, 16360, 12083, 2015, 1012, 2055, 2129, 11771, 1996, 17183, 1012, 4680, 2001, 1012, 1045, 2387, 26452, 1998, 3246, 1012, 1045, 3866, 2009, 1012, 2025, 4394, 1996, 5223, 1010, 14398, 1010, 28616, 15707, 4890, 1998, 2171, 4214, 1012, 1045, 1005, 2222, 2202, 23997, 1010, 3606, 1998, 4847, 2151, 2154, 2058, 1996, 8398, 9661, 1012, 1001, 7226, 2368, 11387, 11387, 102])\n",
      "Row(id=UUID('ea902a1b-3554-4f1e-80c8-80c835fd0e6a'), label='1', tokens=[101, 1030, 5310, 2038, 2085, 15506, 2058, 1996, 4652, 3085, 6677, 1997, 2471, 5018, 2243, 4841, 1012, 2010, 3768, 1997, 1037, 2120, 2933, 1998, 1996, 2051, 2002, 13842, 2038, 4504, 1999, 2023, 8488, 1012, 2002, 2442, 2175, 1012, 1001, 7226, 2368, 11387, 11387, 102])\n",
      "Row(id=UUID('ab5cf957-8610-42f6-984a-84152157fafd'), label='0', tokens=[101, 7226, 2368, 2108, 7861, 21890, 4588, 2008, 2002, 2097, 4060, 1037, 2450, 21210, 1998, 15941, 11333, 4246, 2989, 3084, 2033, 2228, 7226, 2368, 2038, 2525, 3856, 1037, 21210, 1006, 5671, 1029, 1007, 1998, 15941, 2038, 2025, 1012, 1001, 17183, 3207, 20179, 102])\n",
      "Row(id=UUID('6c3e7d71-1a61-444d-811d-d9dc3ec0c757'), label='2', tokens=[101, 6203, 1030, 5310, 1011, 1996, 2062, 1057, 1004, 23713, 1025, 1001, 4314, 1001, 8037, 9772, 2008, 1001, 3424, 7011, 6526, 1004, 23713, 1025, 2024, 9846, 2256, 3655, 1010, 1996, 2062, 2111, 2097, 3789, 2005, 1001, 8398, 999, 2425, 2122, 2967, 2027, 2123, 1005, 1056, 4839, 1024, 1030, 5310, 1030, 5310, 1030, 5310, 1030, 5310, 1030, 5310, 1030, 5310, 1001, 4654, 3995, 2361, 102])\n",
      "Row(id=UUID('cd881527-d68a-4315-8199-1e427f8ccdc1'), label='0', tokens=[101, 1045, 1521, 2310, 2196, 2042, 1037, 2502, 1030, 5310, 5470, 2021, 1045, 2245, 2002, 2106, 1037, 3492, 2204, 3105, 1997, 2108, 1037, 26438, 1998, 11519, 5981, 2099, 1998, 2529, 3892, 4102, 2000, 1030, 5310, 1012, 1001, 5981, 11387, 11387, 102])\n",
      "Row(id=UUID('7989944f-25c2-468e-b922-3da8c6bb2432'), label='0', tokens=[101, 1030, 5310, 4983, 2045, 2003, 1037, 11476, 2135, 20080, 2765, 1037, 1011, 2474, 2456, 1010, 1045, 5987, 2477, 2000, 2175, 4659, 15299, 1010, 2748, 1012, 2070, 20543, 1998, 2070, 2942, 16591, 3383, 1010, 2021, 1045, 2123, 1005, 1056, 5987, 1996, 2406, 2000, 2991, 4237, 1059, 1037, 7226, 2368, 3377, 1012, 2065, 8398, 5222, 1010, 1045, 1005, 1049, 2625, 9657, 1012, 102])\n",
      "Row(id=UUID('5e8935d9-815e-47ba-8f19-f0eda0ab4365'), label='1', tokens=[101, 1030, 5310, 2009, 5807, 1005, 1056, 2031, 2000, 999, 2065, 1001, 3533, 17062, 2368, 2038, 2000, 2644, 2551, 1998, 4769, 2296, 5223, 2099, 2008, 3980, 2010, 5177, 4513, 2030, 2010, 6830, 2501, 2030, 2010, 4424, 6101, 9989, 2030, 2010, 2365, 1005, 1055, 18913, 2030, 2010, 2248, 2449, 24069, 2002, 1005, 1055, 2196, 2183, 2000, 2131, 2505, 2589, 999, 1001, 3789, 16558, 5657, 102])\n",
      "Row(id=UUID('4968e3ad-210f-4b11-a5bf-12435007341d'), label='2', tokens=[101, 1030, 5310, 2997, 12077, 2865, 2202, 7698, 1997, 4018, 3533, 7226, 2368, 2005, 1523, 11094, 22134, 7898, 1524, 1012, 7226, 2368, 2018, 2000, 4530, 2041, 1997, 1996, 2997, 3049, 4647, 1997, 2010, 11888, 13720, 2000, 2425, 1996, 3606, 1012, 4438, 3533, 1012, 2073, 1521, 1055, 1996, 2865, 2651, 1029, 2292, 1521, 1055, 8970, 1037, 2047, 4245, 2000, 1996, 2168, 2214, 3533, 1012, 100, 8299, 102])\n",
      "Row(id=UUID('457c6d37-32a3-4c6f-9f26-e3ab06b12e3b'), label='0', tokens=[101, 2028, 3951, 2040, 2499, 2006, 1996, 2355, 3078, 3049, 1024, 1523, 2065, 2017, 2020, 2005, 7226, 2368, 1010, 2017, 1521, 2128, 2145, 2005, 7226, 2368, 1012, 2065, 2017, 2020, 2005, 8398, 1010, 2017, 1521, 2128, 2145, 2005, 8398, 1012, 2065, 2017, 2234, 2046, 2023, 2025, 4209, 2040, 2000, 2490, 1010, 1045, 2123, 1521, 1056, 2113, 2054, 2003, 2183, 2000, 8054, 2017, 1012, 1524, 8299, 102])\n",
      "Row(id=UUID('8f33111f-2edd-4f36-8116-c0771a7ad9c1'), label='0', tokens=[101, 2054, 1037, 7098, 2617, 999, 999, 2381, 1999, 1996, 2437, 999, 2054, 1037, 3928, 4613, 999, 999, 1001, 3580, 28994, 5178, 3372, 8167, 6935, 1001, 21911, 23278, 18752, 2015, 1001, 2308, 23011, 1001, 3537, 8663, 15338, 3258, 1001, 2057, 3995, 12399, 5963, 1001, 21911, 23278, 18752, 2015, 11387, 11387, 1001, 1040, 12273, 1529, 8299, 102])\n",
      "Row(id=UUID('24da02d5-26fc-4000-8c1d-96bce7aee1a4'), label='0', tokens=[101, 3533, 1001, 7226, 2368, 2921, 3038, 2000, 1996, 2137, 2111, 2008, 2002, 2003, 3201, 2000, 1001, 5981, 2343, 1001, 8398, 1998, 3970, 1017, 1001, 2093, 14379, 2515, 3533, 1001, 7226, 2368, 9530, 18532, 15725, 2010, 6808, 2000, 1001, 10214, 1998, 1001, 4468, 2122, 14379, 2006, 1037, 1001, 5656, 2478, 1996, 1001, 21887, 23350, 2005, 2025, 5782, 2000, 3604, 1001, 17547, 102])\n",
      "Row(id=UUID('7004671e-8602-4e4a-9780-6db6587dfdb0'), label='0', tokens=[101, 2572, 1045, 1996, 2069, 2028, 2008, 4620, 7226, 2368, 2023, 2126, 2035, 1996, 2051, 1029, 1001, 17183, 3207, 20179, 8299, 102])\n",
      "Row(id=UUID('32c00d84-04de-4394-95f5-a5fb8678e098'), label='1', tokens=[101, 2339, 5947, 2003, 7989, 2007, 2522, 17258, 1010, 7071, 7233, 1010, 10690, 2326, 3314, 3388, 2278, 1012, 1012, 1012, 5035, 9579, 1004, 23713, 1025, 6285, 2072, 10728, 1012, 1012, 1012, 2119, 4092, 2012, 8398, 2015, 4680, 2023, 2733, 1012, 2228, 2055, 2009, 1012, 1001, 3789, 16558, 5657, 1001, 2933, 29337, 2099, 22994, 2063, 1001, 7226, 2368, 11387, 11387, 1001, 3828, 10760, 2271, 4523, 11387, 11387, 102])\n",
      "Row(id=UUID('a09095c4-d676-4eee-b191-e818fb3dd079'), label='2', tokens=[101, 1030, 5310, 17183, 2015, 2031, 15682, 2769, 28289, 1010, 2027, 2079, 2009, 2035, 2058, 1996, 2088, 1998, 2655, 2009, 4681, 1001, 8398, 11387, 11387, 102])\n",
      "Row(id=UUID('2186e72f-cb6c-4af2-906d-86c64878e40f'), label='2', tokens=[101, 1030, 5310, 1045, 1005, 1049, 2469, 2057, 1005, 2222, 2963, 1996, 5156, 27984, 27984, 27984, 5223, 8398, 27984, 27984, 27984, 16939, 8398, 27984, 27984, 3607, 27984, 1012, 1001, 8398, 19779, 5974, 21604, 22507, 1001, 10556, 2290, 1001, 23848, 2050, 102])\n",
      "Row(id=UUID('70405078-0bb8-4d8f-9c19-6c3605320d69'), label='2', tokens=[101, 1030, 5310, 2562, 10320, 2023, 2406, 2041, 1001, 23848, 2050, 102])\n",
      "Row(id=UUID('c1fcf4e4-cd9d-44fa-9745-3a5e2f12a326'), label='1', tokens=[101, 8398, 2003, 1037, 8040, 3286, 5017, 1012, 1012, 1012, 2119, 2010, 5952, 1998, 2010, 2118, 2002, 2003, 1037, 8040, 3286, 5017, 2017, 2145, 2079, 2025, 2644, 4637, 2068, 8398, 2003, 1037, 8040, 3286, 5017, 1010, 2119, 2010, 5952, 1998, 2010, 2118, 2024, 8040, 3286, 16862, 2017, 2145, 2079, 2025, 2507, 2039, 2115, 2490, 1001, 8398, 11387, 11387, 1001, 7226, 2368, 1001, 6734, 21572, 22199, 2015, 1001, 2304, 3669, 6961, 18900, 3334, 8299, 102])\n",
      "Row(id=UUID('6d2f6c4c-7979-4ca9-b6d5-27f422403224'), label='2', tokens=[101, 2204, 2851, 3507, 11579, 999, 2477, 2024, 2893, 4689, 1011, 2994, 2844, 999, 2009, 2097, 2131, 2488, 999, 1001, 23848, 2050, 1001, 8398, 11387, 11387, 102])\n",
      "Row(id=UUID('c9b4b9a3-11a8-4998-87a1-16e060a75974'), label='0', tokens=[101, 1996, 10643, 2552, 2003, 2065, 2027, 4995, 1521, 1056, 7703, 2015, 1012, 2027, 2831, 2055, 2035, 1996, 2919, 2477, 1999, 1996, 2088, 2085, 1998, 7499, 1996, 8037, 1012, 10643, 2024, 1999, 2491, 2157, 2085, 2061, 2129, 2064, 2027, 7499, 2673, 2747, 3308, 2006, 1996, 8037, 1029, 1001, 29300, 2278, 11387, 11387, 102])\n",
      "Row(id=UUID('d216e544-2c12-49bb-bc72-6f4b92994c7b'), label='0', tokens=[101, 10166, 7226, 2368, 2183, 2044, 15941, 2006, 4409, 1012, 1012, 1012, 1001, 17183, 3207, 20179, 1001, 17183, 3207, 20179, 11387, 11387, 1001, 3537, 3207, 20179, 2015, 102])\n",
      "Row(id=UUID('80b6ebe0-5f6e-4304-8836-bb38d5a663d2'), label='1', tokens=[101, 1030, 5310, 8224, 1024, 2129, 2000, 3789, 2220, 1999, 1000, 2115, 2110, 1000, 1998, 2131, 2009, 2589, 2004, 2574, 2004, 2825, 1012, 2057, 2031, 1037, 4611, 2000, 2393, 1996, 1057, 1012, 1055, 1012, 10690, 2326, 2832, 2122, 17069, 1998, 5547, 1996, 6911, 6221, 8398, 2003, 15734, 6885, 2006, 2122, 2270, 8858, 1012, 1001, 3789, 14644, 2135, 1001, 3789, 16558, 5657, 1001, 7226, 2368, 8167, 6935, 11387, 11387, 102])\n",
      "Row(id=UUID('2bca7a1f-d885-4aa9-aed7-7c709f2c73d9'), label='2', tokens=[101, 1030, 5310, 1030, 5310, 1030, 5310, 1030, 5310, 1030, 5310, 1030, 5310, 1030, 5310, 1030, 5310, 2017, 2812, 2066, 2035, 1996, 2355, 3972, 14499, 2008, 2018, 2115, 13593, 1998, 3478, 4018, 1030, 5310, 3045, 2011, 1037, 20148, 1999, 2296, 8554, 2130, 2046, 2602, 2305, 1029, 1030, 5310, 1001, 23848, 2050, 1001, 8398, 11387, 11387, 5292, 3270, 3270, 1010, 2156, 2017, 1999, 1001, 13045, 21784, 102])\n",
      "Row(id=UUID('39500c5e-23f4-4dcc-9607-53ecb79300ab'), label='2', tokens=[101, 1030, 5310, 4689, 1998, 17109, 2097, 2025, 2113, 2054, 2718, 7861, 2023, 2281, 999, 1001, 23848, 2050, 102])\n",
      "Row(id=UUID('9148537e-2823-4349-8ec7-609093047f32'), label='0', tokens=[101, 1030, 5310, 4593, 2070, 2111, 2024, 9943, 1011, 2216, 2168, 2111, 2763, 2342, 2488, 29114, 1998, 2020, 2763, 6830, 2005, 7226, 2368, 4312, 1012, 102])\n",
      "Row(id=UUID('9a52b081-3ca4-4f4d-8a25-4298c265caf1'), label='0', tokens=[101, 4897, 2655, 2408, 2637, 2003, 9788, 1998, 1045, 3246, 2057, 2196, 2644, 2725, 2009, 1012, 1006, 2027, 12057, 1996, 10656, 3233, 6279, 5751, 2000, 2169, 3295, 999, 1007, 1001, 17183, 8663, 15338, 3258, 102])\n",
      "Row(id=UUID('7ec6a2c1-4bb1-4107-a506-a2ff70d779cc'), label='0', tokens=[101, 1030, 5310, 1030, 5310, 2204, 2005, 2068, 1012, 1001, 8398, 11387, 11387, 102])\n",
      "Row(id=UUID('3bc1e648-8185-4248-bfc6-8548b43b0a00'), label='0', tokens=[101, 2023, 3179, 2003, 1037, 2442, 3422, 1012, 2054, 3632, 2006, 1999, 1996, 4680, 2077, 1996, 13762, 4664, 2003, 14726, 1012, 1001, 17183, 8663, 15338, 3258, 8299, 102])\n",
      "Row(id=UUID('90925763-c9c0-49d2-a411-f8567ee6cbb2'), label='2', tokens=[101, 1001, 8037, 2024, 10919, 2129, 3809, 2690, 1011, 2637, 2003, 2055, 1001, 2176, 5974, 29100, 2015, 1064, 1001, 8398, 11387, 11387, 1012, 102])\n",
      "Row(id=UUID('0f586097-39b1-4323-ac35-89690a9f31d9'), label='0', tokens=[101, 1030, 5310, 1030, 5310, 1030, 5310, 2003, 1037, 9861, 1012, 1001, 2196, 17062, 2368, 2001, 1037, 16939, 2000, 2014, 1037, 2261, 2460, 2706, 3283, 1010, 2085, 2016, 1521, 1055, 2010, 21210, 1012, 2016, 2036, 2064, 1521, 1056, 2448, 2006, 2014, 2501, 2004, 1001, 10722, 4877, 2072, 11387, 18827, 23263, 4197, 2041, 1999, 1996, 1001, 3537, 14379, 1012, 102])\n",
      "Row(id=UUID('3224313c-fa94-4630-9c87-fb922eaaed4d'), label='1', tokens=[101, 1030, 5310, 1998, 8398, 2180, 1521, 1056, 2022, 2583, 2000, 19815, 2105, 2369, 7226, 2368, 2066, 2002, 2106, 2000, 18520, 1012, 2002, 2038, 2000, 2994, 1999, 2010, 2219, 4644, 100, 102])\n",
      "Row(id=UUID('b29f8855-e8b8-4deb-9cca-9c4f06a91e78'), label='0', tokens=[101, 1030, 5310, 1030, 5310, 2122, 1001, 28988, 2015, 2024, 2074, 1001, 5881, 5644, 8525, 23267, 1001, 23848, 2050, 2040, 10214, 2000, 3582, 7899, 999, 2016, 10849, 2296, 2978, 1997, 2054, 2016, 2288, 999, 1001, 4929, 8067, 6711, 1001, 2994, 8988, 8462, 1001, 2591, 10521, 26897, 1001, 3582, 7076, 18300, 8496, 102])\n",
      "Row(id=UUID('54ef011d-3786-49e8-8761-7fc51c739493'), label='0', tokens=[101, 1030, 5310, 2204, 2739, 2029, 2097, 2582, 14451, 1996, 2137, 2187, 2004, 3061, 2005, 2498, 4902, 1010, 3272, 1996, 7654, 1997, 2373, 1012, 1045, 1521, 1049, 2214, 2438, 2000, 3342, 2043, 5025, 1523, 13350, 1524, 1006, 2025, 13009, 1007, 4941, 10866, 5233, 1998, 1996, 2510, 3919, 3375, 1012, 1001, 23848, 2050, 102])\n",
      "Row(id=UUID('34e61d50-b32e-47ca-ab3e-11fbdc63b095'), label='2', tokens=[101, 4931, 7226, 2368, 1045, 2572, 2025, 4452, 1997, 2115, 4295, 2522, 17258, 1045, 2097, 2514, 13726, 1998, 2007, 2062, 3246, 2007, 1001, 8398, 11387, 11387, 1998, 1045, 2113, 2073, 19337, 28100, 2003, 1998, 2009, 1521, 1055, 2025, 1996, 1050, 6278, 100, 102])\n",
      "Row(id=UUID('856a9aeb-12bb-4a71-8d28-96c1d288cc6a'), label='0', tokens=[101, 6904, 14194, 2072, 14300, 2015, 1999, 1011, 2711, 6830, 1024, 1520, 2045, 1521, 1055, 2053, 3114, 1521, 2339, 2057, 5807, 1521, 1056, 2022, 3039, 1064, 1996, 3679, 7318, 1001, 2637, 8873, 12096, 1001, 2035, 3669, 6961, 18900, 3334, 1001, 21887, 23350, 8299, 102])\n",
      "Row(id=UUID('983476e2-b17a-411f-80e1-cd7ce6fabb28'), label='0', tokens=[101, 1030, 5310, 1030, 5310, 2085, 1001, 2175, 2361, 1001, 2602, 4584, 11193, 2000, 2404, 2041, 8378, 2000, 8145, 1001, 5653, 2378, 7384, 12868, 2000, 4675, 9416, 5653, 23217, 2078, 1059, 8093, 1012, 1030, 5310, 2844, 2000, 16081, 3789, 1012, 2061, 2404, 3074, 8378, 2012, 17888, 3182, 27046, 5833, 2220, 6830, 1012, 1012, 1030, 5310, 102])\n",
      "Row(id=UUID('82bd304a-8e88-4d37-889b-aaee488a36b5'), label='0', tokens=[101, 1001, 10973, 27698, 2050, 1001, 10973, 2078, 10258, 1001, 10973, 25311, 2140, 10973, 2035, 16916, 19582, 4013, 7576, 1001, 2637, 8873, 12096, 102])\n",
      "Row(id=UUID('f777175f-a2dd-42a7-8246-0ffc6e40cd23'), label='1', tokens=[101, 1030, 5310, 2017, 2288, 2008, 2157, 1010, 2637, 2180, 1005, 1056, 2022, 25857, 1010, 2017, 1005, 2222, 2022, 2019, 1001, 17727, 5243, 7690, 2028, 2744, 1030, 5310, 1012, 1001, 2522, 17258, 16147, 1001, 8398, 2923, 5369, 12155, 12096, 28994, 5178, 10111, 6299, 1001, 3789, 16558, 23361, 8820, 3726, 14074, 14735, 1001, 3789, 16558, 24997, 9626, 12079, 2860, 6806, 8299, 102])\n",
      "Row(id=UUID('44740a74-e91b-4b0e-9a68-ba72e5a78d22'), label='1', tokens=[101, 1030, 5310, 4921, 2063, 3427, 2035, 1996, 15281, 2295, 1998, 8398, 2130, 2409, 2317, 10514, 28139, 23738, 2000, 1000, 3233, 2067, 1998, 3233, 2011, 1000, 2612, 1997, 7939, 23709, 6129, 2068, 1012, 102])\n",
      "Row(id=UUID('f692f8a5-92a0-4032-83c0-27aea8ff21d0'), label='1', tokens=[101, 1045, 8239, 5223, 8398, 1001, 14379, 11387, 11387, 102])\n",
      "Row(id=UUID('2363c437-b0be-4135-bc29-e4843e95e3f8'), label='0', tokens=[101, 1000, 2023, 2003, 1037, 2166, 1011, 5278, 2602, 1012, 2023, 2097, 5646, 2054, 2637, 2003, 2183, 2000, 2298, 2066, 2005, 1037, 2146, 1010, 2146, 2051, 1012, 1000, 1516, 1030, 5310, 1001, 17183, 8663, 15338, 3258, 8299, 102])\n",
      "Row(id=UUID('b1288585-7334-4663-9d0f-5b2dda43558f'), label='1', tokens=[101, 1001, 4883, 3207, 20179, 11387, 11387, 15757, 1999, 999, 3582, 2033, 2065, 2017, 1001, 3789, 16558, 5657, 2272, 2006, 7226, 2368, 999, 999, 8299, 102])\n",
      "Row(id=UUID('11153fc5-f724-4458-b151-5284c7cdf942'), label='0', tokens=[101, 1030, 5310, 1030, 5310, 100, 2111, 3280, 1999, 2637, 2013, 7870, 2008, 2024, 9526, 3085, 1998, 7438, 3085, 2138, 1997, 1002, 1999, 2637, 2057, 2024, 2489, 2000, 3280, 2061, 2500, 2064, 9991, 11772, 1012, 1001, 4071, 1001, 23848, 2050, 7743, 1012, 6770, 18424, 1012, 102])\n",
      "Row(id=UUID('e532d771-dea0-4e4b-a986-1fdda49e70db'), label='2', tokens=[101, 1030, 5310, 2092, 1997, 2607, 2031, 2017, 2156, 1996, 3616, 2105, 1996, 2088, 1029, 2003, 2035, 2765, 1997, 1996, 6090, 3207, 7712, 999, 2644, 1996, 5223, 999, 1001, 8398, 11387, 11387, 102])\n",
      "Row(id=UUID('c1165288-91f4-4a61-830c-530ca457ff89'), label='1', tokens=[101, 9826, 2064, 1030, 5310, 2022, 2343, 3892, 1029, 1001, 7226, 2368, 8167, 6935, 11387, 11387, 1001, 17183, 8663, 15338, 3258, 102])\n",
      "Row(id=UUID('1f9dda63-37fe-4b57-b1ed-8ed04c1f37ae'), label='1', tokens=[101, 1030, 5310, 1030, 5310, 1030, 5310, 1030, 5310, 21911, 2050, 5671, 2097, 2022, 1037, 3181, 21210, 999, 1037, 2613, 2450, 1999, 1996, 2317, 2160, 999, 2053, 2062, 22635, 1056, 28394, 3215, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1001, 15653, 24456, 2361, 8299, 102])\n",
      "Row(id=UUID('ab6c673e-4fdc-4923-9e84-1889f608a09f'), label='2', tokens=[101, 1030, 5310, 2017, 2031, 1015, 22399, 1010, 3582, 2053, 2028, 1998, 2031, 2053, 27263, 1012, 1012, 1012, 1012, 2115, 5448, 5609, 2339, 1029, 1001, 8398, 11387, 11387, 1001, 1059, 27767, 2487, 27767, 2050, 102])\n",
      "Row(id=UUID('0b80fc44-e9ef-411b-a571-b0ff9c49aba8'), label='1', tokens=[101, 1001, 9779, 8197, 13791, 2003, 3145, 2000, 3045, 1996, 1001, 12609, 12260, 7542, 999, 3191, 2062, 2182, 2055, 2129, 2057, 2064, 2490, 1001, 7226, 2368, 8167, 6935, 11387, 11387, 1012, 1030, 5310, 1030, 5310, 1030, 5310, 1030, 5310, 1001, 9779, 8197, 11387, 11387, 1001, 9779, 6593, 21967, 8299, 102])\n",
      "Row(id=UUID('73038b17-6626-4378-bc8f-8416acd0902e'), label='0', tokens=[101, 2035, 2023, 2051, 1045, 1521, 2310, 5191, 7226, 2368, 2001, 1037, 19813, 24247, 7490, 2021, 16047, 1996, 1040, 12273, 1521, 1055, 3247, 2000, 12944, 2010, 4434, 2007, 4603, 12505, 1998, 2037, 16730, 2005, 2032, 2038, 26350, 2033, 2010, 12912, 2015, 2007, 1996, 2187, 2024, 10124, 2000, 2512, 1011, 25953, 1001, 3537, 25434, 8663, 15338, 3258, 102])\n",
      "Row(id=UUID('8fef9af2-d99e-4443-a1df-f9f7f3fee454'), label='0', tokens=[101, 7825, 5363, 13599, 4126, 1999, 17183, 3655, 2007, 2175, 2361, 2448, 3655, 1006, 2025, 2469, 10094, 2023, 2391, 1007, 3533, 2667, 2000, 2831, 2055, 9435, 2021, 2025, 2437, 3168, 1012, 7825, 5176, 3533, 2055, 24964, 2863, 11528, 2075, 21107, 1050, 1038, 13728, 3533, 2758, 6135, 4941, 2000, 13366, 8630, 2075, 2610, 1010, 7499, 2015, 8398, 2005, 6276, 2037, 26178, 1012, 102])\n",
      "Row(id=UUID('5172ad59-7c12-410a-a792-b3f3f6b04466'), label='1', tokens=[101, 1030, 5310, 100, 1030, 5310, 2003, 1037, 2200, 1001, 4795, 1001, 2343, 1012, 2002, 2003, 100, 1996, 10442, 4920, 1012, 2002, 2003, 2025, 5214, 2000, 2599, 2002, 2069, 4282, 2129, 2000, 1001, 20716, 100, 1001, 6221, 24456, 2361, 2069, 14977, 2055, 2370, 1012, 2053, 2028, 100, 1001, 8398, 14268, 10483, 2121, 1001, 8398, 3334, 29165, 2964, 3342, 1001, 3789, 100, 1001, 7226, 2368, 8167, 15061, 8820, 3726, 14074, 14735, 102])\n",
      "Row(id=UUID('f66c66e5-dc8e-4ba4-ad33-56c056f08be7'), label='1', tokens=[101, 2026, 2540, 2003, 14107, 999, 2196, 2042, 12774, 999, 1001, 2175, 16558, 5657, 1001, 1040, 12273, 11387, 11387, 1001, 7226, 2368, 8167, 6935, 11387, 11387, 1001, 2131, 14615, 11253, 3995, 13876, 14995, 6591, 8299, 102])\n",
      "Row(id=UUID('2cac1dfd-6f63-4b09-aaa1-37880a3ebf16'), label='0', tokens=[101, 1030, 5310, 1030, 5310, 1030, 5310, 1030, 5310, 1030, 5310, 1030, 5310, 1030, 5310, 1030, 5310, 1030, 5310, 1030, 5310, 1030, 5310, 1030, 5310, 1030, 5310, 1030, 5310, 1030, 5310, 1030, 5310, 12475, 1996, 11963, 999, 2023, 2005, 1000, 13593, 2576, 5069, 1000, 999, 999, 999, 2057, 1996, 2111, 24104, 2491, 2058, 2256, 2231, 1001, 23848, 2050, 1001, 23848, 2050, 1030, 5310, 8299, 102])\n",
      "Row(id=UUID('998e0847-7935-4ea2-84d1-12f2f4039362'), label='2', tokens=[101, 1030, 5310, 1030, 5310, 1001, 3533, 17062, 2368, 1998, 2228, 1999, 1996, 2168, 6251, 1012, 2008, 2428, 2003, 1037, 7683, 1011, 1001, 3835, 11129, 1001, 9857, 2705, 10593, 3215, 1001, 8275, 2638, 9333, 1001, 29450, 8299, 102])\n",
      "Row(id=UUID('441af050-b031-4cce-9ac5-df6c9d144953'), label='1', tokens=[101, 1030, 5310, 6616, 1012, 1012, 1012, 1012, 1012, 2448, 999, 999, 1998, 1001, 3789, 16558, 24997, 9626, 12079, 2860, 6806, 102])\n",
      "Row(id=UUID('6447a7ff-c473-4ac9-8846-77e663e2ab90'), label='2', tokens=[101, 3374, 6733, 1010, 6904, 14194, 2072, 1004, 23713, 1025, 17183, 2015, 1010, 2023, 4485, 2573, 1012, 2057, 2113, 1057, 2215, 2062, 6677, 2000, 2139, 16340, 9869, 1004, 23713, 1025, 2000, 2191, 8398, 2298, 2919, 1010, 2021, 2004, 2062, 2111, 2202, 2009, 1004, 23713, 1025, 5256, 2039, 2115, 2933, 2003, 2108, 100, 1001, 8398, 11387, 11387, 1001, 1053, 6761, 3619, 8299, 102])\n",
      "Row(id=UUID('dd68a8a3-55ed-4e31-a573-e4128768d75e'), label='0', tokens=[101, 1053, 2620, 1024, 2197, 3160, 1012, 2559, 3805, 1010, 2054, 3160, 2006, 4517, 3314, 2024, 2017, 5327, 1996, 5347, 2097, 2022, 2356, 1999, 1996, 1001, 17183, 3207, 20179, 4826, 1029, 1001, 5947, 28014, 2015, 11231, 9681, 102])\n",
      "Row(id=UUID('e2f8ce80-aa22-42ef-a214-bb8597a70724'), label='1', tokens=[101, 1012, 1030, 5310, 2006, 2522, 17258, 1011, 2539, 1024, 2057, 1005, 2222, 2404, 1996, 4331, 4998, 1012, 2057, 1005, 2222, 2202, 1996, 17750, 2125, 1996, 8519, 2061, 1996, 1996, 2270, 4152, 1996, 2592, 2027, 2342, 1998, 10107, 1012, 7481, 4895, 10755, 28357, 3606, 1012, 2027, 2064, 5047, 2009, 1012, 1001, 17183, 8663, 15338, 3258, 1030, 5310, 8299, 102])\n",
      "Row(id=UUID('d47162a9-21c6-48ef-b4f3-ead93e3f3f68'), label='0', tokens=[101, 15887, 2256, 2495, 2291, 2097, 23216, 1996, 4610, 1998, 3073, 2488, 13105, 2005, 4841, 1012, 2008, 1005, 1055, 2339, 6864, 1047, 4135, 25987, 2906, 2097, 2191, 2028, 1011, 1998, 2048, 1011, 2095, 2451, 2267, 3454, 2489, 1012, 1001, 17183, 3207, 20179, 8299, 102])\n",
      "Row(id=UUID('ca93282f-480e-4b19-ade5-9bd71e8de220'), label='0', tokens=[101, 1030, 5310, 1001, 15941, 2038, 2664, 2000, 10797, 2010, 4121, 17053, 1997, 10284, 1012, 15274, 1001, 1040, 12273, 1002, 11503, 1001, 3565, 9247, 29107, 4570, 1002, 1061, 13473, 2213, 2031, 12225, 2039, 2045, 10353, 2074, 1999, 2553, 15941, 1004, 23713, 1025, 1001, 6887, 16585, 8167, 6935, 1005, 2184, 1002, 1051, 13247, 2923, 4132, 2685, 2131, 7463, 1012, 1001, 15941, 8167, 6935, 13675, 11149, 2215, 2000, 2022, 1996, 13997, 23029, 2369, 1001, 4030, 5558, 2063, 999, 1001, 1038, 2571, 9048, 2102, 11387, 11387, 999, 8299, 102])\n",
      "Row(id=UUID('be864ee7-26b2-407d-9bc2-b76d83ecdf68'), label='1', tokens=[101, 8398, 1005, 1055, 4760, 2008, 2002, 2003, 2025, 4906, 2000, 2022, 3003, 1997, 1037, 2879, 7464, 10123, 1010, 2292, 2894, 2343, 1997, 1996, 2142, 2163, 1012, 1001, 14379, 11387, 11387, 1001, 4883, 3207, 20179, 11387, 11387, 1001, 4883, 3207, 20179, 1001, 8398, 11387, 11387, 102])\n",
      "Row(id=UUID('6b01c9eb-7c19-4952-adc0-567dfebadd83'), label='1', tokens=[101, 1030, 5310, 3114, 2070, 13109, 10050, 11692, 2015, 2655, 2032, 3099, 6677, 4630, 2483, 999, 2129, 2064, 1030, 5310, 2130, 2228, 2374, 2043, 4841, 1054, 5996, 1010, 2116, 5604, 3893, 1004, 23713, 1025, 2008, 2950, 2336, 999, 3516, 1005, 1055, 1001, 1005, 1055, 1054, 2025, 2183, 2091, 3435, 2438, 1018, 2023, 18667, 999, 1001, 3789, 16558, 23361, 8820, 3726, 14074, 14735, 2013, 8398, 1004, 23713, 1025, 6677, 4630, 2483, 102])\n",
      "Row(id=UUID('a7425a07-518e-43ff-9429-7ab5addb0c14'), label='1', tokens=[101, 1030, 5310, 1030, 5310, 2008, 2052, 2022, 3835, 999, 1001, 3789, 16558, 23361, 8820, 3726, 14074, 14735, 102])\n",
      "Row(id=UUID('53544cd7-ddc1-47f5-9fad-2c6da9fdcfc8'), label='1', tokens=[101, 1030, 5310, 6300, 2050, 2054, 2428, 29421, 2033, 2003, 8398, 2003, 3497, 2000, 2031, 1037, 2602, 2154, 2599, 2013, 2602, 2154, 3789, 1998, 8398, 2097, 2022, 9577, 1998, 4366, 3377, 5653, 1999, 4494, 1019, 2000, 1015, 2005, 7226, 2368, 1012, 102])\n",
      "Row(id=UUID('75af7c58-c857-4ac4-b9ea-68bd60f66a3c'), label='2', tokens=[101, 1030, 5310, 3422, 2023, 8554, 10436, 2066, 3894, 2320, 8398, 2058, 15166, 2015, 7226, 2368, 1012, 8299, 102])\n",
      "Row(id=UUID('b7c5b1cc-89bd-4c60-a123-6eb0d9b5e036'), label='0', tokens=[101, 1045, 2428, 2228, 1996, 1001, 17183, 3207, 20179, 3065, 2129, 3144, 1030, 5310, 1998, 1030, 5310, 2064, 2022, 2006, 2154, 2028, 1012, 1045, 2228, 2027, 2052, 2022, 2200, 3144, 2000, 26964, 1996, 2783, 5325, 1997, 18906, 2015, 1998, 1041, 24290, 102])\n",
      "Row(id=UUID('a7f81e41-4c70-4fe3-8c89-72ae34358c07'), label='0', tokens=[101, 1030, 5310, 1030, 5310, 2307, 3232, 999, 1001, 2157, 1001, 10556, 2290, 1001, 12925, 1001, 2358, 23743, 2483, 3597, 6279, 2571, 102])\n",
      "Row(id=UUID('2c410006-5b88-4167-8e0a-eef950640ee6'), label='1', tokens=[101, 1030, 5310, 5796, 1012, 5143, 2003, 2019, 6151, 8586, 14097, 14303, 2021, 2035, 1996, 2477, 2016, 2001, 6314, 2055, 2020, 3303, 2011, 1996, 2200, 2711, 2016, 3271, 2404, 1999, 2436, 999, 2054, 2062, 2515, 2016, 2342, 2000, 2156, 1029, 2065, 8380, 1010, 2199, 4903, 2140, 2052, 2031, 2351, 2104, 1037, 17183, 1012, 2008, 2052, 2022, 1996, 2203, 2025, 2000, 5254, 3105, 6409, 999, 1001, 3789, 16558, 5657, 102])\n",
      "Row(id=UUID('8b50197c-d98c-4965-89e4-3ee4b32bf28f'), label='1', tokens=[101, 1001, 2149, 28994, 5178, 19909, 3207, 20179, 11387, 11387, 2054, 1037, 4485, 2265, 1012, 2128, 1024, 8398, 1010, 2009, 2001, 2066, 3666, 1037, 2775, 2383, 1037, 2048, 3178, 21396, 2243, 1998, 9092, 24456, 100, 1001, 3789, 16558, 5657, 2637, 999, 102])\n",
      "Row(id=UUID('53f6304c-847f-443e-a6e2-2d0255e7925a'), label='1', tokens=[101, 1045, 2071, 2031, 2042, 8854, 7632, 9516, 3372, 1012, 1001, 3425, 29278, 9336, 8540, 10606, 16885, 1001, 21911, 23278, 18752, 2015, 2615, 2361, 1001, 3533, 17062, 2368, 29278, 28994, 5178, 3372, 11387, 11387, 1001, 5035, 6673, 8883, 14204, 1001, 6221, 24456, 2361, 1001, 1996, 8873, 3726, 1001, 18281, 4904, 8151, 8299, 102])\n",
      "Row(id=UUID('c6c76bc8-b000-4a87-915e-fd563abba7c8'), label='0', tokens=[101, 2005, 2033, 1010, 1996, 2087, 17075, 2930, 1997, 1030, 5310, 1521, 1055, 9920, 4613, 1024, 1523, 2176, 3181, 25332, 1012, 2035, 2012, 1996, 2168, 2051, 1012, 1037, 3819, 4040, 1012, 1996, 5409, 6090, 3207, 7712, 1999, 2058, 2531, 2086, 1012, 1996, 5409, 3171, 5325, 2144, 1996, 2307, 6245, 1012, 1524, 1001, 17183, 8663, 15338, 3258, 1006, 1015, 1013, 1016, 1007, 102])\n",
      "Row(id=UUID('15f0f7ee-96fb-4438-b9fe-ec482190afc1'), label='2', tokens=[101, 1030, 5310, 1030, 5310, 2087, 4487, 11365, 3512, 1013, 13593, 2343, 1999, 2381, 1001, 8112, 5867, 1001, 8398, 11387, 11387, 102])\n",
      "Row(id=UUID('e4e0c91b-a1c5-4ac7-9edd-bca42b80989c'), label='2', tokens=[101, 1045, 2215, 2000, 5463, 2028, 1997, 8398, 2015, 1001, 9379, 21572, 22199, 1030, 5310, 1001, 8398, 11387, 11387, 1001, 8398, 1001, 8398, 23654, 11387, 11387, 1001, 2190, 28994, 5178, 10111, 6299, 19961, 1001, 3429, 1001, 8398, 11387, 11387, 8653, 24198, 1001, 2008, 6491, 22571, 6072, 5178, 3372, 1001, 4067, 29337, 2213, 14536, 6072, 5178, 3372, 1001, 4642, 2239, 2386, 1001, 1018, 5974, 2549, 24456, 2361, 1001, 2637, 4213, 3489, 6279, 1001, 7226, 10497, 13665, 2401, 1001, 1018, 5974, 29100, 2015, 1001, 8398, 22123, 6824, 7685, 1001, 2293, 8029, 28139, 14326, 4765, 100, 102])\n",
      "Row(id=UUID('0c5b75a6-8f1d-46f5-a066-8e044a0fc8ae'), label='0', tokens=[101, 15941, 12055, 8847, 2012, 1996, 3537, 2120, 4680, 3892, 1030, 5310, 1030, 5310, 1001, 15941, 8791, 13375, 1001, 3537, 8663, 15338, 3258, 1001, 1040, 12273, 11387, 11387, 8299, 102])\n",
      "Row(id=UUID('acecd87d-824a-41e8-bc3a-5a44c2aeed13'), label='0', tokens=[101, 2339, 2052, 3087, 3789, 2005, 1037, 4018, 2008, 2758, 2037, 3789, 5807, 1521, 1056, 4175, 1029, 1029, 1029, 1029, 1029, 10507, 1024, 1030, 5310, 1030, 5310, 1030, 5310, 1030, 5310, 1030, 5310, 1001, 20138, 2098, 8663, 15338, 3258, 1001, 3537, 3207, 20179, 1001, 17183, 3207, 20179, 2015, 1001, 3537, 3207, 20179, 2015, 1001, 17183, 3207, 20179, 11387, 11387, 102])\n",
      "Row(id=UUID('3806a1f6-6f36-415c-8cea-322d5ea4cb2d'), label='2', tokens=[101, 8037, 2215, 1996, 1002, 5174, 12163, 2769, 2000, 2994, 2045, 2061, 2037, 12925, 1045, 2812, 1523, 8090, 1524, 2994, 6787, 1998, 2122, 22822, 5644, 2123, 1521, 1056, 2031, 2000, 2147, 1012, 1001, 8398, 11387, 11387, 1001, 8037, 12069, 6155, 13181, 14147, 14074, 14735, 102])\n",
      "Row(id=UUID('29630b7a-097e-4790-b50d-05627087d0c6'), label='1', tokens=[101, 1045, 2081, 2469, 1045, 2128, 12155, 8630, 1998, 2680, 2008, 4861, 8398, 2056, 2096, 2196, 2412, 28525, 2317, 22006, 1012, 1012, 1523, 7098, 3337, 1012, 1012, 1012, 1012, 1012, 3233, 2067, 1998, 3233, 2011, 1524, 4283, 6221, 999, 999, 999, 15488, 16425, 999, 999, 999, 999, 999, 2065, 1061, 1521, 2035, 2134, 1521, 1056, 2113, 1010, 1010, 1010, 1010, 1010, 2085, 2017, 2113, 1001, 5981, 11387, 11387, 1001, 4883, 3207, 20179, 1001, 2136, 17062, 2368, 102])\n",
      "Row(id=UUID('1226e4ee-2abe-4c40-8b70-61cfdd80fe14'), label='1', tokens=[101, 2023, 7427, 1997, 4464, 4790, 2006, 21887, 1010, 4331, 1010, 5446, 2739, 2064, 2022, 2179, 2012, 8299, 1037, 2655, 2000, 7308, 2039, 1024, 7226, 2368, 23876, 2163, 2000, 11405, 1010, 4841, 2000, 4929, 2068, 8299, 1001, 3789, 2630, 1001, 17183, 2015, 1001, 7226, 2368, 1001, 2136, 5558, 2063, 1001, 9293, 16211, 2094, 1001, 3533, 17062, 2368, 1001, 7226, 2368, 29278, 28994, 5178, 3372, 102])\n",
      "Row(id=UUID('c41f8d5b-e000-4374-a18a-52f9775fa88b'), label='0', tokens=[101, 1030, 5310, 5292, 6203, 8840, 16585, 24247, 3571, 12256, 4590, 2075, 1001, 19971, 8566, 3600, 5172, 9875, 1030, 5310, 1012, 1012, 1012, 2069, 3160, 2003, 1010, 2043, 2038, 2637, 1005, 1055, 6716, 2025, 2018, 1037, 17284, 2006, 2137, 3629, 1013, 3268, 1029, 1001, 8244, 13028, 6824, 2361, 1001, 8244, 1001, 10556, 2290, 1001, 2510, 1001, 3915, 10867, 2100, 1001, 3915, 4313, 14821, 1001, 15529, 2290, 1001, 2149, 2532, 10736, 1001, 2149, 12458, 1001, 2610, 1001, 2244, 14526, 2705, 1001, 20720, 8299, 102])\n",
      "Row(id=UUID('e3a4723a-3899-428a-9835-7072dc65d4f6'), label='2', tokens=[101, 8398, 1037, 2307, 3003, 2003, 1037, 2204, 19373, 1001, 14379, 11387, 11387, 102])\n",
      "Row(id=UUID('6fa83f43-505e-4954-9667-23004ded3af3'), label='2', tokens=[101, 7226, 2368, 2525, 4165, 9069, 102])\n",
      "Row(id=UUID('d7653ad9-9d88-4700-8c98-2b8bf501c9cc'), label='0', tokens=[101, 2065, 2057, 2024, 2183, 2000, 4047, 2256, 4230, 2059, 2057, 2064, 1521, 1056, 11160, 2006, 5434, 1012, 2057, 2342, 6094, 2000, 4047, 2308, 1521, 1055, 2157, 2000, 3601, 1012, 1001, 17183, 3207, 20179, 2015, 1001, 3537, 3207, 20179, 8299, 102])\n",
      "Row(id=UUID('e092cd85-ed2a-4990-a3aa-a71ab62e5baf'), label='2', tokens=[101, 2044, 2305, 1015, 1997, 1996, 3537, 4750, 2283, 4680, 1010, 2009, 1521, 1055, 24509, 2084, 2412, 2077, 2054, 2027, 1521, 2128, 3038, 2000, 2149, 1012, 3789, 2005, 2149, 2030, 2054, 1521, 1055, 2042, 6230, 1999, 6734, 1010, 5862, 1010, 1998, 3190, 2097, 2131, 1037, 2843, 4788, 1012, 1001, 2203, 15222, 26212, 2094, 2791, 1001, 8398, 11387, 11387, 102])\n",
      "Row(id=UUID('8bfd53ad-3519-4617-9aed-c0f5e81340e6'), label='0', tokens=[101, 1030, 5310, 2002, 1005, 1055, 2005, 2505, 2008, 2097, 2562, 1996, 2270, 2013, 3773, 1996, 11113, 7274, 9067, 3737, 1997, 1996, 7672, 7281, 1012, 1001, 14379, 2097, 3584, 1996, 10500, 1998, 1996, 15775, 4246, 1012, 102])\n",
      "Row(id=UUID('6ed82376-452e-4717-8045-5a76438feaf7'), label='0', tokens=[101, 1045, 3046, 2000, 4468, 7603, 2006, 2591, 2865, 1010, 2664, 1045, 2215, 2000, 6978, 999, 1045, 1521, 1049, 2061, 5305, 1997, 1037, 2343, 4039, 2000, 2330, 2010, 2404, 14615, 2677, 2302, 11273, 2000, 1996, 2391, 1997, 29290, 3571, 1012, 15941, 1010, 2485, 2115, 2677, 999, 2017, 2614, 10785, 2066, 8398, 999, 1030, 5310, 100, 1001, 17183, 3207, 20179, 102])\n",
      "Row(id=UUID('2cc1011e-e5c6-4cee-8ef6-a6fd7568a1a8'), label='1', tokens=[101, 1030, 5310, 2017, 2081, 1037, 8257, 1997, 1996, 2436, 1997, 1996, 8798, 1012, 2023, 2347, 1005, 1056, 1037, 5981, 1010, 2017, 2081, 2009, 1037, 9661, 1012, 1996, 2069, 3924, 2187, 2024, 1996, 2145, 7144, 2006, 1996, 12849, 6030, 3593, 8754, 3429, 1012, 1012, 1012, 1012, 1998, 2130, 2070, 1997, 2068, 2024, 3810, 1012, 2031, 2070, 13372, 1998, 12897, 999, 1001, 7226, 2368, 19291, 23816, 15878, 3686, 1001, 5981, 11387, 11387, 1001, 8398, 7011, 18450, 102])\n",
      "Row(id=UUID('a6fcd395-dcd0-47e4-ac1a-872b9a440bbc'), label='0', tokens=[101, 2272, 2006, 1010, 3533, 1012, 6343, 1999, 2037, 2157, 2568, 4122, 2000, 2562, 2037, 2783, 5427, 1012, 2040, 4122, 2000, 2131, 5305, 1010, 2059, 2036, 2131, 1996, 2794, 6781, 1997, 2383, 2000, 3477, 2005, 2009, 1029, 1001, 17183, 3207, 20179, 102])\n",
      "Row(id=UUID('81c0f82a-5925-421b-84ba-57d1bb547eb9'), label='0', tokens=[101, 1030, 5310, 2003, 4013, 22776, 4808, 2007, 4330, 17071, 4765, 17871, 1012, 1001, 29300, 2278, 11387, 11387, 102])\n",
      "Row(id=UUID('01523e8c-dc91-4d87-9168-ec4cb75a9695'), label='0', tokens=[101, 1030, 5310, 3926, 2032, 999, 1001, 17183, 3207, 20179, 102])\n",
      "Row(id=UUID('66393de6-6642-4230-918a-e04655a97505'), label='2', tokens=[101, 1030, 5310, 1030, 5310, 1001, 17183, 8663, 15338, 3258, 2074, 2056, 2002, 2097, 10468, 6100, 1030, 5310, 3426, 2002, 2038, 2053, 4784, 1997, 2010, 2219, 1012, 1001, 2602, 11387, 11387, 2036, 1001, 3915, 2003, 2172, 7046, 1996, 2060, 3032, 2061, 2748, 2057, 2052, 2599, 1996, 2088, 1999, 1001, 21887, 23350, 3572, 1012, 4606, 1030, 5310, 2134, 1521, 1056, 2215, 2000, 2485, 2604, 2545, 1010, 8398, 2106, 2220, 999, 102])\n",
      "Row(id=UUID('c004f32a-ba1b-4858-96e7-3b814ef118cf'), label='2', tokens=[101, 1030, 5310, 1030, 5310, 1045, 3246, 2008, 1005, 1055, 2157, 999, 1001, 14303, 2098, 13122, 10696, 14074, 14735, 1001, 8398, 11387, 11387, 1001, 6181, 15455, 8791, 18927, 4402, 15455, 2015, 102])\n",
      "Row(id=UUID('c41a6a3b-e7ae-4726-984a-d8fd798ec111'), label='0', tokens=[101, 1030, 5310, 1996, 6714, 1030, 5310, 2003, 2667, 2000, 2079, 2006, 2023, 5981, 2003, 9951, 999, 1001, 3870, 9028, 7389, 2743, 2185, 2007, 2023, 5981, 1012, 1001, 17183, 3207, 20179, 102])\n",
      "Row(id=UUID('2b4b9e77-8a65-42d1-ac27-c86f515343f1'), label='2', tokens=[101, 1030, 5310, 4459, 2332, 3268, 1999, 1996, 13132, 4224, 1997, 2010, 17183, 14088, 2568, 1012, 1001, 26505, 2923, 5886, 15879, 9739, 3207, 7712, 1001, 8398, 11387, 11387, 102])\n",
      "Row(id=UUID('a0a3210d-5cf8-4f93-b53a-f3f35de38c68'), label='2', tokens=[101, 1030, 5310, 1030, 5310, 2059, 2339, 2025, 3499, 2370, 2000, 2022, 7039, 2061, 2009, 2064, 2022, 20119, 2011, 1037, 2353, 2283, 2008, 7226, 2368, 2003, 2025, 2108, 7349, 6998, 2030, 7197, 1999, 2151, 2126, 1029, 102])\n",
      "Row(id=UUID('a8d50dd8-3e43-4dd3-8fd3-b9e2db79467e'), label='1', tokens=[101, 1030, 5310, 1030, 5310, 2003, 2019, 20676, 20716, 2040, 2074, 7566, 2058, 2111, 2138, 2002, 2038, 2053, 9789, 1012, 1001, 14379, 11387, 11387, 1001, 3789, 5558, 15878, 5178, 2078, 1001, 8398, 5302, 4948, 102])\n",
      "Row(id=UUID('e85beda0-51ec-4d11-91c7-db41591e595a'), label='1', tokens=[101, 1030, 5310, 1030, 5310, 2129, 2172, 2062, 3350, 2515, 1996, 2088, 2342, 2008, 8398, 1998, 1996, 2610, 2024, 7694, 24083, 1012, 8090, 1999, 2637, 2024, 2467, 9379, 2127, 1996, 2610, 2265, 2039, 1012, 2059, 1996, 4808, 4627, 2007, 2068, 7866, 1010, 3342, 2023, 2154, 1998, 1001, 3789, 16558, 23361, 8820, 3726, 14074, 14735, 102])\n",
      "Row(id=UUID('c494f34f-1350-4b6d-82e2-639091017eef'), label='1', tokens=[101, 2057, 2342, 4177, 2040, 14977, 2005, 2035, 1997, 1996, 2137, 2111, 1012, 2057, 1521, 2128, 2025, 2019, 23949, 2406, 1010, 2057, 2123, 1521, 1056, 2031, 1037, 2332, 100, 1010, 1998, 2637, 2038, 2467, 2018, 1037, 2343, 1012, 2065, 2057, 1001, 3789, 17062, 2368, 1010, 2057, 2097, 3828, 2256, 2406, 1012, 2123, 1521, 1056, 2507, 2185, 2256, 7072, 1012, 1001, 3789, 16558, 5657, 1001, 3789, 16558, 23361, 8820, 3726, 14074, 14735, 8299, 102])\n",
      "Row(id=UUID('14f568fd-47c8-4da0-8f26-ff56378c8213'), label='0', tokens=[101, 4905, 1998, 9568, 2923, 14089, 5416, 2072, 2985, 2172, 1997, 2014, 4613, 21289, 3533, 7226, 2368, 1998, 2010, 2155, 1012, 2016, 2001, 2036, 2028, 1997, 1996, 2261, 1001, 29300, 2278, 11387, 11387, 7492, 2000, 2203, 2014, 4613, 2007, 2592, 2005, 7193, 2006, 2129, 2000, 4236, 2000, 3789, 1012, 8299, 102])\n",
      "Row(id=UUID('a8f37e78-1644-4494-8282-443d03c2ccd8'), label='2', tokens=[101, 1030, 5310, 1045, 2228, 2035, 1996, 1520, 26122, 1521, 2006, 1996, 7226, 2368, 3049, 2420, 2003, 2068, 2667, 2000, 2424, 1996, 2190, 4319, 18901, 2000, 3499, 7226, 2368, 2000, 2298, 2741, 11638, 1012, 3979, 1998, 7561, 1012, 102])\n",
      "Row(id=UUID('e710f184-97a4-4376-8199-813ef9a11ad7'), label='2', tokens=[101, 1030, 5310, 10825, 2149, 1997, 2129, 5367, 21572, 20614, 2003, 3561, 2007, 1037, 9129, 1997, 13009, 2008, 4299, 2331, 2006, 2637, 1029, 2057, 2113, 2008, 2525, 1012, 1001, 8398, 11387, 11387, 102])\n",
      "Row(id=UUID('40b16215-7c25-4799-971d-2128ba5d8612'), label='2', tokens=[101, 1030, 5310, 14649, 2003, 2054, 1996, 3537, 2283, 2003, 2667, 2000, 3288, 2091, 2006, 2256, 2219, 2406, 999, 999, 999, 12475, 2008, 11963, 999, 999, 1001, 8398, 11387, 11387, 1001, 8398, 3736, 3726, 14074, 14735, 102])\n",
      "Row(id=UUID('c37fbfee-c22a-4d4d-bd46-b0bad52f125e'), label='2', tokens=[101, 1030, 5310, 1030, 5310, 1030, 5310, 1030, 5310, 2002, 2323, 2022, 1999, 7173, 2025, 1999, 1996, 2317, 2160, 1012, 1001, 21877, 3527, 5558, 2063, 1001, 3533, 17062, 2368, 8299, 102])\n",
      "Row(id=UUID('b9fbdf99-2bd4-4531-bbd9-ed40c4459c32'), label='0', tokens=[101, 1030, 5310, 18079, 16515, 2099, 1010, 8398, 4282, 2085, 2002, 2097, 4558, 2602, 1012, 2298, 2000, 2032, 2000, 12897, 1012, 11675, 2075, 2035, 17069, 3459, 2005, 7226, 2368, 2138, 7279, 3401, 2347, 1005, 1056, 2006, 1996, 10428, 2004, 2343, 1012, 8398, 2001, 1012, 19195, 2010, 4366, 1996, 5653, 1011, 1999, 17069, 2024, 1037, 9861, 1012, 1996, 26960, 2099, 1010, 16795, 2015, 2365, 1010, 1996, 3424, 26654, 8398, 1012, 102])\n",
      "Row(id=UUID('8cb38409-daa1-4600-94fe-2097a4b5ef02'), label='2', tokens=[101, 1030, 5310, 12464, 1521, 1055, 4167, 1521, 1055, 5525, 5490, 10179, 27512, 2290, 9016, 19983, 1012, 2016, 1013, 2002, 2003, 4415, 12264, 999, 2035, 3855, 4436, 2020, 1037, 13996, 1997, 1996, 17183, 2015, 2219, 7268, 26736, 1011, 2568, 25774, 1012, 2079, 2027, 2031, 1037, 5177, 2004, 2092, 2004, 1037, 6259, 8761, 1029, 2079, 2027, 2066, 1004, 23713, 1025, 6225, 3915, 1010, 1996, 4552, 1004, 23713, 1025, 4480, 1029, 1001, 8398, 11387, 11387, 100, 102])\n",
      "Row(id=UUID('6d9512ed-4061-4965-8033-62703c941976'), label='2', tokens=[101, 1030, 5310, 26823, 2080, 7226, 2368, 2006, 4830, 4424, 5850, 18629, 1029, 1029, 102])\n",
      "Row(id=UUID('f2e2015a-69d9-4b24-af70-5bfe96cf6519'), label='2', tokens=[101, 3533, 7226, 2368, 2003, 1037, 5294, 16939, 1012, 2662, 2001, 2320, 2417, 1012, 2228, 2055, 2077, 1012, 2256, 2047, 4245, 3791, 2000, 2298, 2067, 1012, 1045, 2031, 1037, 2568, 2008, 3632, 2067, 2531, 2086, 2138, 1997, 2054, 2026, 2155, 1998, 2256, 2808, 2031, 4036, 2033, 1012, 2054, 2003, 1037, 3565, 15267, 1010, 3533, 1029, 1029, 1029, 100, 1001, 14379, 102])\n",
      "Row(id=UUID('31f41408-7ab5-4fe8-9efd-d18535e4d6aa'), label='1', tokens=[101, 1030, 5310, 10643, 2024, 17522, 15069, 2015, 1012, 1996, 2972, 2843, 1997, 2068, 1012, 2265, 2068, 2053, 8673, 1999, 2281, 1012, 1001, 3789, 16558, 23361, 8820, 3726, 14074, 14735, 102])\n",
      "Row(id=UUID('67c1fa79-ec1b-4f11-b0c6-05d69a0e588c'), label='0', tokens=[101, 3666, 3533, 7226, 2368, 3921, 1996, 14502, 1004, 23713, 1025, 5138, 1996, 6488, 2302, 2151, 20737, 2003, 4895, 22852, 1001, 17183, 8663, 15338, 3258, 102])\n",
      "Row(id=UUID('1334c4ad-41c6-402b-96b5-61632c2a418f'), label='0', tokens=[101, 1030, 5310, 1045, 1005, 1049, 2469, 2008, 2027, 2071, 2131, 2070, 10552, 6040, 2013, 1996, 7226, 2368, 3049, 102])\n",
      "Row(id=UUID('b007c4b5-3d6a-41b0-adac-53bd7332922e'), label='2', tokens=[101, 1030, 5310, 3374, 2021, 2002, 2003, 2026, 2343, 999, 1001, 8398, 11387, 11387, 1001, 8398, 11387, 11387, 8653, 24198, 1001, 8398, 4063, 22043, 8163, 6038, 29171, 102])\n",
      "Row(id=UUID('6acb1d45-47b9-49a5-95b3-b7ffea049ed1'), label='0', tokens=[101, 1030, 5310, 1030, 5310, 2116, 1997, 1996, 6997, 1996, 8398, 2155, 2038, 5462, 2024, 2110, 1010, 2025, 2976, 1010, 6997, 1012, 7226, 2368, 2038, 2053, 2373, 2000, 14933, 2216, 1012, 4496, 2515, 8398, 2030, 7279, 3401, 2031, 2008, 2373, 1012, 102])\n",
      "Row(id=UUID('873438a0-2fca-416d-9c14-2f1888dd8ac3'), label='2', tokens=[101, 1030, 5310, 1045, 3571, 2017, 1521, 2128, 2157, 1012, 1045, 11839, 1030, 5310, 1520, 1055, 3416, 2744, 2097, 2404, 2062, 3579, 12388, 29163, 1996, 11963, 1012, 2002, 5791, 6086, 2009, 1012, 2228, 2054, 2057, 2113, 2085, 6431, 2602, 2154, 1521, 2385, 1012, 1012, 1012, 1001, 23848, 2050, 1001, 8398, 11387, 11387, 102])\n",
      "Row(id=UUID('9b59d30f-14bf-4dbe-b763-c61326ca06aa'), label='2', tokens=[101, 1030, 5310, 16780, 999, 1001, 8398, 11387, 11387, 2035, 1996, 2126, 999, 1001, 2067, 10760, 16558, 5657, 1001, 3808, 8873, 12096, 102])\n",
      "Row(id=UUID('81666100-4358-4127-ac4c-a8c3dba3d45c'), label='0', tokens=[101, 10166, 15941, 5472, 2121, 2003, 1996, 2069, 2028, 2040, 6753, 7072, 1001, 17183, 3207, 20179, 102])\n",
      "Row(id=UUID('9d12a59f-2786-4d53-aae2-cecd98154e1d'), label='0', tokens=[101, 1045, 1521, 1049, 3492, 2469, 1037, 2843, 1997, 2111, 2097, 2022, 3666, 2023, 5981, 2025, 2138, 2027, 1521, 2128, 4699, 1999, 7226, 2368, 2030, 1996, 4589, 2158, 1521, 1055, 2576, 11032, 2006, 2367, 3314, 1010, 2021, 2138, 2009, 1521, 2222, 2022, 2066, 1037, 4507, 2265, 100, 1012, 1012, 1012, 7226, 2368, 2003, 2525, 2124, 2005, 5177, 11721, 16020, 1521, 1055, 1999, 2010, 2576, 22867, 1012, 1012, 102])\n",
      "Row(id=UUID('3f0336f7-fd95-4502-a8f3-161d1a3a5c4d'), label='2', tokens=[101, 1030, 5310, 3789, 2417, 2111, 1012, 1012, 1001, 8398, 11387, 11387, 102])\n",
      "Row(id=UUID('9e5aaf54-718b-48b2-93bd-45692030ba8f'), label='2', tokens=[101, 1030, 5310, 1030, 5310, 1001, 8398, 11387, 11387, 5514, 3531, 999, 999, 999, 999, 102])\n",
      "Row(id=UUID('4e7537e4-00b7-4371-8ac8-043e28cdda1f'), label='2', tokens=[101, 1030, 5310, 1030, 5310, 1030, 5310, 15624, 999, 2007, 4419, 18329, 2091, 3759, 2015, 1997, 1996, 2137, 2111, 2007, 8040, 2819, 2066, 4380, 2063, 999, 2040, 3791, 6716, 999, 2204, 9436, 25514, 1030, 5310, 1001, 8398, 11387, 11387, 8299, 102])\n",
      "Row(id=UUID('2a4f35f5-acdf-4fd7-a72f-00bfc2c04340'), label='1', tokens=[101, 1030, 5310, 1030, 5310, 1997, 2607, 2017, 2024, 4285, 2111, 999, 1996, 2069, 3114, 1045, 2572, 2006, 10474, 2003, 2000, 7532, 2033, 2007, 2066, 1011, 13128, 2111, 2040, 3745, 1037, 2691, 3125, 1011, 1011, 2203, 2023, 2120, 10103, 999, 1001, 3789, 16558, 23361, 8820, 3726, 14074, 14735, 102])\n",
      "Row(id=UUID('19b33a56-350a-40f2-98d3-4f0c8da12e25'), label='0', tokens=[101, 2057, 2323, 8642, 6366, 9501, 2013, 2035, 4883, 14379, 1012, 2061, 2172, 2051, 2003, 13842, 2011, 23100, 20737, 1998, 24867, 1012, 2023, 2038, 2042, 1037, 3052, 1997, 4840, 2250, 1012, 1001, 17183, 3207, 20179, 102])\n",
      "Row(id=UUID('e0460bca-e641-46fb-9c35-a9d065cd5235'), label='0', tokens=[101, 2013, 1001, 3960, 3702, 7652, 2015, 2338, 1001, 8112, 26760, 11650, 1010, 17012, 2213, 1012, 1012, 1012, 2040, 6732, 1030, 5310, 2323, 2131, 1996, 1001, 10501, 18098, 4697, 2157, 2185, 1012, 2057, 2024, 5026, 2111, 2188, 1012, 1030, 5310, 1030, 5310, 1001, 8398, 11387, 11387, 8299, 102])\n",
      "Row(id=UUID('b1a2c23d-9fd9-44f3-8052-4916bbc0ac79'), label='0', tokens=[101, 1030, 5310, 4283, 2005, 6631, 1030, 5310, 100, 1996, 15285, 2024, 2028, 1997, 1996, 2087, 3928, 2945, 2182, 1999, 2660, 1012, 1001, 12475, 24138, 4213, 8737, 1001, 3828, 8162, 19339, 7389, 102])\n",
      "Row(id=UUID('53e1518d-bc5b-4103-a78b-28b3bbe27b60'), label='1', tokens=[101, 1012, 1030, 5310, 2580, 2023, 16775, 25539, 1025, 2057, 2035, 2342, 2000, 1001, 3789, 16558, 5657, 2000, 3828, 3268, 1012, 1012, 1012, 1057, 1012, 1055, 1012, 4311, 3284, 2193, 1997, 2522, 17258, 1011, 2539, 6677, 1999, 2028, 2154, 2144, 3054, 1011, 2089, 8299, 102])\n",
      "Row(id=UUID('29d27b62-ca9b-4a8f-9451-fb3f81689611'), label='2', tokens=[101, 1030, 5310, 2296, 2801, 7226, 2368, 2404, 5743, 2055, 2522, 17258, 2018, 2525, 2042, 8885, 2011, 8398, 1012, 7226, 2368, 8246, 1012, 102])\n",
      "Row(id=UUID('cd89c921-84dd-482a-bcff-178874e3a981'), label='0', tokens=[101, 1030, 5310, 2017, 2360, 2017, 2215, 2000, 1001, 23848, 2050, 2664, 2017, 9969, 9828, 2000, 1996, 2270, 2012, 1996, 2927, 1997, 1996, 1001, 2522, 17258, 16147, 6090, 3207, 7712, 1998, 1001, 2209, 4183, 7698, 1011, 8299, 102])\n",
      "Row(id=UUID('cc81e9f8-43c1-4cb3-bd7d-ff81b701afa5'), label='1', tokens=[101, 1030, 5310, 2017, 2074, 2123, 1521, 1056, 2507, 1037, 4485, 2055, 2111, 1521, 1055, 3268, 2079, 2017, 1029, 1001, 3789, 16558, 23361, 8820, 3726, 14074, 14735, 100, 999, 102])\n",
      "Row(id=UUID('a453e98a-21e1-4ff0-86bd-db5683b31f1d'), label='2', tokens=[101, 1030, 5310, 1030, 5310, 1030, 5310, 12778, 2010, 7773, 1029, 1048, 2863, 2080, 1012, 1012, 1012, 7226, 2368, 1998, 2010, 2564, 9511, 7079, 26854, 7773, 2006, 2037, 2338, 11372, 2011, 2074, 7079, 3209, 1037, 2235, 3815, 1998, 2975, 1996, 2717, 1999, 2037, 1000, 3840, 1000, 2002, 6994, 3423, 7077, 19990, 2066, 8398, 1998, 3071, 2515, 1012, 102])\n",
      "Row(id=UUID('e91fc802-13ac-45cc-952d-97b159dda459'), label='0', tokens=[101, 1030, 5310, 1030, 5310, 1030, 5310, 7078, 999, 2045, 2003, 1037, 2678, 2029, 3065, 7648, 2108, 5045, 2012, 2011, 1037, 2158, 2007, 1037, 28497, 1012, 2017, 2064, 2941, 2156, 1996, 5195, 2543, 2021, 2009, 2074, 2987, 1005, 1056, 4906, 1996, 7984, 1012, 1001, 8398, 11387, 11387, 102])\n",
      "Row(id=UUID('902d2cfd-e829-414e-b37b-9e9743e74987'), label='1', tokens=[101, 1030, 5310, 1030, 5310, 8148, 1010, 2199, 1009, 4841, 2031, 2351, 2013, 1996, 1001, 8398, 23350, 11387, 11387, 1010, 2131, 1996, 3109, 2125, 10474, 1998, 2079, 2115, 4365, 3105, 999, 999, 999, 1001, 3789, 16558, 23361, 8820, 3726, 14074, 14735, 1001, 7226, 2368, 11387, 11387, 1001, 7226, 2368, 8167, 6935, 11387, 11387, 102])\n",
      "Row(id=UUID('0f8977ce-b0c6-4734-871b-06582bc0b76e'), label='0', tokens=[101, 2821, 2053, 999, 3532, 1030, 5310, 2003, 2551, 2428, 2524, 2000, 17894, 2010, 3611, 1012, 3374, 1010, 2021, 2115, 3658, 2055, 21887, 23350, 1011, 2096, 8052, 1011, 2097, 2196, 2022, 2438, 1012, 1001, 29300, 2278, 11387, 11387, 102])\n",
      "Row(id=UUID('1902b786-d55f-4b67-87e2-05f25638560d'), label='1', tokens=[101, 1030, 5310, 2339, 1029, 2054, 2024, 2027, 1010, 1016, 2086, 4237, 1029, 7226, 2368, 2097, 2272, 2041, 11820, 2302, 1037, 4797, 1012, 102])\n",
      "Row(id=UUID('5f940c05-34c1-4dd5-aac2-77391612c6f6'), label='1', tokens=[101, 1030, 5310, 1030, 5310, 1998, 1030, 5310, 2031, 2000, 2061, 2860, 4797, 1998, 21910, 1998, 2097, 2360, 2044, 1996, 5981, 2008, 7226, 2368, 22673, 1011, 2004, 2027, 2525, 2024, 3188, 2000, 1012, 2010, 8754, 2097, 2903, 3649, 2002, 2758, 1012, 2009, 1521, 1055, 2061, 14888, 102])\n",
      "Row(id=UUID('2cf2fec4-1f97-41f9-baec-c205832788ae'), label='2', tokens=[101, 1030, 5310, 2061, 2017, 2812, 2000, 2425, 2033, 2008, 9282, 2160, 23744, 2064, 1521, 1056, 2022, 2308, 1997, 3609, 1029, 1029, 2030, 2008, 2659, 3318, 2069, 5218, 2000, 2111, 1997, 3609, 1029, 1029, 1029, 1029, 2024, 2017, 3809, 1029, 1029, 1029, 2017, 2024, 2054, 2003, 3308, 2007, 2637, 1010, 19424, 1001, 8398, 11387, 11387, 100, 102])\n",
      "Row(id=UUID('90ca8ff2-f7bc-4ea8-8890-29f1d5bc38d4'), label='2', tokens=[101, 18932, 1018, 1024, 3533, 7226, 2368, 1024, 1037, 6480, 1997, 4688, 1004, 23713, 1025, 20228, 16098, 6935, 2213, 1012, 2292, 1005, 1055, 2202, 1037, 19043, 1010, 4618, 2057, 1029, 1005, 3533, 7226, 2368, 1005, 1055, 4688, 1010, 18917, 1998, 10699, 15074, 2015, 2191, 2032, 1037, 3532, 3601, 2005, 2343, 1012, 2515, 2023, 4863, 2339, 2002, 1005, 1055, 2042, 1005, 2921, 1999, 1996, 8102, 1005, 1029, 1005, 8299, 102])\n",
      "Row(id=UUID('d8cce3f0-dc17-4563-8b32-3ed88ad29467'), label='2', tokens=[101, 2061, 2017, 4364, 2428, 2985, 2048, 3134, 6815, 8398, 2001, 2006, 5587, 21673, 2140, 1517, 2069, 2000, 2031, 3533, 7226, 2368, 10214, 2000, 2202, 1037, 4319, 3231, 3805, 1997, 1996, 14379, 1010, 1998, 2085, 2017, 1521, 2128, 4333, 1029, 13350, 2071, 2025, 2022, 2062, 2440, 1997, 2009, 1012, 1030, 5310, 2097, 2022, 4415, 2022, 8209, 2039, 3892, 1012, 102])\n",
      "Row(id=UUID('6fccb4a8-7e80-4e03-a854-844f219343f9'), label='0', tokens=[101, 1030, 5310, 2017, 2097, 2131, 9436, 1997, 2151, 3382, 1037, 2551, 2137, 2038, 1997, 2437, 2009, 2502, 1010, 2096, 2017, 1998, 2115, 7272, 2814, 2444, 2009, 2039, 1999, 1996, 2317, 2160, 1012, 2057, 2156, 2083, 2115, 18667, 1010, 1998, 2017, 2180, 1521, 1056, 2022, 3045, 1999, 2281, 1012, 1001, 8398, 11387, 11387, 1001, 3328, 9497, 1001, 8398, 23654, 102])\n",
      "Row(id=UUID('75fc9940-ebf8-4719-9dbc-b0ef535c385e'), label='1', tokens=[101, 1030, 5310, 1030, 5310, 5262, 1012, 2438, 2003, 2438, 1012, 2054, 6793, 1997, 1996, 8398, 3447, 3561, 2000, 5382, 2008, 8398, 2003, 1996, 9947, 2008, 2003, 16023, 2023, 2406, 1012, 1001, 3789, 16558, 23361, 8820, 3726, 14074, 14735, 1001, 3789, 17062, 2368, 8167, 15061, 8913, 26379, 24158, 15864, 24376, 102])\n",
      "Row(id=UUID('e17962b6-99bf-4d74-9515-37defab76bdb'), label='0', tokens=[101, 7564, 1997, 5210, 12015, 2012, 1996, 3298, 1011, 1999, 10523, 2648, 1001, 1040, 12273, 11387, 11387, 3892, 2004, 1001, 7226, 2368, 8847, 8299, 102])\n",
      "Row(id=UUID('d1984001-2c4c-4ba1-9d00-9d0c23a44109'), label='1', tokens=[101, 1030, 5310, 1030, 5310, 2339, 2024, 2017, 3666, 2694, 1029, 5807, 1005, 1056, 2017, 2022, 2551, 2006, 4337, 2075, 1996, 1001, 8398, 23350, 1998, 2845, 14974, 1029, 1001, 8398, 23350, 11266, 14083, 18981, 5369, 1001, 17328, 24456, 2361, 1001, 22072, 28745, 7361, 1001, 15653, 24456, 2361, 1001, 14033, 22930, 2818, 1001, 3789, 16558, 23361, 8820, 3726, 14074, 14735, 102])\n",
      "Row(id=UUID('b5a51965-d551-42a9-af86-7b3a29309a22'), label='1', tokens=[101, 1001, 100, 3422, 2054, 3047, 2012, 1014, 1024, 2459, 1999, 1030, 5310, 1005, 1055, 3743, 1024, 4729, 23521, 2050, 1523, 2304, 28283, 24413, 14303, 16341, 1524, 1001, 21911, 23278, 18752, 2015, 1001, 3533, 17062, 2368, 8299, 102])\n",
      "Row(id=UUID('be94c16a-c87e-42ed-b758-7423c97570a1'), label='0', tokens=[101, 4931, 4407, 2265, 2068, 2054, 2785, 1997, 2919, 2477, 4148, 2000, 2023, 4485, 21101, 1030, 5310, 1001, 15653, 24456, 2361, 102])\n",
      "Row(id=UUID('ab15942a-b15f-4950-a0ef-065ddecd39a7'), label='2', tokens=[101, 2008, 3355, 1030, 5310, 2003, 2028, 8295, 1001, 1038, 13728, 10129, 10303, 1012, 1998, 1030, 5310, 3475, 1521, 1056, 3625, 2005, 2014, 14472, 19273, 7179, 1025, 2059, 2053, 2028, 2842, 2003, 2593, 1010, 2157, 1029, 2092, 2008, 1521, 1055, 2129, 9945, 2573, 1012, 1012, 1012, 100, 1001, 8398, 11387, 11387, 8299, 102])\n",
      "Row(id=UUID('b551a66f-4922-439f-b3c1-06de53ce9c6c'), label='0', tokens=[101, 3533, 7226, 2368, 1523, 2038, 2000, 2191, 1037, 3893, 2553, 1998, 2556, 1037, 7046, 4432, 2084, 1520, 1045, 2572, 2025, 8398, 1010, 2292, 1521, 1055, 2175, 2067, 2000, 3671, 1010, 1521, 1524, 2280, 4883, 4018, 1030, 5310, 2056, 1999, 2258, 1012, 8675, 2097, 2022, 4092, 2012, 1996, 1001, 1040, 12273, 3892, 1012, 8299, 102])\n",
      "Row(id=UUID('b94c7a29-0658-40a2-a437-8eea6c9798a1'), label='0', tokens=[101, 13857, 8112, 1024, 1000, 2023, 2343, 1998, 2216, 1999, 2373, 1517, 2216, 2040, 5770, 2013, 4363, 2477, 1996, 2126, 2027, 2024, 1517, 2027, 2024, 10320, 2006, 2115, 22330, 8713, 2964, 1012, 1000, 1000, 2027, 1521, 2128, 5327, 2000, 1529, 8054, 2017, 2008, 2115, 3789, 2515, 2025, 3043, 1012, 2008, 2003, 2129, 2027, 2663, 1012, 1000, 1001, 17183, 8663, 15338, 3258, 8299, 102])\n",
      "Row(id=UUID('379e2456-490e-4c1d-adc0-735ad8f7a825'), label='0', tokens=[101, 2004, 10231, 7685, 2004, 12609, 2038, 2042, 1010, 2012, 2560, 18520, 2145, 3475, 1521, 1056, 2343, 1012, 1001, 10556, 2290, 11387, 11387, 1001, 10556, 2290, 1001, 23848, 2050, 102])\n",
      "Row(id=UUID('514af13d-5d39-4f10-b3a5-6600501de739'), label='1', tokens=[101, 1000, 1996, 2925, 1997, 2256, 7072, 2003, 2012, 8406, 1012, 1996, 2925, 1997, 2256, 4610, 2003, 2012, 8406, 1012, 1996, 2925, 1997, 2256, 4774, 2003, 2012, 8406, 1012, 1000, 1001, 15941, 8791, 13375, 1001, 7226, 2368, 8167, 6935, 11387, 11387, 1001, 1040, 12273, 1001, 2602, 11387, 11387, 8299, 102])\n",
      "Row(id=UUID('810b7a3b-0c6a-4a03-b190-1affe8b9096a'), label='0', tokens=[101, 1042, 13626, 2941, 2359, 2000, 17607, 1037, 4555, 3318, 1012, 2009, 2001, 1037, 2204, 2801, 1012, 1001, 17183, 3207, 20179, 8299, 102])\n",
      "Row(id=UUID('dd28103b-939f-4222-b287-dac4b8602eeb'), label='0', tokens=[101, 1030, 5310, 1030, 5310, 6221, 8398, 1005, 1055, 8220, 2005, 16360, 15859, 26642, 1024, 2053, 8465, 2005, 4688, 1012, 2053, 8465, 2005, 5223, 1012, 2053, 8465, 2005, 10047, 22049, 3012, 1012, 2053, 8465, 2005, 6997, 1012, 2053, 8465, 2005, 8902, 24117, 1012, 2053, 8465, 2005, 17727, 5243, 22729, 1012, 2053, 8465, 2005, 4297, 25377, 12870, 5897, 1012, 6517, 1012, 1001, 3789, 16558, 24997, 9626, 12079, 2860, 6806, 102])\n",
      "Row(id=UUID('310a38ed-dbf2-4b65-a204-16b46c6b32a1'), label='0', tokens=[101, 3928, 4613, 999, 999, 3537, 2120, 4680, 1024, 9393, 8112, 18058, 2014, 4769, 1001, 17183, 8663, 15338, 3258, 8299, 102])\n",
      "Row(id=UUID('4f2a2863-31af-4ee9-acef-89e587103045'), label='1', tokens=[101, 2009, 1005, 1055, 2051, 2000, 2316, 2362, 2000, 2265, 2256, 3997, 1001, 9507, 2545, 1012, 1001, 3582, 5963, 19362, 3723, 1012, 3582, 2035, 2040, 1024, 2066, 100, 2128, 2102, 28394, 2102, 100, 7615, 100, 29525, 2004, 2017, 2175, 1012, 2031, 4569, 1012, 1001, 9507, 1001, 1042, 19892, 19362, 3723, 1001, 2149, 4523, 1001, 4929, 8067, 6711, 1001, 3789, 1001, 7226, 2368, 8167, 6935, 1001, 15653, 24456, 2361, 1001, 2630, 16535, 11387, 11387, 102])\n",
      "Row(id=UUID('9a2a3256-f607-490e-aa8f-ef408cbd9a5f'), label='0', tokens=[101, 1030, 5310, 27791, 2000, 8130, 1037, 2450, 2004, 21210, 1001, 3537, 3207, 20179, 102])\n",
      "Row(id=UUID('b35181d4-6e36-4daf-918e-4233c8c2de92'), label='1', tokens=[101, 1030, 5310, 1030, 5310, 13718, 1010, 2115, 3564, 2041, 1996, 2602, 7126, 2008, 4589, 5469, 2265, 1012, 3531, 1010, 3531, 7949, 2008, 10520, 2000, 4352, 4426, 2000, 3789, 7226, 2368, 1012, 102])\n",
      "Row(id=UUID('daadca1e-d819-4fbf-b087-f6f9a2ce7016'), label='0', tokens=[101, 1030, 5310, 2001, 2009, 1996, 10527, 21752, 4682, 1996, 16634, 3333, 2006, 2149, 2008, 2001, 14044, 1029, 1001, 29300, 2278, 11387, 11387, 102])\n",
      "Row(id=UUID('ea2d2837-c01a-433e-b040-2bceb77e4715'), label='1', tokens=[101, 2362, 2057, 2097, 3786, 6221, 8398, 1516, 1001, 3533, 17062, 2368, 2005, 2343, 1024, 2880, 3049, 4037, 1523, 21911, 2050, 1521, 1055, 2388, 2409, 2014, 3652, 2039, 1523, 2123, 1521, 1056, 4133, 2105, 1998, 17612, 2055, 2477, 1010, 2079, 2242, 1010, 1524, 2029, 2003, 2054, 9297, 1001, 21911, 2050, 2296, 2309, 2154, 1012, 1524, 1001, 3789, 1001, 21357, 8299, 102])\n",
      "Row(id=UUID('d9eeda73-945e-4307-98ea-5044fa1e0395'), label='0', tokens=[101, 1030, 5310, 1030, 5310, 1030, 5310, 1030, 5310, 2115, 5448, 2003, 2806, 2058, 9415, 1012, 8398, 2018, 3488, 2005, 5717, 1012, 102])\n",
      "Row(id=UUID('061844fc-fb95-4c5d-b8ac-182d1970dede'), label='2', tokens=[101, 1030, 5310, 3038, 8398, 2003, 16939, 2003, 26316, 6195, 3533, 2003, 2941, 1037, 16939, 2007, 8832, 16939, 7928, 1012, 1001, 3533, 17062, 2368, 1001, 8962, 3489, 26328, 28522, 3600, 1001, 8398, 11387, 11387, 8653, 24198, 7903, 7062, 102])\n",
      "Row(id=UUID('ad67fb5b-dcde-4bf3-b091-ef4bfd715ce2'), label='2', tokens=[101, 1030, 5310, 3071, 2003, 4363, 2068, 1999, 2037, 8102, 1999, 3932, 1997, 1001, 5342, 27698, 5178, 2078, 1012, 1001, 8398, 11387, 11387, 102])\n",
      "Row(id=UUID('22017a38-b9c1-4470-a4ea-28ade7aa91f7'), label='0', tokens=[101, 1030, 5310, 2026, 2769, 2003, 2006, 4477, 7226, 2368, 102])\n",
      "Row(id=UUID('c8ac9d92-b7e7-49cd-8f32-50afb5f42342'), label='0', tokens=[101, 1045, 2066, 1996, 2308, 2006, 2754, 1001, 17183, 3207, 20179, 1001, 3537, 3207, 20179, 102])\n",
      "Row(id=UUID('acb4df31-589a-4d78-8c2c-a6f2bc353a08'), label='1', tokens=[101, 1045, 1005, 1049, 5241, 1996, 1001, 2308, 29278, 17062, 2368, 3422, 2283, 2005, 1996, 1001, 17183, 8663, 15338, 3258, 2007, 1030, 5310, 1010, 1030, 5310, 1010, 1998, 2048, 5041, 2015, 3331, 4331, 6368, 1030, 5310, 1004, 23713, 1025, 1030, 5310, 999, 100, 102])\n",
      "Row(id=UUID('9cb71a11-c93f-4ffa-b5f0-9b7eee2b0d5f'), label='0', tokens=[101, 2017, 2064, 2131, 1005, 1055, 2663, 1037, 5592, 4003, 999, 999, 999, 2074, 11562, 1996, 4957, 4330, 5004, 1012, 1012, 1012, 8299, 1001, 17183, 3207, 20179, 8299, 102])\n",
      "Row(id=UUID('84a41b04-77a3-4c54-a0bf-fce7be6e7a03'), label='2', tokens=[101, 1996, 1001, 1040, 12273, 10549, 3533, 7226, 2368, 2004, 1037, 4883, 9773, 2003, 2074, 2004, 16801, 2004, 3533, 7226, 2368, 10549, 1037, 4022, 21210, 2241, 2006, 3096, 3609, 1012, 2054, 2024, 1996, 8037, 2039, 2000, 1029, 102])\n",
      "Row(id=UUID('fdb94c76-7a0c-42e2-afc4-7ddf0f6fd5b3'), label='2', tokens=[101, 2129, 1999, 1996, 3109, 2106, 2027, 3499, 1037, 7226, 2368, 2005, 2343, 7308, 2369, 3782, 7825, 999, 102])\n",
      "Row(id=UUID('65cb5959-0aef-4667-9ce5-e99fad48a4a5'), label='0', tokens=[101, 2065, 7637, 2063, 8547, 5811, 2987, 1521, 1056, 3288, 2014, 3566, 2006, 4950, 1010, 1045, 1521, 1049, 2183, 2000, 2022, 9364, 1012, 1001, 17183, 8663, 15338, 3258, 8299, 102])\n",
      "Row(id=UUID('a956d250-9993-4bb8-8619-f59af1f705fc'), label='0', tokens=[101, 4365, 7226, 2368, 7459, 2008, 3663, 2282, 1001, 17183, 3207, 20179, 102])\n",
      "Row(id=UUID('2beb5fd9-4560-4598-8d7b-88f6b0096c9c'), label='0', tokens=[101, 7226, 2368, 2074, 28960, 2054, 2010, 2270, 4092, 2873, 2409, 2032, 2000, 2079, 4953, 1996, 4477, 7226, 2368, 9751, 1010, 2006, 2444, 2694, 1024, 1523, 2196, 17612, 1010, 2196, 4863, 1524, 1001, 17183, 3207, 20179, 102])\n",
      "Row(id=UUID('36de80e5-76ee-4ef3-ba7f-cf6df89bba59'), label='1', tokens=[101, 1030, 5310, 1030, 5310, 1045, 1521, 1049, 6830, 1001, 7226, 2368, 11387, 11387, 2053, 3043, 2054, 1012, 2021, 1010, 2065, 1045, 2089, 1010, 3383, 2720, 1012, 1051, 1521, 18016, 2071, 2128, 26560, 3366, 2007, 2720, 1012, 7226, 2368, 1037, 2978, 2077, 1996, 14379, 1998, 5896, 1037, 2261, 25591, 12845, 2015, 1012, 2004, 1996, 3166, 1997, 1523, 1996, 5981, 1524, 2006, 1001, 1996, 19650, 9328, 2053, 2028, 2003, 2062, 4591, 1012, 1001, 2292, 17062, 2368, 4783, 17062, 2368, 102])\n",
      "Row(id=UUID('c0cb94fc-4148-442f-9f6e-3aff1725b91b'), label='0', tokens=[101, 1030, 5310, 2635, 2041, 3505, 22950, 2066, 2016, 2106, 2198, 22101, 1012, 1012, 1012, 1001, 17183, 3207, 20179, 8299, 102])\n",
      "Row(id=UUID('7805ea43-f454-43a5-b3b4-99f538ddbe9d'), label='2', tokens=[101, 2054, 1037, 13044, 3664, 1012, 2061, 5171, 1010, 7499, 8398, 1012, 1001, 8398, 11387, 11387, 1001, 3190, 1001, 5843, 7698, 1001, 2522, 17258, 16147, 102])\n",
      "Row(id=UUID('12e0e99b-e292-4459-b299-fc40668b999e'), label='0', tokens=[101, 1001, 17183, 8663, 15338, 3258, 16215, 2099, 2034, 3203, 9393, 8112, 2003, 4092, 1012, 1045, 3335, 2014, 1012, 102])\n",
      "Row(id=UUID('b9092be8-0e1b-4a14-af06-7812dba8b8db'), label='1', tokens=[101, 1030, 5310, 2123, 1005, 1056, 2202, 2256, 3099, 100, 2021, 3533, 2071, 2031, 1037, 9004, 2600, 2004, 2010, 21210, 1998, 2002, 1005, 1040, 2145, 2131, 2026, 3789, 1012, 1001, 3789, 16558, 23361, 8820, 3726, 14074, 14735, 102])\n",
      "Row(id=UUID('60bf289b-4719-43d3-ac92-0e9cb50f221c'), label='1', tokens=[101, 1030, 5310, 8398, 3488, 1029, 1045, 1521, 1049, 2469, 7226, 2368, 1998, 2010, 3095, 2031, 2589, 2062, 2084, 2048, 2847, 1997, 17463, 2005, 8398, 1521, 1055, 3488, 1012, 102])\n",
      "Row(id=UUID('bc1a889e-d5bc-47c4-9104-92a5b579f4f0'), label='0', tokens=[101, 21210, 1030, 5310, 3531, 2831, 2055, 2129, 2017, 2097, 4047, 2591, 3036, 2076, 1001, 1040, 12273, 11387, 11387, 1012, 2023, 2003, 1037, 2327, 3277, 2005, 7206, 2753, 1009, 1012, 2591, 3036, 2003, 1037, 2524, 1011, 3687, 5770, 1998, 1037, 4872, 2008, 2442, 2022, 2921, 1012, 1001, 4047, 22994, 2545, 12376, 24759, 2271, 4067, 2017, 1012, 102])\n",
      "Row(id=UUID('18925fcd-1038-4058-a7ec-aa0c35e82554'), label='2', tokens=[101, 2437, 13350, 5390, 2153, 1056, 1011, 3797, 1001, 3951, 23115, 3258, 1001, 8398, 26760, 8490, 1001, 8398, 1001, 8398, 11387, 11387, 1001, 2175, 2361, 1001, 8398, 23654, 1001, 23848, 2050, 1001, 2191, 14074, 14735, 17603, 6790, 23805, 2078, 1001, 2175, 2361, 1001, 10556, 2290, 11387, 11387, 1001, 3951, 1001, 4603, 2229, 1001, 2637, 8873, 12096, 8299, 102])\n",
      "Row(id=UUID('94899d26-277f-4ddb-9498-666f8f0a1c72'), label='1', tokens=[101, 1030, 5310, 4931, 9152, 2102, 9148, 2102, 1010, 2017, 2812, 2054, 2017, 1998, 1996, 8560, 3238, 2128, 2705, 15916, 19341, 3619, 2031, 2042, 2725, 1010, 18168, 2290, 2017, 2064, 1521, 1056, 2022, 3809, 1010, 2064, 1521, 1056, 8081, 5236, 1012, 1001, 4536, 2078, 24415, 17062, 2368, 11387, 11387, 1001, 3789, 16558, 23361, 8820, 3726, 14074, 14735, 1001, 3789, 16558, 5657, 11387, 11387, 1001, 3789, 10359, 29337, 12190, 29323, 3207, 11837, 25112, 4183, 1001, 3789, 16558, 23361, 8820, 7178, 6633, 10085, 22648, 2100, 1001, 3789, 16558, 23361, 8913, 26379, 24158, 15864, 24376, 102])\n",
      "Row(id=UUID('4a96955b-feb2-4d01-be85-336071332c67'), label='1', tokens=[101, 1030, 5310, 2065, 2017, 2066, 8398, 2108, 2343, 1010, 2175, 2157, 3805, 1998, 4339, 1999, 10722, 4877, 2072, 1012, 1045, 2097, 2079, 2054, 2026, 2406, 3791, 1012, 1045, 2097, 8688, 2026, 12157, 1998, 3789, 2005, 2619, 2040, 2064, 2203, 2023, 1012, 2025, 6031, 1010, 2025, 10722, 4877, 2072, 1010, 1001, 3533, 17062, 2368, 1012, 3531, 3693, 2033, 1012, 102])\n",
      "Row(id=UUID('d2b9f276-aa18-442f-9e66-d9c57d2741f9'), label='2', tokens=[101, 1030, 5310, 7226, 2368, 2097, 3046, 2000, 21910, 2005, 2469, 102])\n",
      "Row(id=UUID('21cb9f24-931a-49c1-93c4-a32bf69054b7'), label='1', tokens=[101, 1030, 5310, 1030, 5310, 2053, 1012, 1045, 2097, 2025, 3233, 8909, 2135, 2011, 999, 1045, 1005, 1049, 6830, 2005, 1030, 5310, 2000, 5621, 2191, 2637, 2307, 2011, 7065, 2545, 2075, 1996, 4053, 2589, 2011, 1996, 2235, 1052, 2343, 1004, 23713, 1025, 2010, 25353, 3597, 21890, 7666, 1001, 23848, 2050, 1001, 3789, 16558, 5657, 7698, 7384, 4140, 102])\n",
      "Row(id=UUID('8bd7a8c1-f288-4f5b-ae87-0ed31bfdd2e6'), label='2', tokens=[101, 1030, 5310, 1030, 5310, 13244, 8554, 8684, 2003, 1996, 2925, 1012, 1012, 3531, 2562, 2014, 4142, 1999, 2023, 4750, 2088, 1012, 1012, 2293, 2013, 2710, 1012, 1012, 1001, 23848, 2050, 1001, 8398, 11387, 11387, 13122, 10696, 14074, 14735, 102])\n",
      "Row(id=UUID('4fa201dc-4557-4b47-9658-04040ab5d9f6'), label='1', tokens=[101, 1030, 5310, 3531, 1001, 3582, 5963, 2035, 2040, 19387, 2030, 14686, 19387, 2023, 2460, 1006, 1015, 1024, 4029, 1007, 1010, 5132, 2100, 4013, 1011, 7226, 2368, 4487, 15353, 1012, 1001, 3789, 16558, 5657, 1001, 3789, 17062, 2368, 8167, 6935, 1011, 1523, 21911, 2050, 1998, 3533, 1524, 1011, 3531, 11562, 1523, 16784, 2039, 1524, 2006, 7858, 1012, 1001, 7226, 18595, 3736, 10105, 3678, 1001, 1042, 19892, 2036, 1010, 2514, 2489, 2000, 9231, 2023, 1024, 8299, 102])\n",
      "Row(id=UUID('573f7b17-8c3c-4647-969d-a051d9bce3ed'), label='0', tokens=[101, 1045, 2293, 3773, 6437, 5207, 1010, 1996, 2034, 2304, 2450, 1006, 1998, 2034, 11690, 100, 1007, 2000, 2507, 1037, 25569, 4613, 2012, 1996, 1040, 12273, 1012, 1001, 17183, 8663, 15338, 3258, 1001, 3537, 8663, 15338, 3258, 102])\n",
      "Row(id=UUID('21aa4b48-afdf-4ae5-b0c6-86beb300bff1'), label='2', tokens=[101, 1001, 5292, 3270, 3270, 3270, 3270, 1000, 3220, 10556, 25810, 14163, 28745, 22401, 2015, 2758, 1037, 3789, 2005, 2343, 8398, 1999, 2281, 2003, 1005, 2019, 2552, 1997, 4808, 1005, 1000, 8299, 102])\n",
      "Row(id=UUID('420d8359-f4ad-40d5-9928-192a66fff7db'), label='2', tokens=[101, 3533, 7226, 2368, 1521, 1055, 1037, 25312, 23177, 4682, 2099, 999, 2142, 2057, 3233, 1012, 2007, 7226, 2368, 2057, 2991, 1012, 3789, 1001, 8398, 11387, 11387, 1012, 1001, 23848, 2050, 11387, 11387, 1001, 2053, 2135, 2378, 17062, 2368, 1012, 1001, 2053, 9006, 23041, 2923, 10383, 1001, 16839, 10085, 4818, 2923, 10383, 1001, 25312, 23177, 3533, 7226, 2368, 1521, 1055, 1037, 25312, 23177, 4682, 2099, 999, 8299, 102])\n",
      "Row(id=UUID('a9dc3a33-a515-47d0-8a10-1a6c51abc3dc'), label='0', tokens=[101, 8909, 2278, 1010, 2065, 1061, 1521, 2035, 8398, 3111, 1054, 2135, 2228, 1030, 5310, 2106, 2061, 2204, 10047, 3374, 2021, 2017, 2024, 3109, 2050, 3308, 100, 1001, 14379, 11387, 11387, 1001, 3533, 17062, 2368, 1001, 6221, 24456, 2361, 1001, 3789, 102])\n",
      "Row(id=UUID('d41e72ba-5f4d-469b-a64a-38a6f7f339b2'), label='1', tokens=[101, 1030, 5310, 17927, 2017, 1005, 2310, 2042, 11703, 7416, 7178, 2003, 1996, 2034, 3357, 1012, 2085, 7939, 23709, 3401, 1996, 6270, 5618, 2017, 1005, 2310, 2042, 4352, 1998, 13951, 1999, 4288, 4841, 1012, 1001, 3789, 16558, 5657, 102])\n",
      "Row(id=UUID('aba7d717-79ce-40b5-bf42-983a95421c4f'), label='0', tokens=[101, 1030, 5310, 1030, 5310, 1030, 5310, 2144, 1061, 1005, 2035, 2024, 3666, 1045, 1005, 1049, 6069, 2074, 2083, 2023, 2041, 2045, 1012, 1001, 3348, 6198, 2483, 6198, 1001, 11703, 20026, 19779, 1001, 17183, 3207, 20179, 102])\n",
      "Row(id=UUID('040e06a2-d8f2-463d-b692-db59d609ea58'), label='0', tokens=[101, 1996, 2171, 1997, 1996, 2208, 3892, 2003, 1001, 22950, 19442, 7698, 1001, 17183, 3207, 20179, 102])\n",
      "Row(id=UUID('cfcb33ae-c9c9-4eb6-9813-849bd3c3b68e'), label='1', tokens=[101, 1030, 5310, 2002, 2071, 2907, 1037, 3282, 2000, 2026, 2132, 1998, 2360, 2002, 2052, 4139, 1996, 9495, 2065, 1045, 2134, 1005, 1056, 3789, 2005, 8398, 1998, 1045, 2052, 2074, 2298, 2012, 2032, 1998, 2360, 1010, 1000, 1001, 7226, 2368, 11387, 11387, 1000, 102])\n",
      "Row(id=UUID('18271744-73f0-4efc-9d93-75a337466d4b'), label='1', tokens=[101, 4931, 1030, 5310, 2204, 3046, 1010, 2021, 3331, 2000, 1030, 5310, 2074, 2987, 1521, 1056, 2175, 5973, 1012, 2002, 2180, 1521, 1056, 2412, 2507, 1037, 3722, 3437, 999, 2002, 2074, 4122, 2000, 2022, 8398, 2015, 2677, 11198, 1998, 9530, 10258, 3686, 1996, 3314, 999, 999, 1001, 3789, 16558, 23361, 8820, 3726, 14074, 14735, 102])\n",
      "Row(id=UUID('21aa144c-0730-4076-a365-ea7c1887f8b0'), label='1', tokens=[101, 1001, 3533, 17062, 2368, 2097, 2022, 2343, 2007, 2256, 2393, 1024, 2880, 3049, 4037, 1064, 6951, 1030, 5310, 3696, 2039, 2651, 8299, 102])\n",
      "Row(id=UUID('aeeccfad-cea2-4d5b-bff8-7963fcce5698'), label='1', tokens=[101, 1030, 5310, 2899, 2110, 1012, 2035, 5653, 1999, 1012, 2009, 2003, 1996, 2190, 2126, 2000, 3789, 10047, 6806, 1012, 1001, 3789, 16558, 23361, 8820, 3726, 14074, 14735, 102])\n",
      "Row(id=UUID('c80b12ea-7f96-4c5b-bbb9-281d5ba4598a'), label='2', tokens=[101, 1030, 5310, 1030, 5310, 5292, 3270, 3900, 3900, 3900, 3900, 3501, 3270, 3270, 3270, 1012, 1012, 1012, 2672, 2077, 1996, 2203, 1997, 1996, 2095, 1010, 1998, 2017, 2031, 2000, 4604, 2061, 7352, 2769, 2000, 1996, 28781, 2040, 3582, 2032, 1012, 2002, 2003, 1037, 2200, 2659, 2943, 2711, 1012, 2471, 2004, 5875, 2004, 3666, 6773, 4318, 1012, 1001, 8398, 11387, 11387, 1001, 3328, 9497, 1001, 3328, 9497, 19699, 5358, 3207, 5302, 23423, 29278, 22507, 102])\n",
      "Row(id=UUID('c1348112-ec35-42a9-bf37-4c3628bb060b'), label='1', tokens=[101, 2043, 8398, 7365, 2369, 7226, 2368, 2000, 20014, 27605, 13701, 2032, 1045, 2228, 3533, 2323, 2360, 1000, 3531, 2693, 2067, 2000, 1037, 3647, 3292, 1012, 2017, 2079, 2025, 2031, 1037, 7308, 2006, 1012, 1000, 2002, 3791, 2000, 2219, 2023, 5981, 1012, 1001, 7226, 2368, 9289, 2213, 102])\n",
      "Row(id=UUID('5fb54412-8e38-4536-8ab5-cd5f66170a41'), label='0', tokens=[101, 2009, 2003, 2590, 2005, 8037, 2000, 2022, 2142, 2006, 1996, 2755, 2008, 2296, 2450, 1998, 2155, 2442, 5454, 2005, 3209, 2054, 2003, 2157, 2005, 2068, 1012, 2009, 2003, 2025, 1996, 3105, 1997, 3519, 2030, 2151, 2060, 4884, 2303, 2000, 5646, 1037, 2155, 1521, 1055, 2925, 2030, 2202, 9804, 2185, 2013, 2308, 1012, 1001, 17183, 3207, 20179, 102])\n",
      "Row(id=UUID('18af7096-4a51-4438-83ce-8cd829c36845'), label='0', tokens=[101, 1523, 2002, 2003, 2025, 2005, 2740, 2005, 3087, 11303, 9871, 1012, 1012, 1012, 2002, 2097, 2202, 2009, 2185, 1012, 1524, 1011, 1030, 5310, 1001, 14379, 11387, 11387, 102])\n",
      "Row(id=UUID('d040adb0-913e-430e-9973-61f9bb652afe'), label='0', tokens=[101, 2122, 21237, 2015, 2031, 2445, 2037, 2111, 2495, 1010, 2966, 2729, 1998, 2833, 1998, 2157, 2085, 2057, 1521, 2128, 8084, 2007, 2216, 3722, 2477, 1012, 4486, 2024, 15275, 15941, 2003, 2157, 1001, 17183, 3207, 20179, 102])\n",
      "Row(id=UUID('f8a53dfa-fdc2-4e89-bb0e-8bde0ecd5e4b'), label='1', tokens=[101, 1030, 5310, 1000, 2821, 2026, 1010, 2256, 5394, 2038, 2633, 3706, 2039, 2000, 3828, 1996, 2154, 1012, 1012, 1012, 1012, 1012, 2044, 2034, 4292, 2149, 2039, 2005, 4945, 1012, 1000, 2178, 20150, 1012, 8568, 8398, 1056, 28394, 3215, 1998, 3579, 2006, 3045, 1012, 1001, 3789, 14644, 2135, 1001, 3789, 2378, 27576, 1001, 3789, 8095, 10259, 5685, 15864, 1001, 13292, 2509, 1001, 3789, 16558, 5657, 1001, 3789, 16558, 23361, 8913, 26379, 24158, 15864, 24376, 102])\n",
      "Row(id=UUID('b2ea3489-b937-464d-b03b-6f94ce4a87ee'), label='0', tokens=[101, 1030, 5310, 2023, 20571, 2009, 100, 1001, 8398, 11387, 11387, 8299, 102])\n",
      "Row(id=UUID('d287b4e6-5c49-45b9-9c00-3c6649fcaab9'), label='1', tokens=[101, 2017, 2113, 2129, 2027, 2360, 3087, 2064, 2022, 2343, 1029, 2009, 1005, 1055, 2025, 2995, 1012, 8398, 2699, 2009, 1998, 2002, 2481, 1005, 1056, 2079, 2009, 1012, 2002, 2003, 6719, 4039, 2000, 2079, 1996, 3105, 1012, 2061, 2672, 2057, 2323, 2507, 2009, 2000, 2619, 2842, 1001, 8398, 14268, 10483, 2121, 1001, 7226, 2368, 11387, 11387, 102])\n",
      "Row(id=UUID('b376afaf-a433-47f7-a4db-348ba9404911'), label='2', tokens=[101, 1030, 5310, 3789, 2005, 2375, 1998, 2344, 1999, 2281, 1012, 100, 1001, 8398, 11387, 11387, 1001, 2375, 28574, 26764, 8299, 102])\n",
      "Row(id=UUID('7358e737-c7d6-4325-be66-9b8e296ce86b'), label='1', tokens=[101, 1030, 5310, 1030, 5310, 1030, 5310, 2288, 2008, 2157, 12610, 999, 2021, 2049, 6595, 3537, 999, 1045, 2052, 2738, 2156, 2498, 2131, 2589, 2084, 3499, 1996, 2128, 2705, 15916, 19341, 3619, 2000, 9040, 2637, 999, 1001, 3789, 16558, 23361, 8820, 3726, 14074, 14735, 102])\n",
      "Row(id=UUID('b4471e6d-14b6-4ece-b06c-f00770a514ee'), label='0', tokens=[101, 6423, 1011, 3434, 2852, 3240, 25608, 2635, 2149, 2083, 2023, 2345, 2305, 1997, 1001, 1040, 12273, 1001, 17183, 8663, 15338, 3258, 102])\n",
      "Row(id=UUID('d8528e99-7fd1-4264-8150-13a8fe75c91a'), label='0', tokens=[101, 3666, 1996, 1001, 2175, 2361, 2377, 4767, 4331, 2007, 1030, 5310, 3065, 1996, 1999, 23606, 2791, 1997, 8801, 1012, 2009, 1521, 1055, 2025, 2055, 7300, 1010, 2009, 1521, 1055, 2055, 4024, 1012, 4767, 4331, 2987, 1521, 1056, 2191, 2111, 2892, 1996, 8842, 1010, 2009, 2074, 4599, 1996, 7311, 2005, 2115, 2219, 2217, 1012, 2014, 2501, 2003, 2438, 2572, 5302, 1012, 3579, 999, 102])\n",
      "Row(id=UUID('96e36f21-a0de-4b8f-a8c9-59eca83e7cad'), label='1', tokens=[101, 1523, 1996, 2502, 2702, 3034, 5444, 2000, 17542, 2049, 2374, 2161, 2023, 2991, 2058, 5936, 3141, 2000, 1996, 21887, 23350, 6090, 3207, 7712, 1524, 2502, 10790, 1010, 1037, 2307, 2742, 2000, 3582, 999, 1001, 2522, 17258, 16147, 1001, 21887, 23350, 1001, 4929, 8067, 6711, 3582, 1001, 3533, 17062, 2368, 1521, 1055, 2742, 1012, 2002, 1521, 1055, 2025, 1037, 1001, 2522, 17258, 25185, 8299, 102])\n",
      "Row(id=UUID('7acb62c8-5f77-4ec2-9796-8eb44b947d06'), label='1', tokens=[101, 1030, 5310, 1030, 5310, 1030, 5310, 1030, 5310, 1030, 5310, 1030, 5310, 1030, 5310, 1030, 5310, 1001, 15653, 24456, 2361, 1001, 3789, 5833, 3995, 2361, 2053, 7046, 16374, 2084, 8398, 999, 2191, 2637, 3647, 2153, 3789, 1030, 5310, 3647, 2003, 2488, 2084, 2025, 2307, 2153, 1001, 3789, 16558, 23361, 8820, 3726, 14074, 14735, 1001, 3789, 17062, 2368, 8167, 6935, 11387, 11387, 3422, 2023, 1004, 14181, 1025, 1004, 14181, 1025, 1004, 14181, 1025, 8299, 102])\n",
      "Row(id=UUID('d8bf3d18-5f3f-45b0-b832-a11252d6a3ee'), label='2', tokens=[101, 1030, 5310, 1030, 5310, 1030, 5310, 1030, 5310, 1030, 5310, 6300, 2050, 8398, 2001, 1996, 2028, 2008, 2409, 3537, 11141, 2000, 14906, 2039, 1996, 9750, 2046, 8329, 5014, 2029, 14729, 2005, 2055, 2431, 1997, 2522, 17258, 6677, 1012, 1012, 1012, 2821, 3524, 2053, 2008, 2001, 1996, 17183, 2015, 3772, 2006, 2037, 2219, 15802, 1012, 2049, 2053, 4687, 1019, 2630, 2163, 4070, 2005, 2058, 2871, 1003, 1997, 2522, 17258, 6677, 1012, 1012, 1012, 102])\n",
      "Row(id=UUID('b6babcd0-f210-4d1a-a3fd-a95440a750a5'), label='2', tokens=[101, 1019, 1013, 1019, 1001, 3533, 17062, 2368, 1010, 1001, 21911, 23278, 18752, 2015, 1010, 1998, 1996, 7207, 2015, 2031, 2146, 11041, 2369, 3056, 8037, 5627, 2000, 2404, 4847, 8010, 4331, 2077, 6958, 2043, 2009, 3310, 2000, 1996, 2807, 4126, 3021, 1012, 2174, 1010, 2116, 1999, 1996, 7740, 2304, 13965, 2069, 5444, 2005, 2009, 2041, 1997, 1000, 3571, 1000, 1997, 1000, 4788, 1000, 8236, 1012, 100, 8299, 102])\n",
      "Row(id=UUID('e236b509-da4e-47d4-a3a3-9e647a83634d'), label='2', tokens=[101, 1030, 5310, 1030, 5310, 7672, 4177, 2024, 17328, 2015, 999, 999, 999, 1001, 3789, 5833, 10760, 7913, 3022, 17175, 2271, 3207, 5302, 23423, 1001, 8398, 11387, 11387, 102])\n",
      "Row(id=UUID('9783c463-518e-44a9-a377-4d6c4c7ade2f'), label='1', tokens=[101, 1030, 5310, 2402, 3951, 2308, 2323, 5136, 7226, 2368, 1010, 2040, 2003, 1037, 8777, 1998, 1037, 15908, 2099, 2040, 2097, 4047, 2037, 2916, 1012, 8398, 2003, 1037, 11443, 2099, 1010, 1037, 28616, 15707, 26942, 1010, 1037, 22418, 6723, 5677, 1010, 1037, 22555, 14117, 26470, 2099, 1010, 1998, 1037, 8275, 3951, 1012, 1001, 15653, 24456, 2361, 1001, 10643, 29278, 17062, 2368, 102])\n",
      "Row(id=UUID('1bfcf253-7ff5-4b02-b00f-f3a08e521eb5'), label='2', tokens=[101, 1030, 5310, 2027, 3477, 2017, 1029, 1001, 8398, 11387, 11387, 8299, 102])\n",
      "Row(id=UUID('2d52993d-b933-4383-a7bd-24c43ba0032f'), label='0', tokens=[101, 3422, 1996, 1001, 4883, 3207, 20179, 11387, 11387, 2007, 2033, 2444, 3892, 2012, 1021, 1024, 3429, 7610, 20116, 2102, 1012, 2049, 2633, 2182, 1010, 15854, 999, 2040, 1005, 1055, 3201, 1029, 1001, 8398, 11387, 11387, 8299, 102])\n",
      "Row(id=UUID('ebfa5aa9-dedb-4996-9afe-d66300e2467e'), label='2', tokens=[101, 1030, 5310, 1045, 1521, 1049, 1037, 9421, 25423, 2860, 2266, 1012, 2017, 1521, 2128, 4637, 2330, 6645, 2029, 1027, 2896, 12678, 1012, 4637, 4491, 2006, 10558, 1012, 2026, 3428, 3233, 2005, 2256, 5210, 1004, 23713, 1025, 2005, 1001, 8398, 11387, 11387, 1012, 9245, 11344, 16717, 21519, 6293, 2545, 2035, 2058, 2256, 4497, 1012, 25423, 2860, 2003, 1037, 29591, 2000, 1996, 3915, 1012, 102])\n",
      "Row(id=UUID('116c019c-66f8-4063-8ddd-92e0d30a75b5'), label='2', tokens=[101, 1030, 5310, 8398, 3530, 2000, 2031, 2010, 5551, 7039, 1012, 1012, 1998, 7226, 2368, 2038, 4188, 1012, 2061, 2521, 1010, 7226, 2368, 2038, 7303, 1037, 3338, 2296, 2382, 8117, 2015, 1010, 2038, 4188, 1037, 4319, 3231, 1998, 2085, 2038, 4188, 2000, 2022, 7039, 2005, 2151, 4816, 5733, 2008, 2089, 4681, 2032, 1999, 2010, 5981, 1058, 8398, 1012, 2017, 2079, 1996, 13847, 8785, 999, 102])\n",
      "Row(id=UUID('943f30c0-6e0f-4d38-beb8-7a318ba6be0c'), label='1', tokens=[101, 1030, 5310, 1523, 1037, 3842, 2008, 5300, 2049, 14310, 2682, 2049, 6481, 2574, 12386, 2119, 1524, 16551, 1015, 1013, 2322, 1013, 4052, 1012, 2057, 2085, 2031, 1037, 2343, 2007, 2053, 6481, 1010, 2053, 5300, 1010, 2053, 26452, 1012, 2002, 10220, 2000, 3932, 1996, 4552, 1012, 2057, 2442, 3789, 8398, 1004, 23713, 1025, 2035, 2040, 2490, 2032, 2041, 1001, 3789, 16558, 23361, 8820, 3726, 14074, 14735, 102])\n",
      "Row(id=UUID('5bdfcdcb-272a-4a2f-aec4-adb09521c527'), label='2', tokens=[101, 1030, 5310, 1030, 5310, 1030, 5310, 10699, 6689, 2030, 2041, 1998, 2041, 4688, 1010, 2030, 2119, 999, 1001, 8398, 11387, 11387, 102])\n",
      "Row(id=UUID('d72a0064-d8cb-4946-8f5a-3a305d24e8a6'), label='2', tokens=[101, 1030, 5310, 1030, 5310, 2008, 1005, 1055, 2138, 2002, 1005, 1055, 1037, 4167, 27283, 2304, 1001, 7672, 999, 2002, 1005, 1055, 2037, 9413, 13033, 2879, 1012, 3475, 1005, 1056, 2054, 1996, 1001, 8037, 2515, 1010, 1998, 1010, 2224, 2068, 2004, 17395, 2015, 2000, 4608, 2060, 10823, 999, 10823, 2123, 1005, 1056, 5223, 2060, 2111, 1010, 4983, 2027, 2024, 2919, 1010, 2008, 1005, 1055, 2025, 14398, 999, 102])\n",
      "Row(id=UUID('79d5f6f1-9288-40d2-b0ac-6b88da38cc6a'), label='1', tokens=[101, 3533, 7226, 2368, 2071, 4530, 2757, 2006, 1996, 2754, 3892, 1999, 2392, 1997, 1996, 2878, 2088, 1998, 1045, 2052, 2145, 3789, 2005, 2032, 2138, 1001, 6616, 24456, 2361, 102])\n",
      "Row(id=UUID('36163248-2456-4e4e-bc34-ce6b611999f8'), label='0', tokens=[101, 2017, 2657, 2009, 2034, 2013, 1996, 1001, 17183, 3207, 20179, 1010, 1030, 5310, 2001, 1523, 2046, 1524, 1030, 5310, 2077, 2009, 2001, 4658, 1012, 2134, 1521, 1056, 2113, 15941, 7671, 2008, 2126, 1012, 102])\n",
      "Row(id=UUID('7e707899-26df-4c98-b5b0-afc3874bd90e'), label='0', tokens=[101, 2061, 4166, 2098, 2000, 2131, 2000, 3113, 2925, 1057, 1012, 1055, 1012, 16360, 1030, 5310, 1004, 23713, 1025, 1057, 1012, 1055, 1012, 12411, 1012, 1030, 5310, 2012, 2340, 1024, 2382, 2651, 2012, 2962, 6374, 2380, 1999, 19243, 4674, 3077, 1012, 1001, 12609, 12260, 7542, 102])\n",
      "Row(id=UUID('ffab3746-24a4-4e69-b3ae-f3f71d253fbb'), label='1', tokens=[101, 1030, 5310, 1030, 5310, 2065, 2122, 2024, 1996, 9804, 1010, 1045, 4060, 21911, 2050, 1012, 1045, 1005, 1049, 7704, 2000, 1030, 5310, 2138, 1045, 1005, 1049, 1001, 3516, 1012, 2021, 2428, 1045, 2074, 2215, 9444, 7126, 2149, 2663, 1012, 1001, 3789, 5833, 10760, 3995, 2361, 1001, 3789, 16558, 23361, 8820, 3726, 14074, 14735, 102])\n",
      "Row(id=UUID('985711a2-d997-4543-bb13-46d832f028c2'), label='0', tokens=[101, 3666, 1996, 29300, 2278, 1001, 29300, 2278, 11387, 11387, 102])\n",
      "Row(id=UUID('8bc447fc-67d4-4bff-b25b-7bb0a1cbcf0b'), label='0', tokens=[101, 1030, 5310, 1030, 5310, 1998, 2066, 2017, 1010, 2027, 2024, 2035, 5490, 27020, 2232, 1001, 2175, 2361, 29268, 2015, 2040, 2097, 2022, 1999, 4000, 2006, 2602, 2305, 1012, 102])\n",
      "Row(id=UUID('2933f1b9-150e-4dce-a87b-bc68e41593f6'), label='1', tokens=[101, 1996, 4507, 22552, 2088, 1997, 1996, 29300, 2278, 2003, 3432, 14726, 1012, 2073, 2003, 2023, 2637, 2027, 2024, 3331, 2055, 1010, 1998, 2040, 2003, 2877, 2009, 1029, 1001, 29300, 2278, 11387, 11387, 102])\n",
      "Row(id=UUID('21d25446-064b-426d-934b-7feb84547b57'), label='1', tokens=[101, 1045, 2387, 1037, 1001, 23848, 2050, 2651, 2007, 1010, 1000, 2053, 15806, 999, 1000, 12509, 4993, 2006, 1996, 5725, 5867, 1997, 2010, 4744, 1012, 2017, 3191, 2008, 11178, 1012, 2002, 3158, 9305, 3550, 2010, 2219, 4744, 2000, 2191, 2008, 2022, 2124, 1012, 2023, 2003, 1996, 1030, 5310, 1001, 23848, 2050, 11387, 11387, 2929, 1999, 1037, 12264, 18223, 1012, 3531, 2393, 2149, 1012, 1001, 3789, 17062, 2368, 11387, 11387, 102])\n",
      "Row(id=UUID('3d23bf3e-49c8-4bf3-b74a-69c9dd229c2c'), label='0', tokens=[101, 1030, 5310, 1030, 5310, 22905, 6732, 2016, 1005, 1055, 8550, 11071, 2011, 2478, 4003, 2072, 2000, 2562, 2841, 7882, 1012, 2035, 2016, 1005, 1055, 2428, 2725, 1999, 4852, 4003, 2072, 1005, 1055, 8771, 1012, 1045, 2347, 1005, 1056, 2206, 2077, 1010, 2021, 1045, 2572, 2085, 1012, 2562, 2039, 1996, 2204, 2147, 1030, 5310, 1010, 2057, 2288, 20760, 1012, 100, 1001, 3789, 16558, 5657, 102])\n",
      "Row(id=UUID('69105871-94f5-4bf5-94d1-76d106305a28'), label='1', tokens=[101, 1045, 2064, 1005, 1056, 3524, 2005, 1996, 2034, 1001, 5981, 1010, 2043, 1030, 5310, 1006, 2040, 1996, 1030, 5310, 2038, 6845, 3709, 1000, 17056, 3533, 1000, 2030, 1000, 28767, 3533, 1000, 1007, 14352, 2078, 20735, 1996, 2969, 10116, 1000, 6540, 11067, 1000, 1030, 5310, 1012, 2037, 2171, 4214, 2003, 6069, 2067, 10273, 1001, 8164, 9365, 5051, 1001, 7226, 2368, 11387, 11387, 102])\n",
      "Row(id=UUID('04ed87bd-d086-4d0e-9f2f-a3de024dff66'), label='1', tokens=[101, 1030, 5310, 2562, 2039, 1996, 2307, 2147, 999, 1001, 3789, 16558, 23361, 8820, 3726, 14074, 14735, 1001, 7226, 2368, 8167, 6935, 11387, 11387, 1001, 11238, 24138, 8189, 2618, 16558, 5657, 1001, 2630, 10422, 28987, 11387, 11387, 8299, 102])\n",
      "Row(id=UUID('d43b45cb-bfba-4734-9995-71edf4f20b29'), label='2', tokens=[101, 1030, 5310, 1030, 5310, 2292, 1005, 1055, 2404, 2009, 2035, 2006, 1996, 2795, 1012, 8398, 1005, 1055, 5651, 1012, 3533, 7226, 2368, 1005, 1055, 1012, 4477, 7226, 2368, 1005, 1055, 1012, 1998, 2216, 1997, 2010, 2567, 1998, 5727, 2004, 2092, 1011, 2035, 1997, 3183, 3711, 2000, 2031, 19727, 2013, 4822, 3533, 2218, 1012, 102])\n",
      "Row(id=UUID('83355760-fc1c-4d9f-aa40-5f5ed0eb6ad8'), label='2', tokens=[101, 1996, 2069, 2518, 2008, 15508, 2033, 2055, 2023, 2878, 2518, 2003, 2008, 8398, 2003, 4760, 2068, 1996, 2995, 2373, 1997, 1996, 8798, 1011, 2057, 2064, 2196, 1010, 2412, 3499, 2019, 8112, 2030, 1037, 5747, 2030, 1037, 7207, 2828, 1999, 1996, 2160, 2153, 1012, 1001, 23848, 2050, 102])\n",
      "Row(id=UUID('f13cf6c1-a692-4a0a-9a01-6da24e800508'), label='0', tokens=[101, 21911, 23278, 18752, 2015, 4227, 1023, 1010, 6079, 2575, 10474, 8771, 1999, 1996, 2197, 1020, 2847, 1010, 2005, 1037, 1014, 1012, 7449, 1003, 3623, 1010, 2007, 1037, 2783, 4175, 1997, 1019, 1010, 12963, 1010, 4185, 2575, 8771, 1012, 1001, 12609, 12260, 7542, 102])\n",
      "Row(id=UUID('32f0fbc2-8c81-4cc1-9389-06aba05a46ff'), label='1', tokens=[101, 1030, 5310, 1030, 5310, 1012, 1001, 17727, 5243, 7690, 24456, 2361, 1030, 5310, 3849, 2000, 2191, 12667, 2903, 2027, 1521, 2128, 11311, 2000, 2151, 3423, 1013, 7191, 25928, 1012, 2064, 1521, 1056, 3524, 1019, 8398, 2000, 2022, 2041, 1997, 2436, 1004, 23713, 1025, 2002, 4152, 1996, 7750, 2002, 17210, 999, 1001, 3789, 16558, 23361, 8820, 3726, 14074, 14735, 102])\n",
      "Row(id=UUID('375f99b5-a1cf-4b7a-a792-48e2ddf0fd7d'), label='2', tokens=[101, 1012, 1030, 5310, 2003, 24114, 1996, 4808, 1999, 17183, 2448, 3655, 2006, 8398, 5782, 2375, 1004, 23713, 1025, 2344, 1057, 3685, 2191, 2023, 2039, 17183, 2015, 2024, 1996, 2047, 3958, 3557, 1012, 2027, 2228, 2037, 8771, 2097, 2903, 1996, 3658, 2027, 11867, 7974, 1004, 23713, 1025, 4392, 2037, 12849, 4747, 4681, 4841, 2024, 8300, 999, 17183, 2015, 2031, 2668, 2006, 2037, 2398, 999, 1001, 100, 102])\n",
      "Row(id=UUID('b4e912f2-5782-4dd7-9dc6-f9478517cd4b'), label='2', tokens=[101, 2065, 2017, 2064, 6186, 2017, 2064, 3789, 1999, 2711, 2017, 2175, 2833, 6023, 2017, 2064, 3789, 1999, 2711, 2017, 2064, 2175, 2000, 2377, 15213, 2017, 2064, 3789, 1999, 2711, 2017, 2064, 2175, 2000, 1996, 3509, 2017, 2064, 2175, 3789, 1999, 2711, 2017, 2064, 2147, 2017, 2064, 3789, 1999, 2711, 1001, 8398, 11387, 11387, 102])\n",
      "Row(id=UUID('2c6587f0-bd1d-47a0-ae89-21c241ca2a0c'), label='0', tokens=[101, 1030, 5310, 2156, 1001, 23409, 6793, 1010, 1996, 1001, 2175, 2361, 2003, 2746, 2005, 2115, 4409, 1012, 1000, 1008, 4278, 13780, 8243, 2011, 2012, 2546, 1012, 1000, 4125, 2039, 999, 6985, 1001, 23409, 999, 999, 999, 999, 999, 102])\n",
      "Row(id=UUID('326b9c68-da49-4db5-b12d-e87fc30dbc76'), label='1', tokens=[101, 1030, 5310, 2735, 11514, 1005, 2222, 2022, 2152, 2004, 1996, 17666, 12059, 1012, 2292, 2032, 5823, 1998, 6402, 1012, 2021, 2035, 7226, 2368, 2038, 2000, 2079, 2003, 2298, 6047, 1998, 5681, 2360, 1000, 22017, 999, 1000, 102])\n",
      "Row(id=UUID('aab355d8-7237-4c70-b402-7db9907ddaf8'), label='0', tokens=[101, 1030, 5310, 1030, 5310, 1030, 5310, 1045, 1521, 1049, 2025, 1037, 2502, 7226, 2368, 5470, 1010, 2021, 1045, 1521, 1049, 2214, 2438, 2000, 3342, 2043, 16233, 1004, 23713, 1025, 12667, 2499, 2362, 1010, 2000, 2830, 1996, 2406, 1012, 1996, 2088, 9768, 2149, 1012, 8398, 2038, 4055, 2149, 9249, 1004, 23713, 1025, 1996, 2088, 2085, 2119, 11839, 2015, 2005, 2149, 1004, 23713, 1025, 2003, 4487, 12693, 6129, 2993, 2013, 2149, 1010, 10317, 1004, 23713, 1025, 6719, 1012, 8398, 2003, 2053, 2204, 1030, 2023, 1012, 8299, 102])\n",
      "Row(id=UUID('a22045a4-6adc-4b89-bc82-e846a04b1d68'), label='2', tokens=[101, 2613, 5280, 19058, 24456, 2361, 1024, 2064, 2017, 2903, 2054, 1521, 1055, 6230, 999, 1029, 2027, 2507, 3533, 5342, 2078, 1521, 1996, 3980, 1010, 1998, 2002, 9631, 2068, 2019, 3437, 999, 8299, 102])\n",
      "Row(id=UUID('d152d1ba-d75f-4ab0-be18-565202c4dd80'), label='2', tokens=[101, 1030, 5310, 1030, 5310, 2021, 1996, 2865, 2758, 2049, 9379, 1012, 1048, 2863, 2080, 1010, 5796, 2213, 2003, 1037, 8257, 1010, 4099, 1997, 1996, 2111, 1012, 1001, 8398, 11387, 11387, 102])\n",
      "Row(id=UUID('e429e662-3506-4a97-943a-74ac2c971fab'), label='0', tokens=[101, 5186, 4069, 2767, 1011, 1997, 1011, 7226, 2368, 6554, 20710, 4801, 11245, 5833, 2045, 1999, 1996, 1999, 24443, 18802, 2678, 1012, 1001, 17183, 8663, 15338, 3258, 102])\n",
      "Row(id=UUID('1d544c8d-e5dd-411a-8e1b-bb77bf04af6a'), label='1', tokens=[101, 1030, 5310, 1045, 2572, 1012, 2057, 3685, 5788, 2178, 1018, 2086, 1997, 1037, 8398, 8798, 1012, 2256, 7072, 2097, 3280, 3294, 1010, 1998, 2057, 2097, 2468, 2019, 8285, 17510, 2406, 1012, 2057, 2442, 3789, 2035, 1996, 9099, 25434, 4126, 16229, 2015, 2041, 2077, 2009, 2003, 2205, 2397, 1012, 1001, 3789, 16558, 23361, 8913, 26379, 24158, 15864, 24376, 1001, 3789, 16558, 23361, 8820, 3726, 14074, 14735, 102])\n",
      "Row(id=UUID('a76f5329-3023-468b-9e8d-c62ea2ee3c3c'), label='2', tokens=[101, 1030, 5310, 1030, 5310, 5292, 3270, 1011, 2016, 2003, 2525, 4911, 2115, 2192, 1012, 2054, 2097, 2016, 3338, 2006, 2017, 2279, 1029, 1001, 8398, 11387, 11387, 1001, 28767, 5558, 2063, 8299, 102])\n",
      "Row(id=UUID('bb614199-6663-40b1-9b44-4b744d99cd3b'), label='0', tokens=[101, 2008, 2617, 2043, 2017, 5382, 1030, 5310, 2003, 2941, 2084, 2891, 1012, 1012, 1012, 1012, 1001, 4883, 3207, 20179, 1001, 5981, 11387, 11387, 1001, 12609, 3207, 20179, 1001, 14379, 1001, 7226, 2368, 8167, 6935, 11387, 11387, 1001, 8398, 11387, 11387, 1001, 7226, 2368, 11387, 11387, 8299, 102])\n",
      "Row(id=UUID('31606f86-eefd-4012-a6dc-e8648fb26481'), label='0', tokens=[101, 2330, 2023, 1001, 17183, 8663, 15338, 3258, 27263, 2005, 1037, 8242, 4474, 999, 1008, 11307, 2716, 2000, 2017, 2011, 1030, 5310, 10245, 7507, 2102, 1001, 12536, 15185, 2290, 8299, 102])\n",
      "Row(id=UUID('ce124d48-f8fb-4429-8f03-93b2a024c265'), label='1', tokens=[101, 2023, 4613, 2011, 1001, 3533, 17062, 2368, 2003, 2035, 2008, 1045, 2359, 1012, 3488, 1010, 2054, 2027, 2024, 1010, 2129, 2027, 2097, 2191, 2068, 4148, 1012, 2023, 3475, 1521, 1056, 1996, 4613, 1997, 1037, 24726, 1010, 2021, 1037, 2655, 2000, 2895, 1998, 1037, 4872, 1012, 102])\n",
      "Row(id=UUID('1c3dd59f-ff1d-47d1-b5c2-bb35fc770d30'), label='1', tokens=[101, 1001, 14303, 13699, 12083, 19341, 3619, 5833, 11387, 11387, 1001, 3789, 16558, 23361, 8820, 3726, 14074, 14735, 1030, 5310, 1030, 5310, 10643, 1010, 2017, 1005, 2128, 2589, 999, 8299, 102])\n",
      "Row(id=UUID('f7d606b0-d72d-482d-89ab-ace6cd0bb5c3'), label='2', tokens=[101, 1015, 1013, 9467, 2006, 1030, 5310, 4352, 1996, 24949, 3189, 2011, 1030, 5310, 2128, 1001, 14379, 11387, 11387, 4647, 2198, 2056, 2008, 1001, 8398, 5117, 1999, 2171, 4214, 2247, 1059, 1013, 3533, 18411, 1996, 1001, 3606, 1001, 2755, 2003, 2008, 2069, 7226, 2368, 2012, 2102, 2243, 2094, 1999, 2171, 4214, 1010, 7226, 2368, 2001, 2034, 2000, 2012, 2102, 2243, 7714, 1010, 2034, 2000, 2171, 4214, 1006, 8398, 102])\n",
      "Row(id=UUID('cda72c57-123e-45fe-96e7-737aa79a3c2d'), label='1', tokens=[101, 1030, 5310, 1030, 5310, 11504, 2180, 1005, 1056, 2031, 2000, 5674, 2005, 2205, 2146, 1012, 1012, 1012, 1001, 3533, 17062, 2368, 11387, 11387, 1001, 3533, 17062, 2368, 1001, 8398, 23350, 102])\n",
      "Row(id=UUID('8e531b35-af05-4dcb-9ae7-4e09f27c776c'), label='0', tokens=[101, 1056, 2140, 2003, 2440, 1997, 1996, 4883, 5981, 1998, 1045, 2113, 6830, 2005, 8398, 2003, 6719, 1996, 5409, 2518, 2000, 2079, 2021, 2064, 2619, 2425, 2033, 1996, 3563, 2015, 1029, 1045, 1521, 2310, 2042, 2667, 2000, 2424, 2041, 1013, 3599, 1013, 2054, 2052, 4148, 2000, 2149, 2065, 2002, 1521, 1055, 2700, 1998, 1045, 1521, 1049, 15958, 5191, 1998, 5457, 1024, 1006, 102])\n",
      "Row(id=UUID('764700c4-8c48-4c88-81c9-7471f24a8c75'), label='1', tokens=[101, 1030, 5310, 1523, 1996, 2088, 1521, 1055, 2087, 4795, 2158, 1524, 1012, 2984, 8398, 1012, 2016, 26304, 2009, 1012, 1523, 2438, 2003, 2196, 2438, 1524, 1001, 15653, 24456, 2361, 1001, 8398, 14268, 29201, 8844, 1001, 7226, 2368, 11387, 11387, 1001, 23848, 2050, 2038, 2042, 25857, 102])\n",
      "Row(id=UUID('a2d19599-c578-46cb-a3e0-e62d9d872115'), label='0', tokens=[101, 2154, 1015, 1997, 1996, 1001, 17183, 8663, 15338, 3258, 2003, 2028, 1996, 2808, 1012, 2054, 2064, 2057, 5987, 2651, 1012, 1012, 1012, 2062, 3658, 1012, 1001, 10556, 2290, 11387, 11387, 102])\n",
      "Row(id=UUID('ef0489f9-3ef9-4cf7-8bf4-75806da0394e'), label='2', tokens=[101, 1030, 5310, 7226, 2368, 2003, 20342, 2065, 2002, 2064, 1005, 1056, 5047, 1037, 13847, 3938, 3371, 5981, 1012, 1012, 1012, 2017, 5698, 8609, 2015, 1045, 8415, 1010, 2002, 2003, 2061, 18358, 2039, 1012, 102])\n",
      "Row(id=UUID('840393bd-9ecf-4bb4-adb6-d9255b55d6ea'), label='0', tokens=[101, 1001, 8112, 10930, 1010, 1010, 1010, 1010, 2017, 2020, 2343, 2005, 1022, 2086, 2339, 2134, 1521, 1056, 2017, 8081, 2035, 2023, 1010, 2009, 2022, 2066, 2115, 2343, 2594, 2100, 2196, 3047, 1001, 21911, 23278, 18752, 2015, 1001, 21911, 23278, 18752, 22747, 2953, 2615, 2361, 1001, 8112, 17311, 20915, 14621, 2618, 27900, 20744, 1001, 1040, 12273, 8663, 15338, 3258, 1001, 12609, 12260, 7542, 102])\n",
      "Row(id=UUID('1b4fcb31-2634-499b-9aef-a597e69b6eb4'), label='2', tokens=[101, 1030, 5310, 1030, 5310, 2672, 2016, 5807, 1521, 1056, 1997, 13322, 2039, 2007, 1037, 16216, 4360, 12412, 2000, 2131, 2014, 2034, 3105, 1001, 8398, 11387, 11387, 8299, 102])\n",
      "Row(id=UUID('af5c624f-2cc8-49ee-9ec8-0439ed5c5af2'), label='2', tokens=[101, 2012, 1996, 8398, 2724, 8320, 2651, 1999, 2474, 1001, 8398, 11387, 11387, 2292, 1521, 1055, 2735, 2662, 2417, 999, 8299, 102])\n",
      "Row(id=UUID('d8de9d0c-188c-4009-8f42-98d0c9373c2e'), label='0', tokens=[101, 100, 1996, 4018, 1024, 16360, 1012, 6945, 100, 1517, 2783, 1057, 1012, 1055, 1012, 4387, 1010, 2449, 3954, 1010, 1998, 8291, 1997, 1996, 3416, 7450, 1517, 1030, 5310, 2003, 1996, 3951, 9773, 2005, 2167, 3792, 1521, 1055, 6122, 7740, 2212, 1001, 13316, 17134, 1001, 13316, 18155, 1001, 29300, 2278, 11387, 11387, 8299, 102])\n",
      "Row(id=UUID('351787ed-2c84-42b9-9567-0776d7ad0440'), label='1', tokens=[101, 2130, 2065, 2017, 2123, 1005, 1056, 2066, 1001, 3533, 17062, 2368, 1010, 3789, 2005, 2032, 4312, 1012, 2065, 2002, 8289, 1999, 2436, 1012, 1012, 1012, 1001, 2343, 8167, 6935, 102])\n",
      "Row(id=UUID('74071dc0-b1f5-4e4c-8d04-28d7e19d1395'), label='0', tokens=[101, 1000, 2052, 2017, 3844, 2039, 2158, 1029, 1000, 2008, 2741, 2033, 100, 1001, 3533, 17062, 2368, 11387, 11387, 1001, 21911, 23278, 18752, 2015, 2615, 2361, 1001, 4883, 3207, 20179, 11387, 11387, 102])\n",
      "Row(id=UUID('977369b7-465b-4ef2-8cd1-81144c9d08e0'), label='2', tokens=[101, 3505, 7279, 3401, 2056, 1000, 22017, 10244, 2078, 5927, 2137, 4768, 1010, 2021, 1045, 2156, 2137, 2307, 2791, 1000, 10392, 3105, 2005, 1996, 1001, 29300, 2278, 11387, 11387, 1001, 2176, 5974, 29100, 2015, 1001, 3505, 11837, 3401, 1001, 10556, 2290, 11387, 11387, 1001, 2417, 16535, 11387, 11387, 1001, 23848, 2050, 1001, 11579, 19496, 3064, 102])\n",
      "Row(id=UUID('c5c1b1ec-0d18-43d2-84c6-c1112b9211af'), label='0', tokens=[101, 1030, 5310, 1030, 5310, 2061, 2106, 2017, 15734, 2224, 13425, 1997, 1037, 3345, 1010, 5318, 3121, 1010, 1998, 25007, 7318, 1010, 2030, 2001, 2009, 2074, 1037, 16507, 1029, 2023, 2003, 1037, 2210, 17109, 6195, 1012, 1012, 1012, 2017, 2113, 1010, 8042, 1012, 2323, 2057, 4553, 2845, 2030, 2446, 2000, 7374, 2005, 1996, 15336, 1029, 1001, 14870, 24456, 2361, 1001, 8398, 11387, 11387, 102])\n",
      "Row(id=UUID('4637f0d5-a34e-4c13-84c2-2257a2e4e9f0'), label='1', tokens=[101, 1030, 5310, 1030, 5310, 1030, 5310, 1030, 5310, 2040, 4122, 2000, 3693, 2033, 1999, 24260, 3436, 1002, 2321, 2000, 1996, 7226, 2368, 1013, 5671, 3049, 2651, 1029, 1001, 5417, 3527, 17305, 27439, 22916, 1001, 7226, 2368, 8167, 6935, 11387, 11387, 1001, 7226, 2368, 11387, 11387, 8299, 102])\n",
      "Row(id=UUID('2cd1b6ea-2ddb-4cf4-8e9c-b04acb45cf7e'), label='0', tokens=[101, 1030, 5310, 1030, 5310, 1030, 5310, 2429, 1016, 2115, 3579, 2967, 2055, 1015, 1013, 1017, 1997, 8112, 1011, 8398, 7206, 2933, 1016, 3789, 1018, 7226, 2368, 1012, 2065, 2023, 2003, 6149, 7226, 2368, 5222, 1999, 1037, 20148, 1012, 2065, 2023, 1001, 2003, 2941, 2322, 1003, 1010, 7226, 2368, 5222, 1999, 1037, 20148, 1012, 2130, 1019, 1003, 2168, 20148, 1012, 1001, 12609, 12260, 7542, 1001, 7226, 2368, 1001, 8398, 102])\n",
      "Row(id=UUID('305fdc88-2b5e-4484-a62a-8735702b1e65'), label='2', tokens=[101, 1030, 5310, 2009, 2442, 2031, 2042, 6429, 3773, 2009, 1999, 2711, 1010, 6876, 2066, 2023, 2507, 2033, 13020, 8569, 25370, 999, 100, 1001, 2643, 13510, 21559, 22420, 2050, 1001, 8398, 11387, 11387, 102])\n",
      "Row(id=UUID('8b1308e5-5552-42d4-89fd-ec94838ea421'), label='0', tokens=[101, 2040, 2323, 2663, 1996, 1001, 3537, 6488, 2005, 1001, 8962, 2271, 1999, 12609, 1029, 1001, 12609, 12260, 7542, 1001, 3915, 1006, 1001, 2128, 2102, 28394, 2102, 2005, 7099, 2946, 1007, 1001, 7226, 2368, 11387, 11387, 1001, 15941, 11387, 11387, 1001, 6031, 11387, 11387, 1001, 6969, 11387, 11387, 102])\n",
      "Row(id=UUID('f81fe5d7-33d8-404e-9b0b-81fb85ccad0c'), label='0', tokens=[101, 2292, 1521, 1055, 2831, 2055, 1523, 10866, 2162, 1524, 8398, 2865, 6325, 2021, 2987, 1521, 1056, 5254, 2002, 1521, 1055, 7510, 9767, 2006, 14717, 1004, 23713, 1025, 4762, 2075, 1996, 2162, 1999, 13968, 2029, 2003, 4288, 2336, 2004, 2057, 3713, 999, 1030, 5310, 4282, 2023, 2021, 2003, 3432, 4688, 2004, 5156, 10866, 2162, 2003, 2200, 2172, 1999, 2440, 3466, 999, 1001, 29300, 2278, 11387, 11387, 102])\n",
      "Row(id=UUID('e95f1b29-3b1b-4f9a-acb2-21a1f529d426'), label='2', tokens=[101, 1030, 5310, 1030, 5310, 2008, 1521, 1055, 2986, 1010, 2138, 2023, 2097, 2031, 2115, 3611, 2663, 1999, 1037, 20148, 1999, 2281, 1012, 1001, 8398, 11387, 11387, 102])\n",
      "Row(id=UUID('f5767306-0b9a-4e47-bdd9-f9c467e06167'), label='0', tokens=[101, 100, 1024, 2305, 100, 4105, 5609, 100, 2444, 2085, 1024, 2053, 27932, 13867, 1001, 17183, 8663, 15338, 3258, 1001, 7226, 2368, 8167, 6935, 8299, 102])\n",
      "Row(id=UUID('db4c8330-e338-475b-97f0-84f612cda75b'), label='2', tokens=[101, 1030, 5310, 10643, 2667, 2000, 8954, 1996, 2602, 1029, 2053, 1010, 2008, 2003, 2054, 2017, 5698, 8609, 2015, 2024, 2667, 2000, 2079, 1012, 5653, 1011, 1999, 17069, 2024, 3811, 18002, 2000, 9861, 1998, 2017, 2113, 2023, 1012, 2017, 2064, 1005, 1056, 3786, 8398, 2302, 16789, 1010, 1998, 2057, 2113, 2023, 1012, 1001, 8398, 11387, 11387, 102])\n",
      "Row(id=UUID('55621630-17f9-4b7c-859e-a2ae4a82fdaa'), label='0', tokens=[101, 1001, 17183, 8663, 15338, 3258, 12654, 1024, 1000, 2317, 2111, 2024, 9643, 1010, 18904, 4221, 1029, 2292, 1005, 1055, 12324, 2438, 1997, 1996, 2717, 1997, 1996, 2088, 2000, 29454, 10421, 2122, 9643, 12461, 2041, 1997, 4598, 999, 1000, 102])\n",
      "Row(id=UUID('9fc38e25-8e7b-4258-9fa1-9da4f9554c8d'), label='2', tokens=[101, 1030, 5310, 2129, 2146, 2127, 2017, 5256, 2039, 1998, 2907, 1030, 5310, 1010, 1030, 5310, 1010, 1998, 1030, 5310, 26771, 1029, 2821, 3524, 2008, 1005, 1055, 2157, 1010, 2009, 1005, 1055, 1030, 5310, 6346, 1012, 1001, 1044, 22571, 10085, 6935, 2100, 1001, 12609, 12260, 7542, 1001, 2637, 8873, 12096, 1001, 8037, 5280, 13535, 12069, 1001, 2406, 7840, 19362, 3723, 102])\n",
      "Row(id=UUID('3cebca63-411b-4a39-8bb3-91ac795a520f'), label='2', tokens=[101, 1030, 5310, 1030, 5310, 2672, 2027, 2323, 2298, 2046, 2032, 2108, 2006, 26646, 1521, 1055, 4946, 2656, 2335, 1012, 1998, 2106, 2009, 2202, 7207, 2000, 10386, 2479, 1029, 1001, 8398, 11387, 11387, 1001, 5256, 6279, 14074, 14735, 102])\n",
      "Row(id=UUID('3b5edd5f-9108-4c4d-ab82-712921677837'), label='0', tokens=[101, 1030, 5310, 2002, 1521, 1055, 2119, 999, 1001, 8398, 11387, 11387, 102])\n",
      "Row(id=UUID('0e584981-64aa-4b2f-891c-73e45c9dcace'), label='2', tokens=[101, 1030, 5310, 2343, 8398, 999, 2348, 2027, 1521, 2310, 2699, 2066, 3109, 999, 1001, 23848, 2050, 102])\n",
      "Row(id=UUID('266f3fff-d2dd-49dc-b8da-90f1327c2978'), label='0', tokens=[101, 7226, 2368, 2074, 2056, 1000, 3071, 2842, 2106, 2009, 1000, 2000, 1996, 11804, 7450, 1012, 1001, 17183, 3207, 20179, 102])\n",
      "Row(id=UUID('29a77a69-b6be-424e-8402-75973f972aa8'), label='0', tokens=[101, 1030, 5310, 1030, 5310, 1030, 5310, 1030, 5310, 2009, 2515, 2342, 2000, 2022, 1037, 2304, 2450, 1012, 3422, 2054, 2097, 4148, 2065, 2009, 1521, 1055, 2025, 1012, 2123, 1521, 1056, 2202, 1996, 2304, 2450, 3789, 2005, 4379, 1012, 1037, 3308, 4060, 2071, 4315, 12502, 1001, 7226, 2368, 11387, 11387, 17057, 1998, 1996, 1001, 3537, 2283, 2005, 3864, 2000, 2272, 1012, 2017, 2064, 9231, 2023, 1056, 28394, 2102, 1012, 102])\n",
      "Row(id=UUID('e89e446d-4c98-47ce-97b7-1f4ff90fabd5'), label='1', tokens=[101, 1030, 5310, 2027, 2024, 2667, 2673, 1998, 2059, 2070, 1012, 2057, 3685, 2022, 6247, 2091, 1012, 2057, 2097, 2035, 3789, 999, 1001, 3789, 16558, 23361, 8820, 3726, 14074, 14735, 1001, 3789, 16558, 24997, 9626, 12079, 2860, 6806, 102])\n",
      "Row(id=UUID('bbfbecb9-4329-4785-b02b-845a41c0caf8'), label='0', tokens=[101, 2090, 1001, 6221, 24456, 2361, 9694, 1037, 3622, 3433, 1998, 1996, 1001, 8037, 12549, 23142, 3645, 1013, 5296, 2091, 3121, 1010, 2256, 2406, 2003, 3753, 2005, 7071, 1998, 2942, 2162, 1012, 2065, 2057, 4342, 2000, 4952, 2000, 2169, 2060, 1010, 2035, 1997, 2023, 2071, 2022, 9511, 1012, 102])\n",
      "Row(id=UUID('c82663bc-ca72-4dcc-9cf3-ba4b9940a30d'), label='0', tokens=[101, 1999, 2035, 27994, 1010, 1996, 1001, 4883, 3207, 20179, 2197, 2305, 2001, 6517, 1012, 6586, 1010, 2292, 1521, 1055, 3613, 2000, 2022, 1999, 7083, 2005, 2343, 8398, 1010, 7226, 2368, 1998, 2256, 2406, 1012, 102])\n",
      "Row(id=UUID('3e6bbe09-cfc9-41ea-9605-abe861c75dcc'), label='2', tokens=[101, 1016, 1013, 2008, 2037, 7179, 2097, 1001, 5256, 6279, 10073, 1004, 23713, 1025, 1001, 3328, 9497, 19699, 5358, 3207, 5302, 23423, 1001, 3328, 9497, 19699, 5358, 3207, 5302, 23423, 29278, 22507, 1001, 3328, 9497, 1001, 3328, 3406, 23301, 9527, 24415, 2890, 14289, 16558, 5555, 3619, 1001, 3328, 3406, 23301, 9527, 24415, 3995, 2361, 1001, 3789, 24456, 2361, 1001, 3789, 27058, 19362, 3723, 1001, 3789, 27058, 19362, 3723, 1001, 10556, 2290, 1001, 23848, 2050, 1001, 10507, 4140, 1001, 29525, 2015, 1001, 2510, 1001, 3008, 1001, 2308, 1001, 3611, 2015, 1001, 3566, 2015, 1001, 4715, 5302, 2213, 1001, 8991, 2100, 1001, 8991, 2480, 102])\n",
      "Row(id=UUID('28caadf4-1c3a-4754-8ca0-02f367853c1f'), label='2', tokens=[101, 1030, 5310, 7226, 2368, 2003, 15174, 8299, 102])\n",
      "Row(id=UUID('c6d11cf8-a57e-4400-9be6-b6a0266b0c23'), label='1', tokens=[101, 1030, 5310, 100, 1001, 3789, 16558, 23361, 8820, 3726, 14074, 14735, 100, 1045, 2031, 2042, 2206, 1030, 5310, 2005, 1037, 2096, 1012, 2065, 2017, 2030, 1045, 7271, 2056, 1996, 4895, 6342, 5910, 5794, 10711, 3064, 4689, 2004, 7151, 100, 2477, 2002, 2758, 1010, 2057, 2052, 2022, 3844, 2091, 1999, 1037, 12251, 1012, 2027, 2024, 2041, 1004, 23713, 1025, 2041, 3658, 999, 2057, 2342, 1996, 3606, 1004, 23713, 1025, 1037, 17824, 3003, 1012, 1001, 3789, 16558, 5657, 11387, 11387, 102])\n",
      "Row(id=UUID('d92ef22e-337e-4319-b3b5-5e421bfb2ea2'), label='2', tokens=[101, 1030, 5310, 21877, 10483, 2072, 2074, 2081, 2017, 3297, 1012, 1012, 1001, 8398, 11387, 11387, 102])\n",
      "Row(id=UUID('ef3b99f6-eb04-4d92-8b06-1d60d97d46f5'), label='0', tokens=[101, 1045, 2428, 3335, 1001, 3870, 9028, 7389, 2014, 3754, 2000, 4863, 3471, 1998, 2129, 2000, 9611, 2068, 2020, 2025, 2069, 9414, 2021, 2036, 18988, 1001, 17183, 3207, 20179, 8299, 102])\n",
      "Row(id=UUID('6bde5eb6-e7c1-427c-9702-2184bbc94041'), label='1', tokens=[101, 1030, 5310, 6640, 2420, 1001, 2057, 29602, 2140, 22994, 2063, 1001, 3789, 16558, 23361, 8913, 26379, 24158, 15864, 24376, 1001, 3789, 16558, 5657, 11387, 11387, 1001, 3789, 16558, 23361, 8820, 3726, 14074, 14735, 11387, 11387, 1001, 7226, 2368, 8167, 6935, 11387, 11387, 1001, 7226, 2368, 8167, 6935, 11387, 11387, 8653, 24198, 8299, 102])\n",
      "Row(id=UUID('f42a39d6-4da1-4bb4-97d5-ee2fa8eee8b0'), label='2', tokens=[101, 1001, 3533, 27698, 5178, 2078, 2734, 5587, 21673, 2140, 2000, 2994, 18920, 1001, 5981, 11387, 11387, 1001, 4319, 2098, 6279, 17062, 2368, 102])\n",
      "Row(id=UUID('7821bd7f-6cad-4a35-b4b0-23b31d5a7bb4'), label='0', tokens=[101, 6031, 3957, 24387, 2000, 1047, 4135, 25987, 2906, 2153, 1012, 2003, 2016, 2457, 2075, 2014, 2000, 7496, 2014, 2041, 1997, 1996, 3049, 1029, 1001, 17183, 3207, 20179, 1001, 3537, 3207, 20179, 102])\n",
      "Row(id=UUID('7d92207d-ae5b-45b6-9dd7-b8ae4277b3f4'), label='0', tokens=[101, 2085, 2008, 1030, 5310, 2003, 3985, 1996, 3537, 9773, 1010, 2202, 1037, 2298, 2012, 2010, 1001, 7521, 4884, 18402, 1012, 1001, 17183, 8663, 15338, 3258, 1001, 3537, 8663, 15338, 3258, 8299, 102])\n",
      "Row(id=UUID('563b5a9d-8905-41fb-bbd7-8215d8901d53'), label='1', tokens=[101, 1030, 5310, 2061, 6429, 1012, 2002, 2003, 2005, 28632, 5905, 1012, 1045, 2123, 1521, 1056, 2156, 2339, 2057, 2130, 2507, 2032, 2349, 2832, 1012, 2057, 2442, 12475, 1996, 11963, 1998, 7868, 2122, 2111, 2024, 5905, 2013, 1996, 2927, 2241, 2006, 2037, 2576, 12912, 2015, 1012, 2008, 3241, 2003, 2054, 3084, 2149, 2307, 1012, 1001, 7672, 1001, 7226, 2368, 8167, 6935, 11387, 11387, 13122, 10696, 14074, 14735, 102])\n",
      "Row(id=UUID('21f848d0-1879-4025-b80e-4b6718c4d875'), label='0', tokens=[101, 1030, 5310, 2115, 2388, 2003, 2019, 4763, 21877, 3527, 1012, 1012, 1012, 25312, 2480, 3393, 27304, 1010, 2008, 19424, 2678, 2097, 2022, 17653, 2739, 2574, 1012, 2036, 2057, 2097, 2196, 5293, 12867, 1998, 3841, 5603, 16103, 1012, 1045, 4687, 2129, 2116, 16795, 2594, 13549, 2017, 1005, 2310, 2042, 2000, 9295, 999, 1996, 6548, 7425, 2121, 7207, 2155, 2024, 2183, 2091, 1012, 1001, 1059, 27767, 2487, 27767, 2050, 102])\n",
      "Row(id=UUID('11ccba31-2078-4b04-9025-ca1a40f3dbd3'), label='2', tokens=[101, 2123, 1521, 1056, 5293, 1012, 1030, 5310, 2038, 3271, 2216, 1999, 2342, 2011, 8402, 12163, 6666, 1010, 2025, 8037, 1012, 3342, 2008, 2043, 2017, 2175, 2000, 1996, 14592, 1999, 2281, 1012, 1996, 8037, 2052, 2738, 2377, 13593, 2576, 2399, 2084, 2490, 2172, 2342, 4335, 2005, 1996, 2137, 2111, 999, 1001, 8398, 11387, 11387, 102])\n",
      "Row(id=UUID('77f3ba09-723d-4ce2-9fde-6ccb71624ea1'), label='0', tokens=[101, 3537, 3078, 6830, 6808, 1024, 2047, 7035, 12055, 1024, 2861, 1003, 1006, 1011, 1015, 1007, 10007, 8004, 28872, 1024, 2538, 1003, 1006, 1009, 1018, 1007, 7226, 2368, 1024, 2260, 1003, 1006, 1011, 1015, 1007, 6031, 1024, 2260, 1003, 1006, 1009, 1015, 1007, 1047, 4135, 25987, 2906, 1024, 2340, 1003, 1006, 1027, 1007, 11721, 22414, 4103, 1024, 1019, 1003, 1006, 1011, 1015, 1007, 8675, 1024, 1018, 1003, 1006, 1011, 1016, 1007, 26261, 10532, 1024, 1015, 1003, 1006, 1011, 1015, 1007, 1016, 1013, 1019, 1011, 1030, 5310, 3431, 2007, 1016, 1013, 1018, 1001, 12609, 12260, 7542, 1001, 2047, 3511, 4523, 20908, 102])\n",
      "Row(id=UUID('5ef8e977-deed-4e20-8d81-47c232bb160e'), label='0', tokens=[101, 6057, 2129, 15941, 2288, 4457, 2006, 1996, 10184, 3021, 1006, 2748, 1010, 2002, 2106, 2886, 7226, 2368, 2006, 1996, 5712, 2162, 1007, 1010, 2021, 2196, 2288, 1037, 3382, 2000, 3433, 2000, 2009, 1012, 1001, 17183, 3207, 20179, 102])\n",
      "Row(id=UUID('ce3594ac-87dd-4e45-a36a-669b87a478d7'), label='0', tokens=[101, 1001, 6221, 3501, 6824, 2361, 1010, 1030, 5310, 1010, 1001, 23848, 2050, 1005, 10659, 2921, 1005, 1029, 2025, 4498, 1012, 2182, 2024, 2274, 4979, 8654, 16393, 2015, 8398, 3631, 1012, 8299, 102])\n",
      "Row(id=UUID('4201b528-ff89-484f-ba47-54b6aa0ce382'), label='0', tokens=[101, 3870, 6031, 2038, 2439, 2014, 3382, 2000, 2022, 1996, 2034, 6284, 2343, 1997, 1996, 2142, 2163, 1012, 1001, 17183, 3207, 20179, 102])\n",
      "Row(id=UUID('35222412-4b1c-4aa8-ae09-debedd38d038'), label='2', tokens=[101, 8112, 4796, 2003, 2074, 2707, 2893, 4895, 22401, 2989, 999, 9296, 4812, 2074, 21801, 13593, 8495, 5160, 2040, 6904, 4877, 7810, 2592, 2000, 2131, 27424, 2050, 1012, 2085, 1996, 3160, 2003, 2339, 1037, 5160, 4682, 1029, 2006, 6852, 1997, 3183, 1029, 1996, 8495, 2323, 2022, 14984, 1997, 2993, 999, 1001, 9296, 1001, 8112, 3654, 3686, 1001, 3607, 6806, 8528, 1001, 8398, 11387, 11387, 102])\n",
      "Row(id=UUID('251d3b3b-4daf-44a1-bfa4-2defddfa622f'), label='1', tokens=[101, 2292, 1996, 2501, 2265, 8299, 1045, 2106, 2025, 3789, 2005, 8398, 999, 1045, 2097, 3789, 2005, 1030, 5310, 1998, 2035, 17183, 2015, 2000, 3828, 2149, 2013, 2023, 10103, 999, 1001, 3789, 16558, 23361, 8820, 3726, 14074, 14735, 102])\n",
      "Row(id=UUID('2ad44faa-1568-4cb0-9520-ec1034a20b0c'), label='0', tokens=[101, 4060, 1037, 4001, 2679, 999, 21357, 1012, 2655, 1012, 10683, 1012, 1001, 12609, 12260, 7542, 102])\n",
      "Row(id=UUID('73649de6-0359-4699-8773-0c495cf434b5'), label='0', tokens=[101, 1030, 5310, 1030, 5310, 3464, 2000, 2022, 2464, 2000, 2054, 6698, 10643, 2097, 4681, 1998, 14863, 2102, 1998, 2292, 2023, 17328, 8398, 10083, 2637, 1998, 5853, 4221, 5762, 27865, 1997, 1996, 2627, 2000, 14033, 2637, 2046, 9535, 7747, 1012, 102])\n",
      "Row(id=UUID('4a4c432b-d0a4-49ac-976c-3bf73bd45e8b'), label='0', tokens=[101, 2079, 2057, 2228, 3870, 6031, 2097, 2022, 7226, 2368, 1005, 1055, 21210, 4060, 1029, 1001, 17183, 3207, 20179, 102])\n",
      "Row(id=UUID('61aa1668-e6b2-4a5a-a375-6547ac3d05eb'), label='0', tokens=[101, 1030, 5310, 2003, 2002, 2183, 2000, 2079, 1996, 6128, 13987, 23233, 3917, 3328, 2044, 1996, 5981, 1029, 2057, 2097, 2074, 2031, 2000, 3422, 2343, 8398, 1521, 1055, 2227, 2000, 2113, 2043, 1996, 2617, 6433, 2008, 7226, 2368, 12386, 2009, 1998, 1996, 5437, 4269, 2000, 2566, 4168, 3686, 1012, 102])\n",
      "Row(id=UUID('0ad7d190-b11a-4010-82a4-461b65484fe8'), label='2', tokens=[101, 1030, 5310, 3198, 1001, 21864, 18927, 3217, 28940, 29147, 8913, 2055, 2010, 2125, 5370, 6115, 2007, 2859, 1010, 1001, 20934, 6935, 2863, 1010, 1998, 2500, 4283, 2000, 1001, 3477, 4842, 13068, 5679, 2005, 2032, 1998, 8112, 2083, 1001, 4477, 17062, 2368, 999, 1001, 10454, 17062, 2368, 4593, 2038, 2053, 3314, 2007, 5938, 2122, 17800, 4227, 12256, 3111, 999, 1001, 23848, 2050, 1001, 10556, 2290, 1001, 8398, 11387, 11387, 1001, 3533, 4783, 26100, 2078, 999, 102])\n",
      "Row(id=UUID('30b40afd-ad1c-4051-8bc9-85173949d142'), label='0', tokens=[101, 3951, 2120, 4680, 1024, 14381, 8398, 18058, 2014, 4769, 1001, 29300, 2278, 11387, 11387, 8299, 102])\n",
      "Row(id=UUID('83c2b2a2-6dc8-4249-8c15-25ee8b64137d'), label='1', tokens=[101, 1030, 5310, 2017, 1521, 2128, 7143, 2000, 21910, 999, 1001, 2167, 10010, 18861, 2050, 2022, 6047, 999, 1001, 3789, 16558, 23361, 8820, 3726, 14074, 14735, 2013, 1001, 8398, 999, 1011, 1012, 12609, 2602, 7206, 2031, 2363, 9962, 4402, 10428, 5227, 3596, 1999, 1996, 5653, 2007, 8398, 1005, 1055, 2227, 2006, 2068, 1011, 13229, 18155, 18291, 2015, 8299, 102])\n",
      "Row(id=UUID('5aadcd55-bcd6-466a-a9e1-0a19ede9edee'), label='0', tokens=[101, 1001, 19387, 1030, 5310, 1024, 2064, 2017, 2425, 1996, 4489, 2090, 1037, 2576, 4680, 2008, 2003, 2019, 11322, 3089, 14116, 2724, 6431, 2028, 2008, 2001, 10468, 2019, 3668, 24095, 2655, 1029, 1001, 29300, 2278, 11387, 11387, 102])\n",
      "Row(id=UUID('9b495d31-c983-4bea-b555-90b246f952a0'), label='1', tokens=[101, 1030, 5310, 1030, 5310, 2017, 2024, 2025, 7966, 2075, 3087, 2021, 4426, 1012, 2017, 2113, 4415, 2054, 1053, 6761, 2078, 2003, 1012, 2045, 1005, 1055, 7564, 1997, 15800, 2090, 2017, 1998, 2068, 1012, 2122, 4127, 1997, 11703, 20175, 3993, 9887, 2003, 2339, 1996, 3484, 1997, 4841, 2024, 3788, 2185, 2013, 10643, 1010, 8398, 1998, 1001, 23848, 2050, 2000, 3533, 7226, 2368, 2040, 2027, 2113, 2000, 3404, 1012, 102])\n",
      "Row(id=UUID('976a448d-6e27-4980-835a-e20b13f9deee'), label='1', tokens=[101, 1030, 5310, 1030, 5310, 1030, 5310, 1030, 5310, 2002, 2987, 1005, 1056, 2507, 1037, 4485, 2055, 1061, 1005, 2035, 1012, 1045, 2228, 1045, 1005, 1049, 2074, 6517, 2055, 2023, 6302, 1012, 1001, 3789, 16558, 23361, 8820, 3726, 14074, 14735, 1001, 3789, 16558, 23361, 8913, 26379, 24158, 15864, 24376, 1001, 7226, 2368, 29278, 28994, 5178, 3372, 11387, 11387, 1001, 15653, 24456, 2361, 102])\n",
      "Row(id=UUID('0db71249-506b-44bf-b885-0c866302189e'), label='0', tokens=[101, 1030, 5310, 2129, 2515, 2008, 2735, 2046, 1037, 3789, 2005, 8398, 1029, 2023, 2711, 2038, 1037, 7961, 15074, 1998, 2347, 1521, 1056, 2412, 2183, 2000, 3789, 2005, 7226, 2368, 1012, 102])\n",
      "Row(id=UUID('96bb4278-6b6a-4332-bcd8-c4d029b83f9e'), label='0', tokens=[101, 1045, 1005, 1049, 2025, 1037, 5470, 1997, 15941, 5472, 2121, 1005, 1055, 2021, 2296, 2051, 2002, 2758, 1000, 8529, 2319, 1000, 1010, 1045, 2064, 1005, 1056, 2393, 2021, 2868, 1012, 1001, 17183, 3207, 20179, 102])\n",
      "Row(id=UUID('3148cd38-bce4-4d3d-96bf-beb0e78f583b'), label='1', tokens=[101, 1000, 2004, 2019, 11265, 2050, 2586, 19012, 1010, 1045, 2097, 2954, 2000, 2191, 2469, 2008, 2009, 1521, 1055, 6529, 1010, 3008, 1010, 1998, 19156, 10561, 2043, 2009, 1521, 1055, 3647, 2000, 2175, 2067, 2000, 2082, 1517, 2025, 8801, 1012, 1524, 1011, 1030, 5310, 1012, 4067, 2017, 1010, 23787, 4747, 2005, 2115, 8426, 2000, 2493, 1998, 19156, 999, 1001, 19156, 29278, 5558, 2063, 1001, 17183, 8663, 15338, 3258, 102])\n",
      "Row(id=UUID('9ef1e60b-6269-42b0-9702-c9f1b0118364'), label='0', tokens=[101, 1030, 5310, 1030, 5310, 2205, 2919, 2017, 2481, 1521, 1056, 2156, 2083, 1996, 1001, 1040, 12273, 5365, 14652, 1998, 2156, 2613, 17571, 999, 102])\n",
      "Row(id=UUID('ff881be4-0270-4817-8aeb-f1997652c97f'), label='2', tokens=[101, 1030, 5310, 1030, 5310, 1030, 5310, 11425, 2190, 2001, 2725, 2014, 3105, 999, 999, 2242, 1523, 19156, 1524, 2066, 2017, 1010, 2323, 2022, 2725, 999, 999, 999, 2612, 1010, 2017, 6039, 2402, 9273, 2007, 6102, 2128, 29469, 2278, 1012, 1012, 1012, 1012, 2182, 1521, 1055, 2019, 2801, 1010, 6570, 8866, 1010, 2025, 3167, 8909, 8780, 21615, 1012, 1012, 1012, 1012, 6570, 2489, 3241, 1012, 1012, 1012, 2025, 11424, 6593, 11796, 3508, 999, 999, 999, 1001, 8398, 11387, 11387, 102])\n",
      "Row(id=UUID('045b393b-a676-4283-8bde-8b55a363b3c0'), label='1', tokens=[101, 2274, 1997, 1996, 5221, 6270, 9021, 2015, 1997, 1996, 2034, 2305, 1997, 1996, 3951, 4680, 1001, 29300, 2278, 11387, 11387, 1001, 29300, 21408, 2078, 15338, 3258, 1001, 29300, 6593, 14287, 7245, 1001, 8398, 26775, 14428, 7011, 4328, 2135, 1001, 2175, 15042, 20026, 13290, 2015, 1001, 7226, 2368, 8167, 6935, 11387, 11387, 1001, 7226, 2368, 2890, 14289, 16558, 5555, 3619, 8299, 102])\n",
      "Row(id=UUID('136c6317-a929-4ca1-97af-149f6263ee91'), label='1', tokens=[101, 1996, 2034, 2382, 8117, 2015, 1997, 1996, 1001, 3537, 25434, 8663, 15338, 3258, 2031, 2042, 100, 3685, 3524, 2005, 18520, 1010, 3870, 1010, 13857, 1010, 1998, 21911, 2050, 1012, 1001, 17183, 8663, 15338, 3258, 1001, 3533, 17062, 2368, 27052, 7911, 8167, 6935, 11387, 11387, 1001, 3789, 17062, 2368, 8167, 15061, 8820, 3726, 14074, 14735, 102])\n",
      "Row(id=UUID('921401a3-6a33-4220-8fb1-ef62b2f35878'), label='1', tokens=[101, 2204, 2851, 999, 4931, 3146, 999, 1008, 4228, 2420, 1008, 2127, 2220, 1999, 1011, 2711, 6830, 999, 4868, 2420, 2000, 13292, 1017, 1012, 5653, 1011, 1999, 17069, 2323, 2022, 7194, 2151, 2154, 1012, 11245, 2041, 2043, 2017, 4374, 6737, 3531, 1012, 2057, 1521, 2128, 2183, 2000, 2079, 2023, 999, 1001, 3533, 17062, 2368, 1001, 21911, 23278, 18752, 2015, 8299, 102])\n",
      "Row(id=UUID('1c20a93d-df31-4f26-8c6f-f8b92b8f3124'), label='2', tokens=[101, 1005, 21911, 2050, 5671, 2003, 1996, 2087, 4314, 2711, 1999, 3519, 1005, 1024, 8398, 19764, 2046, 7226, 2368, 1005, 1055, 21210, 4060, 1998, 2758, 2016, 1005, 1055, 2130, 2062, 4314, 2084, 15941, 12055, 1012, 1001, 8398, 1012, 1012, 1001, 2175, 2361, 1012, 1012, 1001, 3864, 102])\n",
      "Row(id=UUID('30f2bce2-f480-4a72-b975-cf18833ebccf'), label='2', tokens=[101, 1996, 2190, 2003, 2664, 2000, 2272, 1012, 1012, 1012, 1001, 8398, 11387, 11387, 1001, 8962, 2271, 1001, 8962, 2271, 19961, 1030, 5310, 1030, 5310, 1030, 5310, 8299, 102])\n",
      "Row(id=UUID('eadd1d3b-ee6c-4cdb-959d-48ba2bb9ae5f'), label='2', tokens=[101, 2122, 1001, 3533, 17062, 2368, 14997, 2024, 9951, 1012, 2644, 2667, 2000, 2191, 2032, 2041, 2066, 1037, 3002, 1012, 2182, 1005, 1055, 2035, 2017, 2428, 2342, 2000, 2154, 2055, 2032, 1024, 1000, 2002, 1005, 1055, 2025, 8398, 1012, 1000, 2008, 1005, 1055, 10468, 2129, 1045, 2514, 2055, 2068, 1012, 7226, 2368, 19237, 1012, 8398, 19237, 4788, 1012, 2203, 1997, 2466, 1012, 102])\n",
      "Row(id=UUID('7e04f659-671c-4135-86ed-0ed9c9b7b6c4'), label='2', tokens=[101, 1000, 1045, 3825, 8817, 1997, 6363, 1999, 7773, 1010, 1000, 2343, 8398, 2758, 1012, 1001, 4883, 3207, 20179, 11387, 11387, 1001, 14379, 11387, 11387, 1001, 3915, 1001, 23848, 2050, 11387, 11387, 1001, 10556, 2290, 1001, 2602, 11387, 11387, 1001, 5981, 15864, 1001, 4610, 102])\n",
      "Row(id=UUID('f1d9db71-f0ab-4686-98a3-2cb51a41ad6b'), label='1', tokens=[101, 1030, 5310, 7050, 100, 1030, 5310, 1030, 5310, 2435, 2033, 10720, 2015, 999, 100, 1001, 8294, 100, 1001, 2373, 15794, 5369, 5051, 27469, 1001, 17183, 8663, 15338, 3258, 1001, 8037, 1001, 17772, 1001, 7226, 2368, 8167, 6935, 11387, 11387, 13122, 10696, 14074, 14735, 1001, 7226, 2368, 8167, 6935, 8653, 24198, 11387, 11387, 102])\n",
      "Row(id=UUID('98f54334-1891-4470-bf4b-ccb0ecf70bd3'), label='2', tokens=[101, 1030, 5310, 1045, 2031, 6719, 2196, 2081, 1037, 7615, 2006, 2023, 2591, 2865, 4132, 1999, 2026, 2166, 1998, 2097, 2763, 2022, 25030, 2044, 2023, 2028, 1012, 7226, 2368, 2003, 2025, 17824, 2130, 2043, 3752, 1037, 3898, 1012, 1998, 2017, 1521, 2128, 24234, 2075, 8398, 3495, 1998, 17351, 2007, 2115, 2780, 3980, 1012, 9217, 7226, 2368, 1521, 1055, 11896, 1012, 102])\n",
      "Row(id=UUID('c9092822-f41f-4d01-b99d-6919616bbe18'), label='0', tokens=[101, 2065, 2017, 2123, 1005, 1056, 3789, 2005, 1046, 1012, 7226, 2368, 1010, 2111, 6069, 7868, 2008, 2017, 2097, 3789, 2005, 8398, 2004, 2065, 2045, 2003, 2053, 2060, 3822, 5724, 8840, 2140, 102])\n",
      "Row(id=UUID('3a550b33-4016-4a2a-a867-df861866b2f6'), label='0', tokens=[101, 1030, 5310, 1030, 5310, 8398, 2064, 1521, 1056, 2360, 6504, 1010, 2010, 6043, 2024, 2025, 2759, 102])\n",
      "Row(id=UUID('f822df9f-aa3c-4040-940b-5db2ec5f7efa'), label='2', tokens=[101, 1030, 5310, 2339, 2123, 1005, 1056, 2017, 13520, 2115, 3714, 12163, 2291, 1037, 2270, 2740, 5325, 1029, 1029, 1029, 1019, 5477, 2706, 1045, 1005, 2310, 2042, 3403, 2005, 2026, 2769, 999, 1045, 6655, 2017, 2030, 2151, 2372, 1997, 2115, 3095, 1004, 23713, 1025, 2155, 2038, 2025, 4771, 1037, 2309, 3477, 5403, 3600, 2076, 2023, 6090, 3207, 7712, 999, 999, 999, 1001, 6366, 2860, 16584, 5017, 1001, 1059, 16584, 16862, 12722, 2015, 1001, 2136, 24456, 2361, 1001, 1059, 27767, 2487, 27767, 2050, 102])\n",
      "Row(id=UUID('96554a99-3208-4a4e-82bc-d72a00332410'), label='1', tokens=[101, 1030, 5310, 2053, 2909, 2057, 2024, 5204, 1997, 2115, 12225, 1998, 2017, 2097, 2022, 5444, 2041, 1999, 2281, 20228, 2015, 12897, 2115, 27118, 7542, 2003, 22072, 2290, 4480, 2012, 3891, 1012, 2057, 1996, 2111, 2024, 3201, 2000, 3789, 2017, 2041, 1999, 1037, 20148, 1012, 1001, 3789, 16558, 23361, 8820, 3726, 14074, 14735, 102])\n",
      "Row(id=UUID('ad49d8af-52a7-4036-aad4-59a1c18597dc'), label='1', tokens=[101, 2017, 2134, 2102, 2079, 2115, 3105, 1012, 2017, 3613, 2000, 2025, 2079, 2115, 3105, 1012, 2035, 6677, 2024, 2006, 2017, 2017, 10041, 1010, 16592, 100, 1012, 2637, 3791, 5604, 2005, 2035, 1012, 2085, 999, 1001, 8398, 11983, 14074, 14735, 3619, 10265, 1001, 5604, 22199, 2075, 22199, 2075, 1996, 2069, 2126, 2000, 5383, 1004, 23713, 1025, 2491, 999, 1001, 3789, 16558, 23361, 8820, 3726, 14074, 14735, 102])\n",
      "Row(id=UUID('8f67fa00-8036-42ba-972a-8f3d893cdfc1'), label='1', tokens=[101, 6396, 2102, 8368, 2604, 25967, 28514, 2005, 10851, 19220, 2007, 1520, 2576, 25869, 9648, 1521, 2004, 2002, 3632, 2006, 10885, 1011, 8299, 102])\n",
      "Row(id=UUID('b349ff26-6178-41e7-96b2-b7130ad67ff6'), label='0', tokens=[101, 3793, 19988, 14142, 2000, 2131, 2592, 1997, 2129, 2000, 3789, 999, 1001, 1040, 12273, 11387, 11387, 102])\n",
      "Row(id=UUID('d3bfaf8e-7d20-47b3-a2ba-05fbb88fbf1c'), label='0', tokens=[101, 2184, 2781, 2000, 2175, 1010, 3246, 2017, 2031, 2288, 2115, 24593, 3201, 1001, 8398, 1001, 7226, 2368, 102])\n",
      "Row(id=UUID('d360e50f-bd94-4706-ac38-d09ac6b6eb1e'), label='0', tokens=[101, 2016, 2134, 1005, 1056, 9772, 2009, 1001, 17183, 3207, 20179, 102])\n",
      "Row(id=UUID('30fc9ed6-8b40-444a-b0bd-f3efd78095e3'), label='0', tokens=[101, 15941, 2038, 2589, 1996, 2157, 2518, 1010, 2295, 2002, 2018, 2053, 2060, 5724, 1012, 2000, 3613, 1037, 2048, 1011, 16660, 5049, 1010, 2002, 1998, 7226, 2368, 2052, 2031, 2000, 7697, 2169, 2060, 4237, 1012, 2069, 8398, 2052, 5770, 1010, 2926, 2065, 2002, 2921, 2010, 5003, 2860, 3844, 1998, 2106, 2498, 999, 1001, 12609, 12260, 7542, 1001, 102])\n",
      "Row(id=UUID('d3362b4a-7b52-46a3-aade-fb4c1a23220e'), label='0', tokens=[101, 1030, 5310, 2017, 2614, 3294, 9951, 2157, 2085, 1012, 2175, 2000, 2793, 999, 1001, 17183, 8663, 15338, 3258, 102])\n",
      "Row(id=UUID('0b44ab46-58b0-4266-a556-037f930a6c14'), label='0', tokens=[101, 2057, 2024, 1037, 2929, 9359, 1996, 2773, 1997, 1996, 2373, 1997, 1996, 2931, 3789, 2058, 1996, 1996, 2197, 2531, 2086, 1030, 5310, 1030, 5310, 1030, 5310, 1001, 3789, 1001, 4236, 26525, 12184, 11387, 11387, 1001, 12609, 12260, 7542, 8299, 102])\n",
      "Row(id=UUID('796fc26b-4b46-4579-9fd3-8247565bff20'), label='1', tokens=[101, 1045, 2123, 1521, 1056, 2131, 2009, 1012, 2129, 2003, 2023, 2130, 1037, 5049, 1029, 1001, 3537, 8663, 15338, 3258, 1001, 17183, 8663, 15338, 3258, 1001, 2637, 14643, 15312, 4590, 24415, 17062, 2368, 1030, 5310, 102])\n",
      "Row(id=UUID('e4e3b5ed-9d8d-414a-bce5-b5c9b45abb9a'), label='0', tokens=[101, 1030, 5310, 1030, 5310, 1001, 8275, 2638, 9333, 1001, 1053, 6761, 2078, 9714, 3399, 102])\n",
      "Row(id=UUID('0b2cdd2c-0abd-43f4-abc8-41c7d1ec14d1'), label='0', tokens=[101, 3220, 18210, 1041, 24411, 2232, 2203, 5668, 2229, 3537, 4883, 9773, 3533, 7226, 2368, 3805, 1997, 2014, 2836, 2012, 1996, 12609, 3537, 2120, 4680, 1012, 8398, 2003, 1005, 9846, 2256, 2406, 1005, 1010, 2016, 2056, 1012, 2005, 2444, 14409, 1024, 8299, 1001, 1040, 12273, 11387, 11387, 1001, 17183, 8663, 15338, 3258, 1001, 2224, 2571, 22014, 1001, 6728, 6977, 2072, 102])\n",
      "Row(id=UUID('73a3e110-5427-44c0-8677-d76178517626'), label='1', tokens=[101, 1030, 5310, 7226, 2368, 2074, 4760, 2039, 1998, 5131, 5222, 999, 1996, 17626, 2003, 2275, 17111, 9541, 2659, 2008, 2002, 2097, 2663, 2074, 2011, 4760, 2039, 1012, 7539, 1997, 2054, 6433, 1012, 102])\n",
      "Row(id=UUID('716fd9f1-c068-444c-9dc4-f064f8826dfd'), label='2', tokens=[101, 1030, 5310, 1030, 5310, 6160, 2000, 1001, 3533, 17062, 2368, 1521, 1055, 2544, 1997, 2637, 1011, 2746, 2000, 1037, 5101, 2379, 2017, 1012, 1001, 12609, 12155, 12096, 29100, 1001, 12609, 12260, 7542, 8299, 102])\n",
      "Row(id=UUID('a2d03070-ede9-4f78-8bfb-01126352b4a1'), label='1', tokens=[101, 1030, 5310, 2065, 1045, 2020, 1996, 7226, 2368, 2136, 1010, 1045, 1005, 1040, 2224, 3892, 2000, 2404, 19817, 1008, 6131, 2006, 4446, 2005, 1996, 6677, 1997, 3263, 1010, 2199, 4841, 1012, 2191, 2032, 5490, 10179, 10867, 1012, 102])\n",
      "Row(id=UUID('07d3d966-52b5-437a-aa14-ae4495928c54'), label='0', tokens=[101, 1045, 2572, 2667, 2000, 5674, 2040, 2052, 2298, 2062, 21934, 4842, 2075, 1998, 5003, 18696, 1999, 1037, 8398, 1013, 22950, 5981, 1012, 1001, 17183, 3207, 20179, 102])\n",
      "Row(id=UUID('fbab8c02-27e6-4762-a7c4-d923b1e5578f'), label='1', tokens=[101, 2023, 2694, 2208, 1011, 2265, 3677, 7551, 2001, 1037, 4945, 1001, 3533, 17062, 2368, 1001, 14379, 11387, 11387, 102])\n",
      "Row(id=UUID('314ea4d1-06e7-444d-b1d5-48ef0db28ac7'), label='0', tokens=[101, 3173, 2026, 3052, 7188, 7226, 2368, 8847, 102])\n",
      "Row(id=UUID('a0a5f34d-6b66-4f4a-93d5-28e0e4f0a675'), label='1', tokens=[101, 1030, 5310, 2489, 1996, 7980, 2094, 2336, 2040, 2020, 14177, 2013, 2308, 2040, 2024, 2017, 2139, 16467, 2023, 2154, 2205, 999, 2017, 2097, 2022, 5091, 2124, 2004, 1996, 2283, 2008, 10312, 2336, 2185, 2013, 2037, 10756, 999, 1001, 3789, 16558, 23361, 8820, 3726, 14074, 14735, 102])\n",
      "Row(id=UUID('68cb0bab-a179-4204-a6ba-14fd9212c961'), label='2', tokens=[101, 4593, 2017, 2134, 1521, 1056, 4952, 2000, 2010, 4613, 1012, 2002, 2106, 2025, 2091, 13068, 2505, 1012, 2009, 2001, 3819, 999, 1045, 2052, 2031, 3866, 2000, 2156, 7226, 2368, 3713, 2444, 2005, 2074, 2321, 2781, 999, 1001, 5263, 1030, 5310, 1001, 8398, 11387, 11387, 100, 8299, 102])\n",
      "Row(id=UUID('a638dadf-122b-4e46-be42-5352ef168556'), label='0', tokens=[101, 1000, 3537, 2120, 4680, 1024, 4174, 18079, 1012, 21625, 1059, 16584, 5017, 18058, 2014, 4769, 1000, 1001, 17183, 8663, 15338, 3258, 8299, 102])\n",
      "Row(id=UUID('45ac51c7-2578-46fe-aa8e-8d93d1c563e2'), label='0', tokens=[101, 1000, 2017, 2215, 2033, 2000, 2171, 2068, 1029, 1000, 1000, 2748, 1012, 1000, 15941, 2003, 2383, 3904, 1997, 2115, 4485, 1012, 1001, 17183, 3207, 20179, 102])\n",
      "Row(id=UUID('9d9beb8b-94bb-4586-b9e9-c9476497c5ee'), label='1', tokens=[101, 2009, 1005, 1055, 2042, 1037, 3565, 10720, 2095, 1012, 1001, 3789, 16558, 23361, 8820, 3726, 14074, 14735, 1001, 7226, 2368, 11387, 11387, 8299, 102])\n",
      "Row(id=UUID('c123e2cd-d499-443d-9ba1-50e4c8934c5c'), label='1', tokens=[101, 1001, 21911, 23278, 18752, 2015, 8847, 2000, 1996, 2200, 2613, 7860, 5307, 4841, 1010, 1999, 1996, 2568, 1997, 23848, 2050, 1004, 23713, 1025, 1001, 29300, 2278, 11387, 11387, 7492, 1517, 2008, 2965, 1001, 7226, 2368, 8167, 6935, 11387, 11387, 1523, 16424, 3915, 1524, 1004, 23713, 1025, 2024, 1523, 2061, 4997, 1012, 1524, 2040, 2428, 16424, 2637, 1013, 24978, 1029, 1001, 8398, 11387, 11387, 2097, 2196, 2079, 1037, 4365, 2518, 2021, 1059, 14014, 1004, 23713, 1025, 7499, 1012, 1001, 8398, 15872, 21559, 22420, 6962, 102])\n",
      "Row(id=UUID('7dbf4d42-28f5-4ba0-8905-875f774310d3'), label='0', tokens=[101, 1030, 5310, 1001, 2175, 2361, 1996, 2283, 1997, 2235, 2231, 1010, 2025, 999, 1030, 5310, 1001, 19220, 23947, 4270, 1001, 19220, 102])\n",
      "Row(id=UUID('d8214a5e-77f7-4ee1-b314-62fe412ab7a3'), label='0', tokens=[101, 1030, 5310, 1030, 5310, 1030, 5310, 1996, 2606, 1004, 23713, 1025, 5789, 2003, 27503, 1001, 3537, 25434, 8663, 15338, 3258, 1001, 3537, 8663, 15338, 3258, 8299, 102])\n",
      "Row(id=UUID('27af3f72-ff50-4dda-a322-8c46b874201a'), label='0', tokens=[101, 7791, 2000, 2119, 2122, 4364, 2005, 1016, 2847, 3892, 1012, 2172, 1997, 2054, 1045, 2657, 2003, 2062, 2502, 2231, 2008, 10659, 2489, 1013, 2489, 1013, 2489, 1010, 2330, 6645, 1010, 4288, 1996, 3514, 3068, 1998, 2062, 1012, 4283, 1010, 2021, 2057, 2253, 2083, 1022, 2086, 1997, 2008, 2007, 8112, 1998, 2116, 1997, 2149, 2079, 2025, 2215, 2000, 2175, 2067, 1012, 1012, 1012, 1012, 1001, 17183, 3207, 20179, 102])\n",
      "Row(id=UUID('aea54374-65c5-4ef6-be13-28add5178d53'), label='2', tokens=[101, 1030, 5310, 1030, 5310, 4480, 1997, 7672, 2448, 3655, 2008, 2024, 2108, 3908, 2342, 2000, 9790, 2037, 2334, 2231, 2005, 4352, 2023, 2375, 24913, 1010, 1998, 7694, 2009, 2004, 1001, 9379, 21572, 22199, 1012, 2202, 2037, 2769, 1010, 1001, 12475, 24138, 4213, 8737, 1998, 2486, 2334, 2231, 2000, 3499, 2976, 2231, 8830, 2000, 9239, 2344, 1012, 102])\n",
      "Row(id=UUID('b42e0b5b-a3c0-4970-9caf-da3cd317f839'), label='0', tokens=[101, 2663, 2054, 1029, 1996, 2971, 2005, 5409, 1998, 2087, 9596, 2398, 20459, 2063, 2412, 1029, 1030, 5310, 1030, 5310, 1001, 7226, 2368, 8167, 6935, 11387, 11387, 1001, 7226, 2368, 11387, 11387, 1001, 21911, 23278, 18752, 2015, 1001, 12609, 12260, 7542, 1001, 2398, 20459, 2063, 100, 8299, 102])\n",
      "Row(id=UUID('1aa975da-85b1-456e-a33a-aafcfc47333a'), label='2', tokens=[101, 1030, 5310, 2057, 2113, 2488, 3533, 1012, 1001, 8398, 11387, 11387, 1030, 5310, 1030, 5310, 100, 8299, 102])\n",
      "Row(id=UUID('775a160e-f2cc-4f5c-aabe-3d638ad34d85'), label='0', tokens=[101, 1030, 5310, 2748, 1010, 2057, 2097, 5454, 1037, 2488, 2126, 1010, 2720, 1012, 1004, 23713, 1025, 3680, 1012, 5747, 999, 999, 100, 1001, 3789, 16558, 23361, 8820, 3726, 14074, 14735, 102])\n",
      "Row(id=UUID('ed1068f6-b24f-441f-8fe0-013d85adb83f'), label='1', tokens=[101, 1030, 5310, 1030, 5310, 1030, 5310, 1030, 5310, 2138, 1045, 2219, 5144, 1999, 3146, 1012, 1045, 5444, 1999, 1996, 2233, 3078, 1999, 3146, 1998, 2097, 2175, 2067, 2000, 3789, 1999, 2281, 1999, 3146, 999, 7359, 2003, 2574, 2000, 2022, 2026, 2047, 2188, 1010, 2021, 2127, 2059, 1045, 2097, 1001, 3789, 16558, 23361, 8820, 3726, 14074, 14735, 1998, 3146, 1012, 1001, 19067, 23115, 2063, 8299, 102])\n",
      "Row(id=UUID('b1e0b303-fc70-4164-875b-a372c2485d72'), label='1', tokens=[101, 15699, 2232, 1996, 3178, 1010, 15699, 2232, 1996, 2158, 1004, 23713, 1025, 2450, 1001, 7226, 2368, 8167, 6935, 11387, 11387, 100, 1001, 7226, 2368, 29278, 14074, 14735, 100, 1001, 15653, 24456, 2361, 100, 1001, 15653, 26775, 14659, 2098, 24456, 2361, 100, 8299, 102])\n",
      "Row(id=UUID('537ba4f2-9bad-45e4-b4ee-bff2fb71c2d0'), label='0', tokens=[101, 9393, 3331, 2055, 1523, 2183, 2152, 1524, 2003, 2074, 4895, 22852, 999, 2054, 2112, 1997, 6012, 2111, 2039, 1999, 1996, 2395, 1004, 23713, 1025, 3046, 2000, 4853, 1037, 25073, 2700, 2343, 2003, 1523, 2183, 2152, 1524, 1029, 1006, 2453, 10587, 13764, 2067, 1996, 8275, 7603, 2205, 1012, 2009, 1521, 1055, 1037, 2210, 2172, 1007, 1001, 17183, 8663, 15338, 3258, 102])\n",
      "Row(id=UUID('76c046c3-98c2-4cb6-b0f5-a061ede57db2'), label='1', tokens=[101, 1001, 3533, 17062, 2368, 27052, 7911, 8167, 6935, 11387, 11387, 1001, 3533, 17062, 2368, 1001, 7672, 26243, 3111, 13337, 2265, 2039, 2012, 2474, 2902, 12318, 22168, 10558, 1010, 14315, 1005, 1045, 3246, 2027, 1042, 1011, 1011, 1011, 1011, 1011, 1011, 3280, 1005, 8299, 102])\n",
      "Row(id=UUID('afa91575-bd30-4787-bca7-8cb00e86214b'), label='0', tokens=[101, 1037, 3232, 7760, 2013, 7483, 1005, 1055, 4013, 1011, 2166, 13965, 2007, 1030, 5310, 1012, 2057, 2024, 7098, 2000, 4256, 2007, 8037, 2005, 2166, 1999, 2344, 2000, 14323, 11324, 4654, 7913, 26725, 2306, 1996, 1040, 12273, 999, 1001, 14008, 14406, 6279, 1001, 17183, 8663, 15338, 3258, 8299, 102])\n",
      "Row(id=UUID('bdd6c18c-ac61-4cda-b860-b1029c375195'), label='2', tokens=[101, 1030, 5310, 2057, 5632, 2296, 3371, 1997, 2009, 999, 999, 999, 1001, 23848, 2050, 1001, 8398, 11387, 11387, 102])\n",
      "Row(id=UUID('5ffd8c97-7afc-4e49-803c-0089e12a35f8'), label='0', tokens=[101, 1001, 2162, 2239, 12155, 3549, 4247, 1001, 28616, 15707, 4890, 3040, 1001, 6335, 4819, 9626, 2099, 12308, 2126, 1001, 7672, 1018, 3423, 1001, 1042, 21693, 1001, 7354, 5856, 3207, 2239, 13945, 6580, 1997, 1009, 3156, 2243, 2210, 3057, 1999, 3915, 1012, 1001, 10958, 20952, 6633, 1030, 5310, 19424, 1012, 1001, 2304, 3669, 6961, 18900, 3334, 1001, 2775, 7875, 8557, 1030, 5310, 1030, 5310, 1030, 5310, 1030, 5310, 8299, 102])\n",
      "Row(id=UUID('02fbc828-b92e-44c1-8a6e-7946d9ebba99'), label='1', tokens=[101, 2104, 1030, 5310, 1520, 1055, 3768, 1997, 4105, 1004, 23713, 1025, 2010, 2342, 2000, 11443, 2149, 2612, 1997, 15908, 2149, 1010, 2097, 2991, 1013, 8246, 1012, 1037, 2160, 4055, 3685, 3233, 1012, 1037, 2160, 2142, 2097, 2004, 2009, 2038, 2307, 2490, 1004, 23713, 1025, 1037, 2204, 3192, 1012, 1001, 3789, 16558, 23361, 8820, 3726, 14074, 14735, 8299, 102])\n",
      "Row(id=UUID('a06d2396-e53b-4ae2-a6a4-93043eeb2720'), label='1', tokens=[101, 2064, 2017, 3305, 1030, 5310, 2043, 2002, 8847, 1029, 1001, 3533, 17062, 2368, 1001, 7226, 2368, 11387, 11387, 1001, 2184, 29100, 6499, 14876, 7228, 7442, 7542, 102])\n",
      "Row(id=UUID('f9980818-0a44-4606-90f9-d5e80e65f39e'), label='2', tokens=[101, 1045, 1005, 1040, 5454, 26261, 10532, 2058, 7226, 2368, 2151, 2154, 1012, 1001, 3537, 3207, 20179, 1001, 17183, 3207, 20179, 1001, 17183, 3207, 20179, 11387, 11387, 102])\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fetch all table names in the keyspace\n",
    "rows = session.execute(\"SELECT table_name FROM system_schema.tables WHERE keyspace_name = %s\", [\"keyspace\"])\n",
    "table_names = [row[0] for row in rows]\n",
    "\n",
    "for table_name in table_names:\n",
    "    print(f\"Contents of table {table_name}:\")\n",
    "    rows = session.execute(f\"SELECT * FROM {table_name}\")\n",
    "    for row in rows:\n",
    "        print(row)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SELECT * FROM \"keyspace\".features LIMIT 10;\n",
    "#SELECT COUNT(*) FROM \"keyspace\".features;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to the Cassandra Database...\n",
      "\u001b[33mCluster.__init__ called with contact_points specified, but no load_balancing_policy. In the next major version, this will raise an error; please specify a load-balancing policy. (contact_points = ['127.0.0.1'], lbp = None)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDowngrading core protocol version from 66 to 65 for 127.0.0.1:9042. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version\u001b[0m\n",
      "\u001b[33mDowngrading core protocol version from 65 to 5 for 127.0.0.1:9042. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version\u001b[0m\n",
      "\u001b[1;35mUsing datacenter 'datacenter1' for DCAwareRoundRobinPolicy (via host '127.0.0.1:9042'); if incorrect, please specify a local_dc to the constructor, or limit contact points to local cluster nodes\u001b[0m\n",
      "Data loading completed successfully.\n"
     ]
    }
   ],
   "source": [
    "from cassandra.cluster import Cluster\n",
    "import pandas as pd\n",
    "\n",
    "print(\"Connecting to the Cassandra Database...\")\n",
    "cluster = Cluster(contact_points=['127.0.0.1'], port=9042)\n",
    "session = cluster.connect()\n",
    "session.set_keyspace('keyspace')\n",
    "\n",
    "# Query to retrieve data from Cassandra\n",
    "query = \"SELECT id, tokens, label FROM features;\"\n",
    "rows = session.execute(query)\n",
    "\n",
    "# Convert the data to a Pandas DataFrame\n",
    "data = pd.DataFrame(list(rows))\n",
    "\n",
    "# Assuming 'tokens' are your features and 'label' is the target variable\n",
    "# Format data for training\n",
    "X = data['tokens'].tolist()  # Feature set\n",
    "y = data['label'].tolist()   # Labels\n",
    "\n",
    "# At this point, X and y can be used in a machine learning model training process.\n",
    "# Example: Using these in a PyTorch or TensorFlow training pipeline.\n",
    "\n",
    "print(\"Data loading completed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[101, 1030, 5310, 1030, 5310, 1030, 5310, 1030, 5310, 1030, 5310, 2017, 2024, 2019, 7780, 2000, 2035, 1010, 2926, 2308, 2040, 2954, 10126, 2007, 2061, 2116, 2367, 15314, 999, 4067, 2017, 100, 1001, 17183, 8663, 15338, 3258, 1001, 7226, 2368, 8167, 6935, 11387, 11387, 102], [101, 1030, 5310, 3533, 7226, 2368, 1521, 1055, 4700, 2086, 5443, 8398, 2015, 4700, 2706, 1012, 1998, 8398, 2347, 1521, 1056, 1037, 3761, 1010, 2085, 2008, 1521, 1055, 16436, 2005, 2017, 3533, 1012, 2009, 2003, 2054, 2009, 2003, 2138, 3533, 7226, 2368, 2003, 2040, 2002, 2003, 1012, 102], [101, 1030, 5310, 2053, 1010, 2027, 2024, 2145, 2045, 1012, 2030, 2062, 2031, 2042, 6866, 1012, 9733, 2038, 3314, 2007, 2023, 2828, 1997, 4031, 2035, 1996, 2051, 1012, 3531, 2562, 7316, 2068, 1012, 1001, 3789, 16558, 5657, 102], [101, 1030, 5310, 1030, 5310, 1048, 2863, 2080, 1012, 1012, 1012, 2066, 2017, 2729, 2055, 2637, 1012, 1001, 8398, 11387, 11387, 1998, 2009, 2097, 2022, 1037, 4121, 2663, 999, 102], [101, 1001, 3407, 18447, 11795, 3370, 27318, 5620, 10259, 1001, 3407, 25434, 16168, 10259, 1001, 3407, 18447, 11795, 3370, 27318, 2290, 10259, 1001, 28844, 25688, 4710, 1998, 1001, 2176, 5974, 29100, 2015, 2005, 1001, 6221, 24456, 2361, 100, 8299, 102], [101, 1030, 5310, 1030, 5310, 1030, 5310, 13366, 2571, 6593, 2172, 1029, 1996, 12043, 4366, 2002, 1042, 1011, 13749, 4282, 7226, 2368, 2987, 1521, 1056, 2031, 2009, 1998, 8040, 11614, 2015, 2500, 1012, 8299, 102], [101, 7226, 2368, 2003, 3262, 18524, 2005, 1996, 2236, 2602, 1010, 2025, 20767, 15941, 1012, 1001, 3537, 3207, 20179, 102], [101, 1045, 6655, 2065, 1001, 3013, 3111, 2001, 5868, 2011, 11992, 7249, 1001, 4314, 2865, 2052, 2131, 2037, 6510, 19896, 1998, 24711, 3201, 4214, 2041, 1001, 21877, 3527, 21850, 6632, 2021, 1997, 2607, 2025, 2138, 1996, 2088, 2003, 2743, 2011, 1001, 5305, 11263, 10603, 2040, 2228, 1996, 23760, 3366, 2595, 8787, 3989, 1997, 2336, 2003, 1001, 9191, 1012, 6616, 2017, 12731, 10603, 102], [101, 1001, 10047, 22994, 2075, 29278, 5558, 2063, 2138, 1045, 1005, 1049, 5458, 1997, 3773, 2023, 22078, 7377, 5443, 1012, 9018, 10231, 2012, 1001, 1015, 1001, 22078, 7377, 1001, 3533, 17062, 2368, 1001, 8398, 10199, 2102, 7698, 102], [101, 1030, 5310, 6554, 13970, 27002, 2003, 2498, 2021, 1037, 8398, 25353, 3597, 21890, 3372, 1012, 1001, 29300, 2278, 11387, 11387, 1001, 29300, 21408, 2078, 15338, 3258, 11387, 11387, 102], [101, 1030, 5310, 23156, 1012, 2085, 2425, 2149, 2153, 2129, 8398, 4342, 2010, 10800, 2013, 5924, 2061, 1057, 2134, 1005, 1056, 2342, 2000, 17727, 5243, 2818, 2032, 1010, 2030, 2129, 10556, 27313, 8953, 2097, 4047, 1037, 2450, 1005, 1055, 2157, 2000, 5454, 1012, 1001, 3789, 16558, 5657, 102], [101, 1030, 5310, 1030, 5310, 8112, 2015, 27157, 1012, 8112, 1521, 1055, 2637, 1012, 2908, 999, 4283, 2000, 8398, 999, 1001, 23848, 2050, 102], [101, 2129, 2097, 7226, 2368, 2079, 20767, 8398, 1029, 2057, 2356, 2216, 2040, 2031, 4320, 1996, 2280, 21210, 8299, 2035, 7226, 2368, 3791, 2000, 3342, 2003, 2008, 8398, 2196, 12237, 2006, 2391, 1998, 2002, 2097, 13366, 2571, 6593, 1010, 9772, 1010, 1998, 3046, 2000, 2689, 1996, 7984, 1997, 2151, 3395, 3160, 1012, 6293, 2000, 5896, 1012, 102], [101, 3752, 1056, 28394, 3215, 2013, 16360, 12083, 2015, 1012, 2055, 2129, 11771, 1996, 17183, 1012, 4680, 2001, 1012, 1045, 2387, 26452, 1998, 3246, 1012, 1045, 3866, 2009, 1012, 2025, 4394, 1996, 5223, 1010, 14398, 1010, 28616, 15707, 4890, 1998, 2171, 4214, 1012, 1045, 1005, 2222, 2202, 23997, 1010, 3606, 1998, 4847, 2151, 2154, 2058, 1996, 8398, 9661, 1012, 1001, 7226, 2368, 11387, 11387, 102], [101, 1030, 5310, 2038, 2085, 15506, 2058, 1996, 4652, 3085, 6677, 1997, 2471, 5018, 2243, 4841, 1012, 2010, 3768, 1997, 1037, 2120, 2933, 1998, 1996, 2051, 2002, 13842, 2038, 4504, 1999, 2023, 8488, 1012, 2002, 2442, 2175, 1012, 1001, 7226, 2368, 11387, 11387, 102], [101, 7226, 2368, 2108, 7861, 21890, 4588, 2008, 2002, 2097, 4060, 1037, 2450, 21210, 1998, 15941, 11333, 4246, 2989, 3084, 2033, 2228, 7226, 2368, 2038, 2525, 3856, 1037, 21210, 1006, 5671, 1029, 1007, 1998, 15941, 2038, 2025, 1012, 1001, 17183, 3207, 20179, 102], [101, 6203, 1030, 5310, 1011, 1996, 2062, 1057, 1004, 23713, 1025, 1001, 4314, 1001, 8037, 9772, 2008, 1001, 3424, 7011, 6526, 1004, 23713, 1025, 2024, 9846, 2256, 3655, 1010, 1996, 2062, 2111, 2097, 3789, 2005, 1001, 8398, 999, 2425, 2122, 2967, 2027, 2123, 1005, 1056, 4839, 1024, 1030, 5310, 1030, 5310, 1030, 5310, 1030, 5310, 1030, 5310, 1030, 5310, 1001, 4654, 3995, 2361, 102], [101, 1045, 1521, 2310, 2196, 2042, 1037, 2502, 1030, 5310, 5470, 2021, 1045, 2245, 2002, 2106, 1037, 3492, 2204, 3105, 1997, 2108, 1037, 26438, 1998, 11519, 5981, 2099, 1998, 2529, 3892, 4102, 2000, 1030, 5310, 1012, 1001, 5981, 11387, 11387, 102], [101, 1030, 5310, 4983, 2045, 2003, 1037, 11476, 2135, 20080, 2765, 1037, 1011, 2474, 2456, 1010, 1045, 5987, 2477, 2000, 2175, 4659, 15299, 1010, 2748, 1012, 2070, 20543, 1998, 2070, 2942, 16591, 3383, 1010, 2021, 1045, 2123, 1005, 1056, 5987, 1996, 2406, 2000, 2991, 4237, 1059, 1037, 7226, 2368, 3377, 1012, 2065, 8398, 5222, 1010, 1045, 1005, 1049, 2625, 9657, 1012, 102], [101, 1030, 5310, 2009, 5807, 1005, 1056, 2031, 2000, 999, 2065, 1001, 3533, 17062, 2368, 2038, 2000, 2644, 2551, 1998, 4769, 2296, 5223, 2099, 2008, 3980, 2010, 5177, 4513, 2030, 2010, 6830, 2501, 2030, 2010, 4424, 6101, 9989, 2030, 2010, 2365, 1005, 1055, 18913, 2030, 2010, 2248, 2449, 24069, 2002, 1005, 1055, 2196, 2183, 2000, 2131, 2505, 2589, 999, 1001, 3789, 16558, 5657, 102], [101, 1030, 5310, 2997, 12077, 2865, 2202, 7698, 1997, 4018, 3533, 7226, 2368, 2005, 1523, 11094, 22134, 7898, 1524, 1012, 7226, 2368, 2018, 2000, 4530, 2041, 1997, 1996, 2997, 3049, 4647, 1997, 2010, 11888, 13720, 2000, 2425, 1996, 3606, 1012, 4438, 3533, 1012, 2073, 1521, 1055, 1996, 2865, 2651, 1029, 2292, 1521, 1055, 8970, 1037, 2047, 4245, 2000, 1996, 2168, 2214, 3533, 1012, 100, 8299, 102], [101, 2028, 3951, 2040, 2499, 2006, 1996, 2355, 3078, 3049, 1024, 1523, 2065, 2017, 2020, 2005, 7226, 2368, 1010, 2017, 1521, 2128, 2145, 2005, 7226, 2368, 1012, 2065, 2017, 2020, 2005, 8398, 1010, 2017, 1521, 2128, 2145, 2005, 8398, 1012, 2065, 2017, 2234, 2046, 2023, 2025, 4209, 2040, 2000, 2490, 1010, 1045, 2123, 1521, 1056, 2113, 2054, 2003, 2183, 2000, 8054, 2017, 1012, 1524, 8299, 102], [101, 2054, 1037, 7098, 2617, 999, 999, 2381, 1999, 1996, 2437, 999, 2054, 1037, 3928, 4613, 999, 999, 1001, 3580, 28994, 5178, 3372, 8167, 6935, 1001, 21911, 23278, 18752, 2015, 1001, 2308, 23011, 1001, 3537, 8663, 15338, 3258, 1001, 2057, 3995, 12399, 5963, 1001, 21911, 23278, 18752, 2015, 11387, 11387, 1001, 1040, 12273, 1529, 8299, 102], [101, 3533, 1001, 7226, 2368, 2921, 3038, 2000, 1996, 2137, 2111, 2008, 2002, 2003, 3201, 2000, 1001, 5981, 2343, 1001, 8398, 1998, 3970, 1017, 1001, 2093, 14379, 2515, 3533, 1001, 7226, 2368, 9530, 18532, 15725, 2010, 6808, 2000, 1001, 10214, 1998, 1001, 4468, 2122, 14379, 2006, 1037, 1001, 5656, 2478, 1996, 1001, 21887, 23350, 2005, 2025, 5782, 2000, 3604, 1001, 17547, 102], [101, 2572, 1045, 1996, 2069, 2028, 2008, 4620, 7226, 2368, 2023, 2126, 2035, 1996, 2051, 1029, 1001, 17183, 3207, 20179, 8299, 102], [101, 2339, 5947, 2003, 7989, 2007, 2522, 17258, 1010, 7071, 7233, 1010, 10690, 2326, 3314, 3388, 2278, 1012, 1012, 1012, 5035, 9579, 1004, 23713, 1025, 6285, 2072, 10728, 1012, 1012, 1012, 2119, 4092, 2012, 8398, 2015, 4680, 2023, 2733, 1012, 2228, 2055, 2009, 1012, 1001, 3789, 16558, 5657, 1001, 2933, 29337, 2099, 22994, 2063, 1001, 7226, 2368, 11387, 11387, 1001, 3828, 10760, 2271, 4523, 11387, 11387, 102], [101, 1030, 5310, 17183, 2015, 2031, 15682, 2769, 28289, 1010, 2027, 2079, 2009, 2035, 2058, 1996, 2088, 1998, 2655, 2009, 4681, 1001, 8398, 11387, 11387, 102], [101, 1030, 5310, 1045, 1005, 1049, 2469, 2057, 1005, 2222, 2963, 1996, 5156, 27984, 27984, 27984, 5223, 8398, 27984, 27984, 27984, 16939, 8398, 27984, 27984, 3607, 27984, 1012, 1001, 8398, 19779, 5974, 21604, 22507, 1001, 10556, 2290, 1001, 23848, 2050, 102], [101, 1030, 5310, 2562, 10320, 2023, 2406, 2041, 1001, 23848, 2050, 102], [101, 8398, 2003, 1037, 8040, 3286, 5017, 1012, 1012, 1012, 2119, 2010, 5952, 1998, 2010, 2118, 2002, 2003, 1037, 8040, 3286, 5017, 2017, 2145, 2079, 2025, 2644, 4637, 2068, 8398, 2003, 1037, 8040, 3286, 5017, 1010, 2119, 2010, 5952, 1998, 2010, 2118, 2024, 8040, 3286, 16862, 2017, 2145, 2079, 2025, 2507, 2039, 2115, 2490, 1001, 8398, 11387, 11387, 1001, 7226, 2368, 1001, 6734, 21572, 22199, 2015, 1001, 2304, 3669, 6961, 18900, 3334, 8299, 102], [101, 2204, 2851, 3507, 11579, 999, 2477, 2024, 2893, 4689, 1011, 2994, 2844, 999, 2009, 2097, 2131, 2488, 999, 1001, 23848, 2050, 1001, 8398, 11387, 11387, 102], [101, 1996, 10643, 2552, 2003, 2065, 2027, 4995, 1521, 1056, 7703, 2015, 1012, 2027, 2831, 2055, 2035, 1996, 2919, 2477, 1999, 1996, 2088, 2085, 1998, 7499, 1996, 8037, 1012, 10643, 2024, 1999, 2491, 2157, 2085, 2061, 2129, 2064, 2027, 7499, 2673, 2747, 3308, 2006, 1996, 8037, 1029, 1001, 29300, 2278, 11387, 11387, 102], [101, 10166, 7226, 2368, 2183, 2044, 15941, 2006, 4409, 1012, 1012, 1012, 1001, 17183, 3207, 20179, 1001, 17183, 3207, 20179, 11387, 11387, 1001, 3537, 3207, 20179, 2015, 102], [101, 1030, 5310, 8224, 1024, 2129, 2000, 3789, 2220, 1999, 1000, 2115, 2110, 1000, 1998, 2131, 2009, 2589, 2004, 2574, 2004, 2825, 1012, 2057, 2031, 1037, 4611, 2000, 2393, 1996, 1057, 1012, 1055, 1012, 10690, 2326, 2832, 2122, 17069, 1998, 5547, 1996, 6911, 6221, 8398, 2003, 15734, 6885, 2006, 2122, 2270, 8858, 1012, 1001, 3789, 14644, 2135, 1001, 3789, 16558, 5657, 1001, 7226, 2368, 8167, 6935, 11387, 11387, 102], [101, 1030, 5310, 1030, 5310, 1030, 5310, 1030, 5310, 1030, 5310, 1030, 5310, 1030, 5310, 1030, 5310, 2017, 2812, 2066, 2035, 1996, 2355, 3972, 14499, 2008, 2018, 2115, 13593, 1998, 3478, 4018, 1030, 5310, 3045, 2011, 1037, 20148, 1999, 2296, 8554, 2130, 2046, 2602, 2305, 1029, 1030, 5310, 1001, 23848, 2050, 1001, 8398, 11387, 11387, 5292, 3270, 3270, 1010, 2156, 2017, 1999, 1001, 13045, 21784, 102], [101, 1030, 5310, 4689, 1998, 17109, 2097, 2025, 2113, 2054, 2718, 7861, 2023, 2281, 999, 1001, 23848, 2050, 102], [101, 1030, 5310, 4593, 2070, 2111, 2024, 9943, 1011, 2216, 2168, 2111, 2763, 2342, 2488, 29114, 1998, 2020, 2763, 6830, 2005, 7226, 2368, 4312, 1012, 102], [101, 4897, 2655, 2408, 2637, 2003, 9788, 1998, 1045, 3246, 2057, 2196, 2644, 2725, 2009, 1012, 1006, 2027, 12057, 1996, 10656, 3233, 6279, 5751, 2000, 2169, 3295, 999, 1007, 1001, 17183, 8663, 15338, 3258, 102], [101, 1030, 5310, 1030, 5310, 2204, 2005, 2068, 1012, 1001, 8398, 11387, 11387, 102], [101, 2023, 3179, 2003, 1037, 2442, 3422, 1012, 2054, 3632, 2006, 1999, 1996, 4680, 2077, 1996, 13762, 4664, 2003, 14726, 1012, 1001, 17183, 8663, 15338, 3258, 8299, 102], [101, 1001, 8037, 2024, 10919, 2129, 3809, 2690, 1011, 2637, 2003, 2055, 1001, 2176, 5974, 29100, 2015, 1064, 1001, 8398, 11387, 11387, 1012, 102], [101, 1030, 5310, 1030, 5310, 1030, 5310, 2003, 1037, 9861, 1012, 1001, 2196, 17062, 2368, 2001, 1037, 16939, 2000, 2014, 1037, 2261, 2460, 2706, 3283, 1010, 2085, 2016, 1521, 1055, 2010, 21210, 1012, 2016, 2036, 2064, 1521, 1056, 2448, 2006, 2014, 2501, 2004, 1001, 10722, 4877, 2072, 11387, 18827, 23263, 4197, 2041, 1999, 1996, 1001, 3537, 14379, 1012, 102], [101, 1030, 5310, 1998, 8398, 2180, 1521, 1056, 2022, 2583, 2000, 19815, 2105, 2369, 7226, 2368, 2066, 2002, 2106, 2000, 18520, 1012, 2002, 2038, 2000, 2994, 1999, 2010, 2219, 4644, 100, 102], [101, 1030, 5310, 1030, 5310, 2122, 1001, 28988, 2015, 2024, 2074, 1001, 5881, 5644, 8525, 23267, 1001, 23848, 2050, 2040, 10214, 2000, 3582, 7899, 999, 2016, 10849, 2296, 2978, 1997, 2054, 2016, 2288, 999, 1001, 4929, 8067, 6711, 1001, 2994, 8988, 8462, 1001, 2591, 10521, 26897, 1001, 3582, 7076, 18300, 8496, 102], [101, 1030, 5310, 2204, 2739, 2029, 2097, 2582, 14451, 1996, 2137, 2187, 2004, 3061, 2005, 2498, 4902, 1010, 3272, 1996, 7654, 1997, 2373, 1012, 1045, 1521, 1049, 2214, 2438, 2000, 3342, 2043, 5025, 1523, 13350, 1524, 1006, 2025, 13009, 1007, 4941, 10866, 5233, 1998, 1996, 2510, 3919, 3375, 1012, 1001, 23848, 2050, 102], [101, 4931, 7226, 2368, 1045, 2572, 2025, 4452, 1997, 2115, 4295, 2522, 17258, 1045, 2097, 2514, 13726, 1998, 2007, 2062, 3246, 2007, 1001, 8398, 11387, 11387, 1998, 1045, 2113, 2073, 19337, 28100, 2003, 1998, 2009, 1521, 1055, 2025, 1996, 1050, 6278, 100, 102], [101, 6904, 14194, 2072, 14300, 2015, 1999, 1011, 2711, 6830, 1024, 1520, 2045, 1521, 1055, 2053, 3114, 1521, 2339, 2057, 5807, 1521, 1056, 2022, 3039, 1064, 1996, 3679, 7318, 1001, 2637, 8873, 12096, 1001, 2035, 3669, 6961, 18900, 3334, 1001, 21887, 23350, 8299, 102], [101, 1030, 5310, 1030, 5310, 2085, 1001, 2175, 2361, 1001, 2602, 4584, 11193, 2000, 2404, 2041, 8378, 2000, 8145, 1001, 5653, 2378, 7384, 12868, 2000, 4675, 9416, 5653, 23217, 2078, 1059, 8093, 1012, 1030, 5310, 2844, 2000, 16081, 3789, 1012, 2061, 2404, 3074, 8378, 2012, 17888, 3182, 27046, 5833, 2220, 6830, 1012, 1012, 1030, 5310, 102], [101, 1001, 10973, 27698, 2050, 1001, 10973, 2078, 10258, 1001, 10973, 25311, 2140, 10973, 2035, 16916, 19582, 4013, 7576, 1001, 2637, 8873, 12096, 102], [101, 1030, 5310, 2017, 2288, 2008, 2157, 1010, 2637, 2180, 1005, 1056, 2022, 25857, 1010, 2017, 1005, 2222, 2022, 2019, 1001, 17727, 5243, 7690, 2028, 2744, 1030, 5310, 1012, 1001, 2522, 17258, 16147, 1001, 8398, 2923, 5369, 12155, 12096, 28994, 5178, 10111, 6299, 1001, 3789, 16558, 23361, 8820, 3726, 14074, 14735, 1001, 3789, 16558, 24997, 9626, 12079, 2860, 6806, 8299, 102], [101, 1030, 5310, 4921, 2063, 3427, 2035, 1996, 15281, 2295, 1998, 8398, 2130, 2409, 2317, 10514, 28139, 23738, 2000, 1000, 3233, 2067, 1998, 3233, 2011, 1000, 2612, 1997, 7939, 23709, 6129, 2068, 1012, 102], [101, 1045, 8239, 5223, 8398, 1001, 14379, 11387, 11387, 102], [101, 1000, 2023, 2003, 1037, 2166, 1011, 5278, 2602, 1012, 2023, 2097, 5646, 2054, 2637, 2003, 2183, 2000, 2298, 2066, 2005, 1037, 2146, 1010, 2146, 2051, 1012, 1000, 1516, 1030, 5310, 1001, 17183, 8663, 15338, 3258, 8299, 102], [101, 1001, 4883, 3207, 20179, 11387, 11387, 15757, 1999, 999, 3582, 2033, 2065, 2017, 1001, 3789, 16558, 5657, 2272, 2006, 7226, 2368, 999, 999, 8299, 102], [101, 1030, 5310, 1030, 5310, 100, 2111, 3280, 1999, 2637, 2013, 7870, 2008, 2024, 9526, 3085, 1998, 7438, 3085, 2138, 1997, 1002, 1999, 2637, 2057, 2024, 2489, 2000, 3280, 2061, 2500, 2064, 9991, 11772, 1012, 1001, 4071, 1001, 23848, 2050, 7743, 1012, 6770, 18424, 1012, 102], [101, 1030, 5310, 2092, 1997, 2607, 2031, 2017, 2156, 1996, 3616, 2105, 1996, 2088, 1029, 2003, 2035, 2765, 1997, 1996, 6090, 3207, 7712, 999, 2644, 1996, 5223, 999, 1001, 8398, 11387, 11387, 102], [101, 9826, 2064, 1030, 5310, 2022, 2343, 3892, 1029, 1001, 7226, 2368, 8167, 6935, 11387, 11387, 1001, 17183, 8663, 15338, 3258, 102], [101, 1030, 5310, 1030, 5310, 1030, 5310, 1030, 5310, 21911, 2050, 5671, 2097, 2022, 1037, 3181, 21210, 999, 1037, 2613, 2450, 1999, 1996, 2317, 2160, 999, 2053, 2062, 22635, 1056, 28394, 3215, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1001, 15653, 24456, 2361, 8299, 102], [101, 1030, 5310, 2017, 2031, 1015, 22399, 1010, 3582, 2053, 2028, 1998, 2031, 2053, 27263, 1012, 1012, 1012, 1012, 2115, 5448, 5609, 2339, 1029, 1001, 8398, 11387, 11387, 1001, 1059, 27767, 2487, 27767, 2050, 102], [101, 1001, 9779, 8197, 13791, 2003, 3145, 2000, 3045, 1996, 1001, 12609, 12260, 7542, 999, 3191, 2062, 2182, 2055, 2129, 2057, 2064, 2490, 1001, 7226, 2368, 8167, 6935, 11387, 11387, 1012, 1030, 5310, 1030, 5310, 1030, 5310, 1030, 5310, 1001, 9779, 8197, 11387, 11387, 1001, 9779, 6593, 21967, 8299, 102], [101, 2035, 2023, 2051, 1045, 1521, 2310, 5191, 7226, 2368, 2001, 1037, 19813, 24247, 7490, 2021, 16047, 1996, 1040, 12273, 1521, 1055, 3247, 2000, 12944, 2010, 4434, 2007, 4603, 12505, 1998, 2037, 16730, 2005, 2032, 2038, 26350, 2033, 2010, 12912, 2015, 2007, 1996, 2187, 2024, 10124, 2000, 2512, 1011, 25953, 1001, 3537, 25434, 8663, 15338, 3258, 102], [101, 7825, 5363, 13599, 4126, 1999, 17183, 3655, 2007, 2175, 2361, 2448, 3655, 1006, 2025, 2469, 10094, 2023, 2391, 1007, 3533, 2667, 2000, 2831, 2055, 9435, 2021, 2025, 2437, 3168, 1012, 7825, 5176, 3533, 2055, 24964, 2863, 11528, 2075, 21107, 1050, 1038, 13728, 3533, 2758, 6135, 4941, 2000, 13366, 8630, 2075, 2610, 1010, 7499, 2015, 8398, 2005, 6276, 2037, 26178, 1012, 102], [101, 1030, 5310, 100, 1030, 5310, 2003, 1037, 2200, 1001, 4795, 1001, 2343, 1012, 2002, 2003, 100, 1996, 10442, 4920, 1012, 2002, 2003, 2025, 5214, 2000, 2599, 2002, 2069, 4282, 2129, 2000, 1001, 20716, 100, 1001, 6221, 24456, 2361, 2069, 14977, 2055, 2370, 1012, 2053, 2028, 100, 1001, 8398, 14268, 10483, 2121, 1001, 8398, 3334, 29165, 2964, 3342, 1001, 3789, 100, 1001, 7226, 2368, 8167, 15061, 8820, 3726, 14074, 14735, 102], [101, 2026, 2540, 2003, 14107, 999, 2196, 2042, 12774, 999, 1001, 2175, 16558, 5657, 1001, 1040, 12273, 11387, 11387, 1001, 7226, 2368, 8167, 6935, 11387, 11387, 1001, 2131, 14615, 11253, 3995, 13876, 14995, 6591, 8299, 102], [101, 1030, 5310, 1030, 5310, 1030, 5310, 1030, 5310, 1030, 5310, 1030, 5310, 1030, 5310, 1030, 5310, 1030, 5310, 1030, 5310, 1030, 5310, 1030, 5310, 1030, 5310, 1030, 5310, 1030, 5310, 1030, 5310, 12475, 1996, 11963, 999, 2023, 2005, 1000, 13593, 2576, 5069, 1000, 999, 999, 999, 2057, 1996, 2111, 24104, 2491, 2058, 2256, 2231, 1001, 23848, 2050, 1001, 23848, 2050, 1030, 5310, 8299, 102], [101, 1030, 5310, 1030, 5310, 1001, 3533, 17062, 2368, 1998, 2228, 1999, 1996, 2168, 6251, 1012, 2008, 2428, 2003, 1037, 7683, 1011, 1001, 3835, 11129, 1001, 9857, 2705, 10593, 3215, 1001, 8275, 2638, 9333, 1001, 29450, 8299, 102], [101, 1030, 5310, 6616, 1012, 1012, 1012, 1012, 1012, 2448, 999, 999, 1998, 1001, 3789, 16558, 24997, 9626, 12079, 2860, 6806, 102], [101, 3374, 6733, 1010, 6904, 14194, 2072, 1004, 23713, 1025, 17183, 2015, 1010, 2023, 4485, 2573, 1012, 2057, 2113, 1057, 2215, 2062, 6677, 2000, 2139, 16340, 9869, 1004, 23713, 1025, 2000, 2191, 8398, 2298, 2919, 1010, 2021, 2004, 2062, 2111, 2202, 2009, 1004, 23713, 1025, 5256, 2039, 2115, 2933, 2003, 2108, 100, 1001, 8398, 11387, 11387, 1001, 1053, 6761, 3619, 8299, 102], [101, 1053, 2620, 1024, 2197, 3160, 1012, 2559, 3805, 1010, 2054, 3160, 2006, 4517, 3314, 2024, 2017, 5327, 1996, 5347, 2097, 2022, 2356, 1999, 1996, 1001, 17183, 3207, 20179, 4826, 1029, 1001, 5947, 28014, 2015, 11231, 9681, 102], [101, 1012, 1030, 5310, 2006, 2522, 17258, 1011, 2539, 1024, 2057, 1005, 2222, 2404, 1996, 4331, 4998, 1012, 2057, 1005, 2222, 2202, 1996, 17750, 2125, 1996, 8519, 2061, 1996, 1996, 2270, 4152, 1996, 2592, 2027, 2342, 1998, 10107, 1012, 7481, 4895, 10755, 28357, 3606, 1012, 2027, 2064, 5047, 2009, 1012, 1001, 17183, 8663, 15338, 3258, 1030, 5310, 8299, 102], [101, 15887, 2256, 2495, 2291, 2097, 23216, 1996, 4610, 1998, 3073, 2488, 13105, 2005, 4841, 1012, 2008, 1005, 1055, 2339, 6864, 1047, 4135, 25987, 2906, 2097, 2191, 2028, 1011, 1998, 2048, 1011, 2095, 2451, 2267, 3454, 2489, 1012, 1001, 17183, 3207, 20179, 8299, 102], [101, 1030, 5310, 1001, 15941, 2038, 2664, 2000, 10797, 2010, 4121, 17053, 1997, 10284, 1012, 15274, 1001, 1040, 12273, 1002, 11503, 1001, 3565, 9247, 29107, 4570, 1002, 1061, 13473, 2213, 2031, 12225, 2039, 2045, 10353, 2074, 1999, 2553, 15941, 1004, 23713, 1025, 1001, 6887, 16585, 8167, 6935, 1005, 2184, 1002, 1051, 13247, 2923, 4132, 2685, 2131, 7463, 1012, 1001, 15941, 8167, 6935, 13675, 11149, 2215, 2000, 2022, 1996, 13997, 23029, 2369, 1001, 4030, 5558, 2063, 999, 1001, 1038, 2571, 9048, 2102, 11387, 11387, 999, 8299, 102], [101, 8398, 1005, 1055, 4760, 2008, 2002, 2003, 2025, 4906, 2000, 2022, 3003, 1997, 1037, 2879, 7464, 10123, 1010, 2292, 2894, 2343, 1997, 1996, 2142, 2163, 1012, 1001, 14379, 11387, 11387, 1001, 4883, 3207, 20179, 11387, 11387, 1001, 4883, 3207, 20179, 1001, 8398, 11387, 11387, 102], [101, 1030, 5310, 3114, 2070, 13109, 10050, 11692, 2015, 2655, 2032, 3099, 6677, 4630, 2483, 999, 2129, 2064, 1030, 5310, 2130, 2228, 2374, 2043, 4841, 1054, 5996, 1010, 2116, 5604, 3893, 1004, 23713, 1025, 2008, 2950, 2336, 999, 3516, 1005, 1055, 1001, 1005, 1055, 1054, 2025, 2183, 2091, 3435, 2438, 1018, 2023, 18667, 999, 1001, 3789, 16558, 23361, 8820, 3726, 14074, 14735, 2013, 8398, 1004, 23713, 1025, 6677, 4630, 2483, 102], [101, 1030, 5310, 1030, 5310, 2008, 2052, 2022, 3835, 999, 1001, 3789, 16558, 23361, 8820, 3726, 14074, 14735, 102], [101, 1030, 5310, 6300, 2050, 2054, 2428, 29421, 2033, 2003, 8398, 2003, 3497, 2000, 2031, 1037, 2602, 2154, 2599, 2013, 2602, 2154, 3789, 1998, 8398, 2097, 2022, 9577, 1998, 4366, 3377, 5653, 1999, 4494, 1019, 2000, 1015, 2005, 7226, 2368, 1012, 102], [101, 1030, 5310, 3422, 2023, 8554, 10436, 2066, 3894, 2320, 8398, 2058, 15166, 2015, 7226, 2368, 1012, 8299, 102], [101, 1045, 2428, 2228, 1996, 1001, 17183, 3207, 20179, 3065, 2129, 3144, 1030, 5310, 1998, 1030, 5310, 2064, 2022, 2006, 2154, 2028, 1012, 1045, 2228, 2027, 2052, 2022, 2200, 3144, 2000, 26964, 1996, 2783, 5325, 1997, 18906, 2015, 1998, 1041, 24290, 102], [101, 1030, 5310, 1030, 5310, 2307, 3232, 999, 1001, 2157, 1001, 10556, 2290, 1001, 12925, 1001, 2358, 23743, 2483, 3597, 6279, 2571, 102], [101, 1030, 5310, 5796, 1012, 5143, 2003, 2019, 6151, 8586, 14097, 14303, 2021, 2035, 1996, 2477, 2016, 2001, 6314, 2055, 2020, 3303, 2011, 1996, 2200, 2711, 2016, 3271, 2404, 1999, 2436, 999, 2054, 2062, 2515, 2016, 2342, 2000, 2156, 1029, 2065, 8380, 1010, 2199, 4903, 2140, 2052, 2031, 2351, 2104, 1037, 17183, 1012, 2008, 2052, 2022, 1996, 2203, 2025, 2000, 5254, 3105, 6409, 999, 1001, 3789, 16558, 5657, 102], [101, 1001, 2149, 28994, 5178, 19909, 3207, 20179, 11387, 11387, 2054, 1037, 4485, 2265, 1012, 2128, 1024, 8398, 1010, 2009, 2001, 2066, 3666, 1037, 2775, 2383, 1037, 2048, 3178, 21396, 2243, 1998, 9092, 24456, 100, 1001, 3789, 16558, 5657, 2637, 999, 102], [101, 1045, 2071, 2031, 2042, 8854, 7632, 9516, 3372, 1012, 1001, 3425, 29278, 9336, 8540, 10606, 16885, 1001, 21911, 23278, 18752, 2015, 2615, 2361, 1001, 3533, 17062, 2368, 29278, 28994, 5178, 3372, 11387, 11387, 1001, 5035, 6673, 8883, 14204, 1001, 6221, 24456, 2361, 1001, 1996, 8873, 3726, 1001, 18281, 4904, 8151, 8299, 102], [101, 2005, 2033, 1010, 1996, 2087, 17075, 2930, 1997, 1030, 5310, 1521, 1055, 9920, 4613, 1024, 1523, 2176, 3181, 25332, 1012, 2035, 2012, 1996, 2168, 2051, 1012, 1037, 3819, 4040, 1012, 1996, 5409, 6090, 3207, 7712, 1999, 2058, 2531, 2086, 1012, 1996, 5409, 3171, 5325, 2144, 1996, 2307, 6245, 1012, 1524, 1001, 17183, 8663, 15338, 3258, 1006, 1015, 1013, 1016, 1007, 102], [101, 1030, 5310, 1030, 5310, 2087, 4487, 11365, 3512, 1013, 13593, 2343, 1999, 2381, 1001, 8112, 5867, 1001, 8398, 11387, 11387, 102], [101, 1045, 2215, 2000, 5463, 2028, 1997, 8398, 2015, 1001, 9379, 21572, 22199, 1030, 5310, 1001, 8398, 11387, 11387, 1001, 8398, 1001, 8398, 23654, 11387, 11387, 1001, 2190, 28994, 5178, 10111, 6299, 19961, 1001, 3429, 1001, 8398, 11387, 11387, 8653, 24198, 1001, 2008, 6491, 22571, 6072, 5178, 3372, 1001, 4067, 29337, 2213, 14536, 6072, 5178, 3372, 1001, 4642, 2239, 2386, 1001, 1018, 5974, 2549, 24456, 2361, 1001, 2637, 4213, 3489, 6279, 1001, 7226, 10497, 13665, 2401, 1001, 1018, 5974, 29100, 2015, 1001, 8398, 22123, 6824, 7685, 1001, 2293, 8029, 28139, 14326, 4765, 100, 102], [101, 15941, 12055, 8847, 2012, 1996, 3537, 2120, 4680, 3892, 1030, 5310, 1030, 5310, 1001, 15941, 8791, 13375, 1001, 3537, 8663, 15338, 3258, 1001, 1040, 12273, 11387, 11387, 8299, 102], [101, 2339, 2052, 3087, 3789, 2005, 1037, 4018, 2008, 2758, 2037, 3789, 5807, 1521, 1056, 4175, 1029, 1029, 1029, 1029, 1029, 10507, 1024, 1030, 5310, 1030, 5310, 1030, 5310, 1030, 5310, 1030, 5310, 1001, 20138, 2098, 8663, 15338, 3258, 1001, 3537, 3207, 20179, 1001, 17183, 3207, 20179, 2015, 1001, 3537, 3207, 20179, 2015, 1001, 17183, 3207, 20179, 11387, 11387, 102], [101, 8037, 2215, 1996, 1002, 5174, 12163, 2769, 2000, 2994, 2045, 2061, 2037, 12925, 1045, 2812, 1523, 8090, 1524, 2994, 6787, 1998, 2122, 22822, 5644, 2123, 1521, 1056, 2031, 2000, 2147, 1012, 1001, 8398, 11387, 11387, 1001, 8037, 12069, 6155, 13181, 14147, 14074, 14735, 102], [101, 1045, 2081, 2469, 1045, 2128, 12155, 8630, 1998, 2680, 2008, 4861, 8398, 2056, 2096, 2196, 2412, 28525, 2317, 22006, 1012, 1012, 1523, 7098, 3337, 1012, 1012, 1012, 1012, 1012, 3233, 2067, 1998, 3233, 2011, 1524, 4283, 6221, 999, 999, 999, 15488, 16425, 999, 999, 999, 999, 999, 2065, 1061, 1521, 2035, 2134, 1521, 1056, 2113, 1010, 1010, 1010, 1010, 1010, 2085, 2017, 2113, 1001, 5981, 11387, 11387, 1001, 4883, 3207, 20179, 1001, 2136, 17062, 2368, 102], [101, 2023, 7427, 1997, 4464, 4790, 2006, 21887, 1010, 4331, 1010, 5446, 2739, 2064, 2022, 2179, 2012, 8299, 1037, 2655, 2000, 7308, 2039, 1024, 7226, 2368, 23876, 2163, 2000, 11405, 1010, 4841, 2000, 4929, 2068, 8299, 1001, 3789, 2630, 1001, 17183, 2015, 1001, 7226, 2368, 1001, 2136, 5558, 2063, 1001, 9293, 16211, 2094, 1001, 3533, 17062, 2368, 1001, 7226, 2368, 29278, 28994, 5178, 3372, 102], [101, 1030, 5310, 5292, 6203, 8840, 16585, 24247, 3571, 12256, 4590, 2075, 1001, 19971, 8566, 3600, 5172, 9875, 1030, 5310, 1012, 1012, 1012, 2069, 3160, 2003, 1010, 2043, 2038, 2637, 1005, 1055, 6716, 2025, 2018, 1037, 17284, 2006, 2137, 3629, 1013, 3268, 1029, 1001, 8244, 13028, 6824, 2361, 1001, 8244, 1001, 10556, 2290, 1001, 2510, 1001, 3915, 10867, 2100, 1001, 3915, 4313, 14821, 1001, 15529, 2290, 1001, 2149, 2532, 10736, 1001, 2149, 12458, 1001, 2610, 1001, 2244, 14526, 2705, 1001, 20720, 8299, 102], [101, 8398, 1037, 2307, 3003, 2003, 1037, 2204, 19373, 1001, 14379, 11387, 11387, 102], [101, 7226, 2368, 2525, 4165, 9069, 102], [101, 2065, 2057, 2024, 2183, 2000, 4047, 2256, 4230, 2059, 2057, 2064, 1521, 1056, 11160, 2006, 5434, 1012, 2057, 2342, 6094, 2000, 4047, 2308, 1521, 1055, 2157, 2000, 3601, 1012, 1001, 17183, 3207, 20179, 2015, 1001, 3537, 3207, 20179, 8299, 102], [101, 2044, 2305, 1015, 1997, 1996, 3537, 4750, 2283, 4680, 1010, 2009, 1521, 1055, 24509, 2084, 2412, 2077, 2054, 2027, 1521, 2128, 3038, 2000, 2149, 1012, 3789, 2005, 2149, 2030, 2054, 1521, 1055, 2042, 6230, 1999, 6734, 1010, 5862, 1010, 1998, 3190, 2097, 2131, 1037, 2843, 4788, 1012, 1001, 2203, 15222, 26212, 2094, 2791, 1001, 8398, 11387, 11387, 102], [101, 1030, 5310, 2002, 1005, 1055, 2005, 2505, 2008, 2097, 2562, 1996, 2270, 2013, 3773, 1996, 11113, 7274, 9067, 3737, 1997, 1996, 7672, 7281, 1012, 1001, 14379, 2097, 3584, 1996, 10500, 1998, 1996, 15775, 4246, 1012, 102], [101, 1045, 3046, 2000, 4468, 7603, 2006, 2591, 2865, 1010, 2664, 1045, 2215, 2000, 6978, 999, 1045, 1521, 1049, 2061, 5305, 1997, 1037, 2343, 4039, 2000, 2330, 2010, 2404, 14615, 2677, 2302, 11273, 2000, 1996, 2391, 1997, 29290, 3571, 1012, 15941, 1010, 2485, 2115, 2677, 999, 2017, 2614, 10785, 2066, 8398, 999, 1030, 5310, 100, 1001, 17183, 3207, 20179, 102], [101, 1030, 5310, 2017, 2081, 1037, 8257, 1997, 1996, 2436, 1997, 1996, 8798, 1012, 2023, 2347, 1005, 1056, 1037, 5981, 1010, 2017, 2081, 2009, 1037, 9661, 1012, 1996, 2069, 3924, 2187, 2024, 1996, 2145, 7144, 2006, 1996, 12849, 6030, 3593, 8754, 3429, 1012, 1012, 1012, 1012, 1998, 2130, 2070, 1997, 2068, 2024, 3810, 1012, 2031, 2070, 13372, 1998, 12897, 999, 1001, 7226, 2368, 19291, 23816, 15878, 3686, 1001, 5981, 11387, 11387, 1001, 8398, 7011, 18450, 102], [101, 2272, 2006, 1010, 3533, 1012, 6343, 1999, 2037, 2157, 2568, 4122, 2000, 2562, 2037, 2783, 5427, 1012, 2040, 4122, 2000, 2131, 5305, 1010, 2059, 2036, 2131, 1996, 2794, 6781, 1997, 2383, 2000, 3477, 2005, 2009, 1029, 1001, 17183, 3207, 20179, 102], [101, 1030, 5310, 2003, 4013, 22776, 4808, 2007, 4330, 17071, 4765, 17871, 1012, 1001, 29300, 2278, 11387, 11387, 102], [101, 1030, 5310, 3926, 2032, 999, 1001, 17183, 3207, 20179, 102], [101, 1030, 5310, 1030, 5310, 1001, 17183, 8663, 15338, 3258, 2074, 2056, 2002, 2097, 10468, 6100, 1030, 5310, 3426, 2002, 2038, 2053, 4784, 1997, 2010, 2219, 1012, 1001, 2602, 11387, 11387, 2036, 1001, 3915, 2003, 2172, 7046, 1996, 2060, 3032, 2061, 2748, 2057, 2052, 2599, 1996, 2088, 1999, 1001, 21887, 23350, 3572, 1012, 4606, 1030, 5310, 2134, 1521, 1056, 2215, 2000, 2485, 2604, 2545, 1010, 8398, 2106, 2220, 999, 102], [101, 1030, 5310, 1030, 5310, 1045, 3246, 2008, 1005, 1055, 2157, 999, 1001, 14303, 2098, 13122, 10696, 14074, 14735, 1001, 8398, 11387, 11387, 1001, 6181, 15455, 8791, 18927, 4402, 15455, 2015, 102], [101, 1030, 5310, 1996, 6714, 1030, 5310, 2003, 2667, 2000, 2079, 2006, 2023, 5981, 2003, 9951, 999, 1001, 3870, 9028, 7389, 2743, 2185, 2007, 2023, 5981, 1012, 1001, 17183, 3207, 20179, 102], [101, 1030, 5310, 4459, 2332, 3268, 1999, 1996, 13132, 4224, 1997, 2010, 17183, 14088, 2568, 1012, 1001, 26505, 2923, 5886, 15879, 9739, 3207, 7712, 1001, 8398, 11387, 11387, 102], [101, 1030, 5310, 1030, 5310, 2059, 2339, 2025, 3499, 2370, 2000, 2022, 7039, 2061, 2009, 2064, 2022, 20119, 2011, 1037, 2353, 2283, 2008, 7226, 2368, 2003, 2025, 2108, 7349, 6998, 2030, 7197, 1999, 2151, 2126, 1029, 102], [101, 1030, 5310, 1030, 5310, 2003, 2019, 20676, 20716, 2040, 2074, 7566, 2058, 2111, 2138, 2002, 2038, 2053, 9789, 1012, 1001, 14379, 11387, 11387, 1001, 3789, 5558, 15878, 5178, 2078, 1001, 8398, 5302, 4948, 102], [101, 1030, 5310, 1030, 5310, 2129, 2172, 2062, 3350, 2515, 1996, 2088, 2342, 2008, 8398, 1998, 1996, 2610, 2024, 7694, 24083, 1012, 8090, 1999, 2637, 2024, 2467, 9379, 2127, 1996, 2610, 2265, 2039, 1012, 2059, 1996, 4808, 4627, 2007, 2068, 7866, 1010, 3342, 2023, 2154, 1998, 1001, 3789, 16558, 23361, 8820, 3726, 14074, 14735, 102], [101, 2057, 2342, 4177, 2040, 14977, 2005, 2035, 1997, 1996, 2137, 2111, 1012, 2057, 1521, 2128, 2025, 2019, 23949, 2406, 1010, 2057, 2123, 1521, 1056, 2031, 1037, 2332, 100, 1010, 1998, 2637, 2038, 2467, 2018, 1037, 2343, 1012, 2065, 2057, 1001, 3789, 17062, 2368, 1010, 2057, 2097, 3828, 2256, 2406, 1012, 2123, 1521, 1056, 2507, 2185, 2256, 7072, 1012, 1001, 3789, 16558, 5657, 1001, 3789, 16558, 23361, 8820, 3726, 14074, 14735, 8299, 102], [101, 4905, 1998, 9568, 2923, 14089, 5416, 2072, 2985, 2172, 1997, 2014, 4613, 21289, 3533, 7226, 2368, 1998, 2010, 2155, 1012, 2016, 2001, 2036, 2028, 1997, 1996, 2261, 1001, 29300, 2278, 11387, 11387, 7492, 2000, 2203, 2014, 4613, 2007, 2592, 2005, 7193, 2006, 2129, 2000, 4236, 2000, 3789, 1012, 8299, 102], [101, 1030, 5310, 1045, 2228, 2035, 1996, 1520, 26122, 1521, 2006, 1996, 7226, 2368, 3049, 2420, 2003, 2068, 2667, 2000, 2424, 1996, 2190, 4319, 18901, 2000, 3499, 7226, 2368, 2000, 2298, 2741, 11638, 1012, 3979, 1998, 7561, 1012, 102], [101, 1030, 5310, 10825, 2149, 1997, 2129, 5367, 21572, 20614, 2003, 3561, 2007, 1037, 9129, 1997, 13009, 2008, 4299, 2331, 2006, 2637, 1029, 2057, 2113, 2008, 2525, 1012, 1001, 8398, 11387, 11387, 102], [101, 1030, 5310, 14649, 2003, 2054, 1996, 3537, 2283, 2003, 2667, 2000, 3288, 2091, 2006, 2256, 2219, 2406, 999, 999, 999, 12475, 2008, 11963, 999, 999, 1001, 8398, 11387, 11387, 1001, 8398, 3736, 3726, 14074, 14735, 102], [101, 1030, 5310, 1030, 5310, 1030, 5310, 1030, 5310, 2002, 2323, 2022, 1999, 7173, 2025, 1999, 1996, 2317, 2160, 1012, 1001, 21877, 3527, 5558, 2063, 1001, 3533, 17062, 2368, 8299, 102], [101, 1030, 5310, 18079, 16515, 2099, 1010, 8398, 4282, 2085, 2002, 2097, 4558, 2602, 1012, 2298, 2000, 2032, 2000, 12897, 1012, 11675, 2075, 2035, 17069, 3459, 2005, 7226, 2368, 2138, 7279, 3401, 2347, 1005, 1056, 2006, 1996, 10428, 2004, 2343, 1012, 8398, 2001, 1012, 19195, 2010, 4366, 1996, 5653, 1011, 1999, 17069, 2024, 1037, 9861, 1012, 1996, 26960, 2099, 1010, 16795, 2015, 2365, 1010, 1996, 3424, 26654, 8398, 1012, 102], [101, 1030, 5310, 12464, 1521, 1055, 4167, 1521, 1055, 5525, 5490, 10179, 27512, 2290, 9016, 19983, 1012, 2016, 1013, 2002, 2003, 4415, 12264, 999, 2035, 3855, 4436, 2020, 1037, 13996, 1997, 1996, 17183, 2015, 2219, 7268, 26736, 1011, 2568, 25774, 1012, 2079, 2027, 2031, 1037, 5177, 2004, 2092, 2004, 1037, 6259, 8761, 1029, 2079, 2027, 2066, 1004, 23713, 1025, 6225, 3915, 1010, 1996, 4552, 1004, 23713, 1025, 4480, 1029, 1001, 8398, 11387, 11387, 100, 102], [101, 1030, 5310, 26823, 2080, 7226, 2368, 2006, 4830, 4424, 5850, 18629, 1029, 1029, 102], [101, 3533, 7226, 2368, 2003, 1037, 5294, 16939, 1012, 2662, 2001, 2320, 2417, 1012, 2228, 2055, 2077, 1012, 2256, 2047, 4245, 3791, 2000, 2298, 2067, 1012, 1045, 2031, 1037, 2568, 2008, 3632, 2067, 2531, 2086, 2138, 1997, 2054, 2026, 2155, 1998, 2256, 2808, 2031, 4036, 2033, 1012, 2054, 2003, 1037, 3565, 15267, 1010, 3533, 1029, 1029, 1029, 100, 1001, 14379, 102], [101, 1030, 5310, 10643, 2024, 17522, 15069, 2015, 1012, 1996, 2972, 2843, 1997, 2068, 1012, 2265, 2068, 2053, 8673, 1999, 2281, 1012, 1001, 3789, 16558, 23361, 8820, 3726, 14074, 14735, 102], [101, 3666, 3533, 7226, 2368, 3921, 1996, 14502, 1004, 23713, 1025, 5138, 1996, 6488, 2302, 2151, 20737, 2003, 4895, 22852, 1001, 17183, 8663, 15338, 3258, 102], [101, 1030, 5310, 1045, 1005, 1049, 2469, 2008, 2027, 2071, 2131, 2070, 10552, 6040, 2013, 1996, 7226, 2368, 3049, 102], [101, 1030, 5310, 3374, 2021, 2002, 2003, 2026, 2343, 999, 1001, 8398, 11387, 11387, 1001, 8398, 11387, 11387, 8653, 24198, 1001, 8398, 4063, 22043, 8163, 6038, 29171, 102], [101, 1030, 5310, 1030, 5310, 2116, 1997, 1996, 6997, 1996, 8398, 2155, 2038, 5462, 2024, 2110, 1010, 2025, 2976, 1010, 6997, 1012, 7226, 2368, 2038, 2053, 2373, 2000, 14933, 2216, 1012, 4496, 2515, 8398, 2030, 7279, 3401, 2031, 2008, 2373, 1012, 102], [101, 1030, 5310, 1045, 3571, 2017, 1521, 2128, 2157, 1012, 1045, 11839, 1030, 5310, 1520, 1055, 3416, 2744, 2097, 2404, 2062, 3579, 12388, 29163, 1996, 11963, 1012, 2002, 5791, 6086, 2009, 1012, 2228, 2054, 2057, 2113, 2085, 6431, 2602, 2154, 1521, 2385, 1012, 1012, 1012, 1001, 23848, 2050, 1001, 8398, 11387, 11387, 102], [101, 1030, 5310, 16780, 999, 1001, 8398, 11387, 11387, 2035, 1996, 2126, 999, 1001, 2067, 10760, 16558, 5657, 1001, 3808, 8873, 12096, 102], [101, 10166, 15941, 5472, 2121, 2003, 1996, 2069, 2028, 2040, 6753, 7072, 1001, 17183, 3207, 20179, 102], [101, 1045, 1521, 1049, 3492, 2469, 1037, 2843, 1997, 2111, 2097, 2022, 3666, 2023, 5981, 2025, 2138, 2027, 1521, 2128, 4699, 1999, 7226, 2368, 2030, 1996, 4589, 2158, 1521, 1055, 2576, 11032, 2006, 2367, 3314, 1010, 2021, 2138, 2009, 1521, 2222, 2022, 2066, 1037, 4507, 2265, 100, 1012, 1012, 1012, 7226, 2368, 2003, 2525, 2124, 2005, 5177, 11721, 16020, 1521, 1055, 1999, 2010, 2576, 22867, 1012, 1012, 102], [101, 1030, 5310, 3789, 2417, 2111, 1012, 1012, 1001, 8398, 11387, 11387, 102], [101, 1030, 5310, 1030, 5310, 1001, 8398, 11387, 11387, 5514, 3531, 999, 999, 999, 999, 102], [101, 1030, 5310, 1030, 5310, 1030, 5310, 15624, 999, 2007, 4419, 18329, 2091, 3759, 2015, 1997, 1996, 2137, 2111, 2007, 8040, 2819, 2066, 4380, 2063, 999, 2040, 3791, 6716, 999, 2204, 9436, 25514, 1030, 5310, 1001, 8398, 11387, 11387, 8299, 102], [101, 1030, 5310, 1030, 5310, 1997, 2607, 2017, 2024, 4285, 2111, 999, 1996, 2069, 3114, 1045, 2572, 2006, 10474, 2003, 2000, 7532, 2033, 2007, 2066, 1011, 13128, 2111, 2040, 3745, 1037, 2691, 3125, 1011, 1011, 2203, 2023, 2120, 10103, 999, 1001, 3789, 16558, 23361, 8820, 3726, 14074, 14735, 102], [101, 2057, 2323, 8642, 6366, 9501, 2013, 2035, 4883, 14379, 1012, 2061, 2172, 2051, 2003, 13842, 2011, 23100, 20737, 1998, 24867, 1012, 2023, 2038, 2042, 1037, 3052, 1997, 4840, 2250, 1012, 1001, 17183, 3207, 20179, 102], [101, 2013, 1001, 3960, 3702, 7652, 2015, 2338, 1001, 8112, 26760, 11650, 1010, 17012, 2213, 1012, 1012, 1012, 2040, 6732, 1030, 5310, 2323, 2131, 1996, 1001, 10501, 18098, 4697, 2157, 2185, 1012, 2057, 2024, 5026, 2111, 2188, 1012, 1030, 5310, 1030, 5310, 1001, 8398, 11387, 11387, 8299, 102], [101, 1030, 5310, 4283, 2005, 6631, 1030, 5310, 100, 1996, 15285, 2024, 2028, 1997, 1996, 2087, 3928, 2945, 2182, 1999, 2660, 1012, 1001, 12475, 24138, 4213, 8737, 1001, 3828, 8162, 19339, 7389, 102], [101, 1012, 1030, 5310, 2580, 2023, 16775, 25539, 1025, 2057, 2035, 2342, 2000, 1001, 3789, 16558, 5657, 2000, 3828, 3268, 1012, 1012, 1012, 1057, 1012, 1055, 1012, 4311, 3284, 2193, 1997, 2522, 17258, 1011, 2539, 6677, 1999, 2028, 2154, 2144, 3054, 1011, 2089, 8299, 102], [101, 1030, 5310, 2296, 2801, 7226, 2368, 2404, 5743, 2055, 2522, 17258, 2018, 2525, 2042, 8885, 2011, 8398, 1012, 7226, 2368, 8246, 1012, 102], [101, 1030, 5310, 2017, 2360, 2017, 2215, 2000, 1001, 23848, 2050, 2664, 2017, 9969, 9828, 2000, 1996, 2270, 2012, 1996, 2927, 1997, 1996, 1001, 2522, 17258, 16147, 6090, 3207, 7712, 1998, 1001, 2209, 4183, 7698, 1011, 8299, 102], [101, 1030, 5310, 2017, 2074, 2123, 1521, 1056, 2507, 1037, 4485, 2055, 2111, 1521, 1055, 3268, 2079, 2017, 1029, 1001, 3789, 16558, 23361, 8820, 3726, 14074, 14735, 100, 999, 102], [101, 1030, 5310, 1030, 5310, 1030, 5310, 12778, 2010, 7773, 1029, 1048, 2863, 2080, 1012, 1012, 1012, 7226, 2368, 1998, 2010, 2564, 9511, 7079, 26854, 7773, 2006, 2037, 2338, 11372, 2011, 2074, 7079, 3209, 1037, 2235, 3815, 1998, 2975, 1996, 2717, 1999, 2037, 1000, 3840, 1000, 2002, 6994, 3423, 7077, 19990, 2066, 8398, 1998, 3071, 2515, 1012, 102], [101, 1030, 5310, 1030, 5310, 1030, 5310, 7078, 999, 2045, 2003, 1037, 2678, 2029, 3065, 7648, 2108, 5045, 2012, 2011, 1037, 2158, 2007, 1037, 28497, 1012, 2017, 2064, 2941, 2156, 1996, 5195, 2543, 2021, 2009, 2074, 2987, 1005, 1056, 4906, 1996, 7984, 1012, 1001, 8398, 11387, 11387, 102], [101, 1030, 5310, 1030, 5310, 8148, 1010, 2199, 1009, 4841, 2031, 2351, 2013, 1996, 1001, 8398, 23350, 11387, 11387, 1010, 2131, 1996, 3109, 2125, 10474, 1998, 2079, 2115, 4365, 3105, 999, 999, 999, 1001, 3789, 16558, 23361, 8820, 3726, 14074, 14735, 1001, 7226, 2368, 11387, 11387, 1001, 7226, 2368, 8167, 6935, 11387, 11387, 102], [101, 2821, 2053, 999, 3532, 1030, 5310, 2003, 2551, 2428, 2524, 2000, 17894, 2010, 3611, 1012, 3374, 1010, 2021, 2115, 3658, 2055, 21887, 23350, 1011, 2096, 8052, 1011, 2097, 2196, 2022, 2438, 1012, 1001, 29300, 2278, 11387, 11387, 102], [101, 1030, 5310, 2339, 1029, 2054, 2024, 2027, 1010, 1016, 2086, 4237, 1029, 7226, 2368, 2097, 2272, 2041, 11820, 2302, 1037, 4797, 1012, 102], [101, 1030, 5310, 1030, 5310, 1998, 1030, 5310, 2031, 2000, 2061, 2860, 4797, 1998, 21910, 1998, 2097, 2360, 2044, 1996, 5981, 2008, 7226, 2368, 22673, 1011, 2004, 2027, 2525, 2024, 3188, 2000, 1012, 2010, 8754, 2097, 2903, 3649, 2002, 2758, 1012, 2009, 1521, 1055, 2061, 14888, 102], [101, 1030, 5310, 2061, 2017, 2812, 2000, 2425, 2033, 2008, 9282, 2160, 23744, 2064, 1521, 1056, 2022, 2308, 1997, 3609, 1029, 1029, 2030, 2008, 2659, 3318, 2069, 5218, 2000, 2111, 1997, 3609, 1029, 1029, 1029, 1029, 2024, 2017, 3809, 1029, 1029, 1029, 2017, 2024, 2054, 2003, 3308, 2007, 2637, 1010, 19424, 1001, 8398, 11387, 11387, 100, 102], [101, 18932, 1018, 1024, 3533, 7226, 2368, 1024, 1037, 6480, 1997, 4688, 1004, 23713, 1025, 20228, 16098, 6935, 2213, 1012, 2292, 1005, 1055, 2202, 1037, 19043, 1010, 4618, 2057, 1029, 1005, 3533, 7226, 2368, 1005, 1055, 4688, 1010, 18917, 1998, 10699, 15074, 2015, 2191, 2032, 1037, 3532, 3601, 2005, 2343, 1012, 2515, 2023, 4863, 2339, 2002, 1005, 1055, 2042, 1005, 2921, 1999, 1996, 8102, 1005, 1029, 1005, 8299, 102], [101, 2061, 2017, 4364, 2428, 2985, 2048, 3134, 6815, 8398, 2001, 2006, 5587, 21673, 2140, 1517, 2069, 2000, 2031, 3533, 7226, 2368, 10214, 2000, 2202, 1037, 4319, 3231, 3805, 1997, 1996, 14379, 1010, 1998, 2085, 2017, 1521, 2128, 4333, 1029, 13350, 2071, 2025, 2022, 2062, 2440, 1997, 2009, 1012, 1030, 5310, 2097, 2022, 4415, 2022, 8209, 2039, 3892, 1012, 102], [101, 1030, 5310, 2017, 2097, 2131, 9436, 1997, 2151, 3382, 1037, 2551, 2137, 2038, 1997, 2437, 2009, 2502, 1010, 2096, 2017, 1998, 2115, 7272, 2814, 2444, 2009, 2039, 1999, 1996, 2317, 2160, 1012, 2057, 2156, 2083, 2115, 18667, 1010, 1998, 2017, 2180, 1521, 1056, 2022, 3045, 1999, 2281, 1012, 1001, 8398, 11387, 11387, 1001, 3328, 9497, 1001, 8398, 23654, 102], [101, 1030, 5310, 1030, 5310, 5262, 1012, 2438, 2003, 2438, 1012, 2054, 6793, 1997, 1996, 8398, 3447, 3561, 2000, 5382, 2008, 8398, 2003, 1996, 9947, 2008, 2003, 16023, 2023, 2406, 1012, 1001, 3789, 16558, 23361, 8820, 3726, 14074, 14735, 1001, 3789, 17062, 2368, 8167, 15061, 8913, 26379, 24158, 15864, 24376, 102], [101, 7564, 1997, 5210, 12015, 2012, 1996, 3298, 1011, 1999, 10523, 2648, 1001, 1040, 12273, 11387, 11387, 3892, 2004, 1001, 7226, 2368, 8847, 8299, 102], [101, 1030, 5310, 1030, 5310, 2339, 2024, 2017, 3666, 2694, 1029, 5807, 1005, 1056, 2017, 2022, 2551, 2006, 4337, 2075, 1996, 1001, 8398, 23350, 1998, 2845, 14974, 1029, 1001, 8398, 23350, 11266, 14083, 18981, 5369, 1001, 17328, 24456, 2361, 1001, 22072, 28745, 7361, 1001, 15653, 24456, 2361, 1001, 14033, 22930, 2818, 1001, 3789, 16558, 23361, 8820, 3726, 14074, 14735, 102], [101, 1001, 100, 3422, 2054, 3047, 2012, 1014, 1024, 2459, 1999, 1030, 5310, 1005, 1055, 3743, 1024, 4729, 23521, 2050, 1523, 2304, 28283, 24413, 14303, 16341, 1524, 1001, 21911, 23278, 18752, 2015, 1001, 3533, 17062, 2368, 8299, 102], [101, 4931, 4407, 2265, 2068, 2054, 2785, 1997, 2919, 2477, 4148, 2000, 2023, 4485, 21101, 1030, 5310, 1001, 15653, 24456, 2361, 102], [101, 2008, 3355, 1030, 5310, 2003, 2028, 8295, 1001, 1038, 13728, 10129, 10303, 1012, 1998, 1030, 5310, 3475, 1521, 1056, 3625, 2005, 2014, 14472, 19273, 7179, 1025, 2059, 2053, 2028, 2842, 2003, 2593, 1010, 2157, 1029, 2092, 2008, 1521, 1055, 2129, 9945, 2573, 1012, 1012, 1012, 100, 1001, 8398, 11387, 11387, 8299, 102], [101, 3533, 7226, 2368, 1523, 2038, 2000, 2191, 1037, 3893, 2553, 1998, 2556, 1037, 7046, 4432, 2084, 1520, 1045, 2572, 2025, 8398, 1010, 2292, 1521, 1055, 2175, 2067, 2000, 3671, 1010, 1521, 1524, 2280, 4883, 4018, 1030, 5310, 2056, 1999, 2258, 1012, 8675, 2097, 2022, 4092, 2012, 1996, 1001, 1040, 12273, 3892, 1012, 8299, 102], [101, 13857, 8112, 1024, 1000, 2023, 2343, 1998, 2216, 1999, 2373, 1517, 2216, 2040, 5770, 2013, 4363, 2477, 1996, 2126, 2027, 2024, 1517, 2027, 2024, 10320, 2006, 2115, 22330, 8713, 2964, 1012, 1000, 1000, 2027, 1521, 2128, 5327, 2000, 1529, 8054, 2017, 2008, 2115, 3789, 2515, 2025, 3043, 1012, 2008, 2003, 2129, 2027, 2663, 1012, 1000, 1001, 17183, 8663, 15338, 3258, 8299, 102], [101, 2004, 10231, 7685, 2004, 12609, 2038, 2042, 1010, 2012, 2560, 18520, 2145, 3475, 1521, 1056, 2343, 1012, 1001, 10556, 2290, 11387, 11387, 1001, 10556, 2290, 1001, 23848, 2050, 102], [101, 1000, 1996, 2925, 1997, 2256, 7072, 2003, 2012, 8406, 1012, 1996, 2925, 1997, 2256, 4610, 2003, 2012, 8406, 1012, 1996, 2925, 1997, 2256, 4774, 2003, 2012, 8406, 1012, 1000, 1001, 15941, 8791, 13375, 1001, 7226, 2368, 8167, 6935, 11387, 11387, 1001, 1040, 12273, 1001, 2602, 11387, 11387, 8299, 102], [101, 1042, 13626, 2941, 2359, 2000, 17607, 1037, 4555, 3318, 1012, 2009, 2001, 1037, 2204, 2801, 1012, 1001, 17183, 3207, 20179, 8299, 102], [101, 1030, 5310, 1030, 5310, 6221, 8398, 1005, 1055, 8220, 2005, 16360, 15859, 26642, 1024, 2053, 8465, 2005, 4688, 1012, 2053, 8465, 2005, 5223, 1012, 2053, 8465, 2005, 10047, 22049, 3012, 1012, 2053, 8465, 2005, 6997, 1012, 2053, 8465, 2005, 8902, 24117, 1012, 2053, 8465, 2005, 17727, 5243, 22729, 1012, 2053, 8465, 2005, 4297, 25377, 12870, 5897, 1012, 6517, 1012, 1001, 3789, 16558, 24997, 9626, 12079, 2860, 6806, 102], [101, 3928, 4613, 999, 999, 3537, 2120, 4680, 1024, 9393, 8112, 18058, 2014, 4769, 1001, 17183, 8663, 15338, 3258, 8299, 102], [101, 2009, 1005, 1055, 2051, 2000, 2316, 2362, 2000, 2265, 2256, 3997, 1001, 9507, 2545, 1012, 1001, 3582, 5963, 19362, 3723, 1012, 3582, 2035, 2040, 1024, 2066, 100, 2128, 2102, 28394, 2102, 100, 7615, 100, 29525, 2004, 2017, 2175, 1012, 2031, 4569, 1012, 1001, 9507, 1001, 1042, 19892, 19362, 3723, 1001, 2149, 4523, 1001, 4929, 8067, 6711, 1001, 3789, 1001, 7226, 2368, 8167, 6935, 1001, 15653, 24456, 2361, 1001, 2630, 16535, 11387, 11387, 102], [101, 1030, 5310, 27791, 2000, 8130, 1037, 2450, 2004, 21210, 1001, 3537, 3207, 20179, 102], [101, 1030, 5310, 1030, 5310, 13718, 1010, 2115, 3564, 2041, 1996, 2602, 7126, 2008, 4589, 5469, 2265, 1012, 3531, 1010, 3531, 7949, 2008, 10520, 2000, 4352, 4426, 2000, 3789, 7226, 2368, 1012, 102], [101, 1030, 5310, 2001, 2009, 1996, 10527, 21752, 4682, 1996, 16634, 3333, 2006, 2149, 2008, 2001, 14044, 1029, 1001, 29300, 2278, 11387, 11387, 102], [101, 2362, 2057, 2097, 3786, 6221, 8398, 1516, 1001, 3533, 17062, 2368, 2005, 2343, 1024, 2880, 3049, 4037, 1523, 21911, 2050, 1521, 1055, 2388, 2409, 2014, 3652, 2039, 1523, 2123, 1521, 1056, 4133, 2105, 1998, 17612, 2055, 2477, 1010, 2079, 2242, 1010, 1524, 2029, 2003, 2054, 9297, 1001, 21911, 2050, 2296, 2309, 2154, 1012, 1524, 1001, 3789, 1001, 21357, 8299, 102], [101, 1030, 5310, 1030, 5310, 1030, 5310, 1030, 5310, 2115, 5448, 2003, 2806, 2058, 9415, 1012, 8398, 2018, 3488, 2005, 5717, 1012, 102], [101, 1030, 5310, 3038, 8398, 2003, 16939, 2003, 26316, 6195, 3533, 2003, 2941, 1037, 16939, 2007, 8832, 16939, 7928, 1012, 1001, 3533, 17062, 2368, 1001, 8962, 3489, 26328, 28522, 3600, 1001, 8398, 11387, 11387, 8653, 24198, 7903, 7062, 102], [101, 1030, 5310, 3071, 2003, 4363, 2068, 1999, 2037, 8102, 1999, 3932, 1997, 1001, 5342, 27698, 5178, 2078, 1012, 1001, 8398, 11387, 11387, 102], [101, 1030, 5310, 2026, 2769, 2003, 2006, 4477, 7226, 2368, 102], [101, 1045, 2066, 1996, 2308, 2006, 2754, 1001, 17183, 3207, 20179, 1001, 3537, 3207, 20179, 102], [101, 1045, 1005, 1049, 5241, 1996, 1001, 2308, 29278, 17062, 2368, 3422, 2283, 2005, 1996, 1001, 17183, 8663, 15338, 3258, 2007, 1030, 5310, 1010, 1030, 5310, 1010, 1998, 2048, 5041, 2015, 3331, 4331, 6368, 1030, 5310, 1004, 23713, 1025, 1030, 5310, 999, 100, 102], [101, 2017, 2064, 2131, 1005, 1055, 2663, 1037, 5592, 4003, 999, 999, 999, 2074, 11562, 1996, 4957, 4330, 5004, 1012, 1012, 1012, 8299, 1001, 17183, 3207, 20179, 8299, 102], [101, 1996, 1001, 1040, 12273, 10549, 3533, 7226, 2368, 2004, 1037, 4883, 9773, 2003, 2074, 2004, 16801, 2004, 3533, 7226, 2368, 10549, 1037, 4022, 21210, 2241, 2006, 3096, 3609, 1012, 2054, 2024, 1996, 8037, 2039, 2000, 1029, 102], [101, 2129, 1999, 1996, 3109, 2106, 2027, 3499, 1037, 7226, 2368, 2005, 2343, 7308, 2369, 3782, 7825, 999, 102], [101, 2065, 7637, 2063, 8547, 5811, 2987, 1521, 1056, 3288, 2014, 3566, 2006, 4950, 1010, 1045, 1521, 1049, 2183, 2000, 2022, 9364, 1012, 1001, 17183, 8663, 15338, 3258, 8299, 102], [101, 4365, 7226, 2368, 7459, 2008, 3663, 2282, 1001, 17183, 3207, 20179, 102], [101, 7226, 2368, 2074, 28960, 2054, 2010, 2270, 4092, 2873, 2409, 2032, 2000, 2079, 4953, 1996, 4477, 7226, 2368, 9751, 1010, 2006, 2444, 2694, 1024, 1523, 2196, 17612, 1010, 2196, 4863, 1524, 1001, 17183, 3207, 20179, 102], [101, 1030, 5310, 1030, 5310, 1045, 1521, 1049, 6830, 1001, 7226, 2368, 11387, 11387, 2053, 3043, 2054, 1012, 2021, 1010, 2065, 1045, 2089, 1010, 3383, 2720, 1012, 1051, 1521, 18016, 2071, 2128, 26560, 3366, 2007, 2720, 1012, 7226, 2368, 1037, 2978, 2077, 1996, 14379, 1998, 5896, 1037, 2261, 25591, 12845, 2015, 1012, 2004, 1996, 3166, 1997, 1523, 1996, 5981, 1524, 2006, 1001, 1996, 19650, 9328, 2053, 2028, 2003, 2062, 4591, 1012, 1001, 2292, 17062, 2368, 4783, 17062, 2368, 102], [101, 1030, 5310, 2635, 2041, 3505, 22950, 2066, 2016, 2106, 2198, 22101, 1012, 1012, 1012, 1001, 17183, 3207, 20179, 8299, 102], [101, 2054, 1037, 13044, 3664, 1012, 2061, 5171, 1010, 7499, 8398, 1012, 1001, 8398, 11387, 11387, 1001, 3190, 1001, 5843, 7698, 1001, 2522, 17258, 16147, 102], [101, 1001, 17183, 8663, 15338, 3258, 16215, 2099, 2034, 3203, 9393, 8112, 2003, 4092, 1012, 1045, 3335, 2014, 1012, 102], [101, 1030, 5310, 2123, 1005, 1056, 2202, 2256, 3099, 100, 2021, 3533, 2071, 2031, 1037, 9004, 2600, 2004, 2010, 21210, 1998, 2002, 1005, 1040, 2145, 2131, 2026, 3789, 1012, 1001, 3789, 16558, 23361, 8820, 3726, 14074, 14735, 102], [101, 1030, 5310, 8398, 3488, 1029, 1045, 1521, 1049, 2469, 7226, 2368, 1998, 2010, 3095, 2031, 2589, 2062, 2084, 2048, 2847, 1997, 17463, 2005, 8398, 1521, 1055, 3488, 1012, 102], [101, 21210, 1030, 5310, 3531, 2831, 2055, 2129, 2017, 2097, 4047, 2591, 3036, 2076, 1001, 1040, 12273, 11387, 11387, 1012, 2023, 2003, 1037, 2327, 3277, 2005, 7206, 2753, 1009, 1012, 2591, 3036, 2003, 1037, 2524, 1011, 3687, 5770, 1998, 1037, 4872, 2008, 2442, 2022, 2921, 1012, 1001, 4047, 22994, 2545, 12376, 24759, 2271, 4067, 2017, 1012, 102], [101, 2437, 13350, 5390, 2153, 1056, 1011, 3797, 1001, 3951, 23115, 3258, 1001, 8398, 26760, 8490, 1001, 8398, 1001, 8398, 11387, 11387, 1001, 2175, 2361, 1001, 8398, 23654, 1001, 23848, 2050, 1001, 2191, 14074, 14735, 17603, 6790, 23805, 2078, 1001, 2175, 2361, 1001, 10556, 2290, 11387, 11387, 1001, 3951, 1001, 4603, 2229, 1001, 2637, 8873, 12096, 8299, 102], [101, 1030, 5310, 4931, 9152, 2102, 9148, 2102, 1010, 2017, 2812, 2054, 2017, 1998, 1996, 8560, 3238, 2128, 2705, 15916, 19341, 3619, 2031, 2042, 2725, 1010, 18168, 2290, 2017, 2064, 1521, 1056, 2022, 3809, 1010, 2064, 1521, 1056, 8081, 5236, 1012, 1001, 4536, 2078, 24415, 17062, 2368, 11387, 11387, 1001, 3789, 16558, 23361, 8820, 3726, 14074, 14735, 1001, 3789, 16558, 5657, 11387, 11387, 1001, 3789, 10359, 29337, 12190, 29323, 3207, 11837, 25112, 4183, 1001, 3789, 16558, 23361, 8820, 7178, 6633, 10085, 22648, 2100, 1001, 3789, 16558, 23361, 8913, 26379, 24158, 15864, 24376, 102], [101, 1030, 5310, 2065, 2017, 2066, 8398, 2108, 2343, 1010, 2175, 2157, 3805, 1998, 4339, 1999, 10722, 4877, 2072, 1012, 1045, 2097, 2079, 2054, 2026, 2406, 3791, 1012, 1045, 2097, 8688, 2026, 12157, 1998, 3789, 2005, 2619, 2040, 2064, 2203, 2023, 1012, 2025, 6031, 1010, 2025, 10722, 4877, 2072, 1010, 1001, 3533, 17062, 2368, 1012, 3531, 3693, 2033, 1012, 102], [101, 1030, 5310, 7226, 2368, 2097, 3046, 2000, 21910, 2005, 2469, 102], [101, 1030, 5310, 1030, 5310, 2053, 1012, 1045, 2097, 2025, 3233, 8909, 2135, 2011, 999, 1045, 1005, 1049, 6830, 2005, 1030, 5310, 2000, 5621, 2191, 2637, 2307, 2011, 7065, 2545, 2075, 1996, 4053, 2589, 2011, 1996, 2235, 1052, 2343, 1004, 23713, 1025, 2010, 25353, 3597, 21890, 7666, 1001, 23848, 2050, 1001, 3789, 16558, 5657, 7698, 7384, 4140, 102], [101, 1030, 5310, 1030, 5310, 13244, 8554, 8684, 2003, 1996, 2925, 1012, 1012, 3531, 2562, 2014, 4142, 1999, 2023, 4750, 2088, 1012, 1012, 2293, 2013, 2710, 1012, 1012, 1001, 23848, 2050, 1001, 8398, 11387, 11387, 13122, 10696, 14074, 14735, 102], [101, 1030, 5310, 3531, 1001, 3582, 5963, 2035, 2040, 19387, 2030, 14686, 19387, 2023, 2460, 1006, 1015, 1024, 4029, 1007, 1010, 5132, 2100, 4013, 1011, 7226, 2368, 4487, 15353, 1012, 1001, 3789, 16558, 5657, 1001, 3789, 17062, 2368, 8167, 6935, 1011, 1523, 21911, 2050, 1998, 3533, 1524, 1011, 3531, 11562, 1523, 16784, 2039, 1524, 2006, 7858, 1012, 1001, 7226, 18595, 3736, 10105, 3678, 1001, 1042, 19892, 2036, 1010, 2514, 2489, 2000, 9231, 2023, 1024, 8299, 102], [101, 1045, 2293, 3773, 6437, 5207, 1010, 1996, 2034, 2304, 2450, 1006, 1998, 2034, 11690, 100, 1007, 2000, 2507, 1037, 25569, 4613, 2012, 1996, 1040, 12273, 1012, 1001, 17183, 8663, 15338, 3258, 1001, 3537, 8663, 15338, 3258, 102], [101, 1001, 5292, 3270, 3270, 3270, 3270, 1000, 3220, 10556, 25810, 14163, 28745, 22401, 2015, 2758, 1037, 3789, 2005, 2343, 8398, 1999, 2281, 2003, 1005, 2019, 2552, 1997, 4808, 1005, 1000, 8299, 102], [101, 3533, 7226, 2368, 1521, 1055, 1037, 25312, 23177, 4682, 2099, 999, 2142, 2057, 3233, 1012, 2007, 7226, 2368, 2057, 2991, 1012, 3789, 1001, 8398, 11387, 11387, 1012, 1001, 23848, 2050, 11387, 11387, 1001, 2053, 2135, 2378, 17062, 2368, 1012, 1001, 2053, 9006, 23041, 2923, 10383, 1001, 16839, 10085, 4818, 2923, 10383, 1001, 25312, 23177, 3533, 7226, 2368, 1521, 1055, 1037, 25312, 23177, 4682, 2099, 999, 8299, 102], [101, 8909, 2278, 1010, 2065, 1061, 1521, 2035, 8398, 3111, 1054, 2135, 2228, 1030, 5310, 2106, 2061, 2204, 10047, 3374, 2021, 2017, 2024, 3109, 2050, 3308, 100, 1001, 14379, 11387, 11387, 1001, 3533, 17062, 2368, 1001, 6221, 24456, 2361, 1001, 3789, 102], [101, 1030, 5310, 17927, 2017, 1005, 2310, 2042, 11703, 7416, 7178, 2003, 1996, 2034, 3357, 1012, 2085, 7939, 23709, 3401, 1996, 6270, 5618, 2017, 1005, 2310, 2042, 4352, 1998, 13951, 1999, 4288, 4841, 1012, 1001, 3789, 16558, 5657, 102], [101, 1030, 5310, 1030, 5310, 1030, 5310, 2144, 1061, 1005, 2035, 2024, 3666, 1045, 1005, 1049, 6069, 2074, 2083, 2023, 2041, 2045, 1012, 1001, 3348, 6198, 2483, 6198, 1001, 11703, 20026, 19779, 1001, 17183, 3207, 20179, 102], [101, 1996, 2171, 1997, 1996, 2208, 3892, 2003, 1001, 22950, 19442, 7698, 1001, 17183, 3207, 20179, 102], [101, 1030, 5310, 2002, 2071, 2907, 1037, 3282, 2000, 2026, 2132, 1998, 2360, 2002, 2052, 4139, 1996, 9495, 2065, 1045, 2134, 1005, 1056, 3789, 2005, 8398, 1998, 1045, 2052, 2074, 2298, 2012, 2032, 1998, 2360, 1010, 1000, 1001, 7226, 2368, 11387, 11387, 1000, 102], [101, 4931, 1030, 5310, 2204, 3046, 1010, 2021, 3331, 2000, 1030, 5310, 2074, 2987, 1521, 1056, 2175, 5973, 1012, 2002, 2180, 1521, 1056, 2412, 2507, 1037, 3722, 3437, 999, 2002, 2074, 4122, 2000, 2022, 8398, 2015, 2677, 11198, 1998, 9530, 10258, 3686, 1996, 3314, 999, 999, 1001, 3789, 16558, 23361, 8820, 3726, 14074, 14735, 102], [101, 1001, 3533, 17062, 2368, 2097, 2022, 2343, 2007, 2256, 2393, 1024, 2880, 3049, 4037, 1064, 6951, 1030, 5310, 3696, 2039, 2651, 8299, 102], [101, 1030, 5310, 2899, 2110, 1012, 2035, 5653, 1999, 1012, 2009, 2003, 1996, 2190, 2126, 2000, 3789, 10047, 6806, 1012, 1001, 3789, 16558, 23361, 8820, 3726, 14074, 14735, 102], [101, 1030, 5310, 1030, 5310, 5292, 3270, 3900, 3900, 3900, 3900, 3501, 3270, 3270, 3270, 1012, 1012, 1012, 2672, 2077, 1996, 2203, 1997, 1996, 2095, 1010, 1998, 2017, 2031, 2000, 4604, 2061, 7352, 2769, 2000, 1996, 28781, 2040, 3582, 2032, 1012, 2002, 2003, 1037, 2200, 2659, 2943, 2711, 1012, 2471, 2004, 5875, 2004, 3666, 6773, 4318, 1012, 1001, 8398, 11387, 11387, 1001, 3328, 9497, 1001, 3328, 9497, 19699, 5358, 3207, 5302, 23423, 29278, 22507, 102], [101, 2043, 8398, 7365, 2369, 7226, 2368, 2000, 20014, 27605, 13701, 2032, 1045, 2228, 3533, 2323, 2360, 1000, 3531, 2693, 2067, 2000, 1037, 3647, 3292, 1012, 2017, 2079, 2025, 2031, 1037, 7308, 2006, 1012, 1000, 2002, 3791, 2000, 2219, 2023, 5981, 1012, 1001, 7226, 2368, 9289, 2213, 102], [101, 2009, 2003, 2590, 2005, 8037, 2000, 2022, 2142, 2006, 1996, 2755, 2008, 2296, 2450, 1998, 2155, 2442, 5454, 2005, 3209, 2054, 2003, 2157, 2005, 2068, 1012, 2009, 2003, 2025, 1996, 3105, 1997, 3519, 2030, 2151, 2060, 4884, 2303, 2000, 5646, 1037, 2155, 1521, 1055, 2925, 2030, 2202, 9804, 2185, 2013, 2308, 1012, 1001, 17183, 3207, 20179, 102], [101, 1523, 2002, 2003, 2025, 2005, 2740, 2005, 3087, 11303, 9871, 1012, 1012, 1012, 2002, 2097, 2202, 2009, 2185, 1012, 1524, 1011, 1030, 5310, 1001, 14379, 11387, 11387, 102], [101, 2122, 21237, 2015, 2031, 2445, 2037, 2111, 2495, 1010, 2966, 2729, 1998, 2833, 1998, 2157, 2085, 2057, 1521, 2128, 8084, 2007, 2216, 3722, 2477, 1012, 4486, 2024, 15275, 15941, 2003, 2157, 1001, 17183, 3207, 20179, 102], [101, 1030, 5310, 1000, 2821, 2026, 1010, 2256, 5394, 2038, 2633, 3706, 2039, 2000, 3828, 1996, 2154, 1012, 1012, 1012, 1012, 1012, 2044, 2034, 4292, 2149, 2039, 2005, 4945, 1012, 1000, 2178, 20150, 1012, 8568, 8398, 1056, 28394, 3215, 1998, 3579, 2006, 3045, 1012, 1001, 3789, 14644, 2135, 1001, 3789, 2378, 27576, 1001, 3789, 8095, 10259, 5685, 15864, 1001, 13292, 2509, 1001, 3789, 16558, 5657, 1001, 3789, 16558, 23361, 8913, 26379, 24158, 15864, 24376, 102], [101, 1030, 5310, 2023, 20571, 2009, 100, 1001, 8398, 11387, 11387, 8299, 102], [101, 2017, 2113, 2129, 2027, 2360, 3087, 2064, 2022, 2343, 1029, 2009, 1005, 1055, 2025, 2995, 1012, 8398, 2699, 2009, 1998, 2002, 2481, 1005, 1056, 2079, 2009, 1012, 2002, 2003, 6719, 4039, 2000, 2079, 1996, 3105, 1012, 2061, 2672, 2057, 2323, 2507, 2009, 2000, 2619, 2842, 1001, 8398, 14268, 10483, 2121, 1001, 7226, 2368, 11387, 11387, 102], [101, 1030, 5310, 3789, 2005, 2375, 1998, 2344, 1999, 2281, 1012, 100, 1001, 8398, 11387, 11387, 1001, 2375, 28574, 26764, 8299, 102], [101, 1030, 5310, 1030, 5310, 1030, 5310, 2288, 2008, 2157, 12610, 999, 2021, 2049, 6595, 3537, 999, 1045, 2052, 2738, 2156, 2498, 2131, 2589, 2084, 3499, 1996, 2128, 2705, 15916, 19341, 3619, 2000, 9040, 2637, 999, 1001, 3789, 16558, 23361, 8820, 3726, 14074, 14735, 102], [101, 6423, 1011, 3434, 2852, 3240, 25608, 2635, 2149, 2083, 2023, 2345, 2305, 1997, 1001, 1040, 12273, 1001, 17183, 8663, 15338, 3258, 102], [101, 3666, 1996, 1001, 2175, 2361, 2377, 4767, 4331, 2007, 1030, 5310, 3065, 1996, 1999, 23606, 2791, 1997, 8801, 1012, 2009, 1521, 1055, 2025, 2055, 7300, 1010, 2009, 1521, 1055, 2055, 4024, 1012, 4767, 4331, 2987, 1521, 1056, 2191, 2111, 2892, 1996, 8842, 1010, 2009, 2074, 4599, 1996, 7311, 2005, 2115, 2219, 2217, 1012, 2014, 2501, 2003, 2438, 2572, 5302, 1012, 3579, 999, 102], [101, 1523, 1996, 2502, 2702, 3034, 5444, 2000, 17542, 2049, 2374, 2161, 2023, 2991, 2058, 5936, 3141, 2000, 1996, 21887, 23350, 6090, 3207, 7712, 1524, 2502, 10790, 1010, 1037, 2307, 2742, 2000, 3582, 999, 1001, 2522, 17258, 16147, 1001, 21887, 23350, 1001, 4929, 8067, 6711, 3582, 1001, 3533, 17062, 2368, 1521, 1055, 2742, 1012, 2002, 1521, 1055, 2025, 1037, 1001, 2522, 17258, 25185, 8299, 102], [101, 1030, 5310, 1030, 5310, 1030, 5310, 1030, 5310, 1030, 5310, 1030, 5310, 1030, 5310, 1030, 5310, 1001, 15653, 24456, 2361, 1001, 3789, 5833, 3995, 2361, 2053, 7046, 16374, 2084, 8398, 999, 2191, 2637, 3647, 2153, 3789, 1030, 5310, 3647, 2003, 2488, 2084, 2025, 2307, 2153, 1001, 3789, 16558, 23361, 8820, 3726, 14074, 14735, 1001, 3789, 17062, 2368, 8167, 6935, 11387, 11387, 3422, 2023, 1004, 14181, 1025, 1004, 14181, 1025, 1004, 14181, 1025, 8299, 102], [101, 1030, 5310, 1030, 5310, 1030, 5310, 1030, 5310, 1030, 5310, 6300, 2050, 8398, 2001, 1996, 2028, 2008, 2409, 3537, 11141, 2000, 14906, 2039, 1996, 9750, 2046, 8329, 5014, 2029, 14729, 2005, 2055, 2431, 1997, 2522, 17258, 6677, 1012, 1012, 1012, 2821, 3524, 2053, 2008, 2001, 1996, 17183, 2015, 3772, 2006, 2037, 2219, 15802, 1012, 2049, 2053, 4687, 1019, 2630, 2163, 4070, 2005, 2058, 2871, 1003, 1997, 2522, 17258, 6677, 1012, 1012, 1012, 102], [101, 1019, 1013, 1019, 1001, 3533, 17062, 2368, 1010, 1001, 21911, 23278, 18752, 2015, 1010, 1998, 1996, 7207, 2015, 2031, 2146, 11041, 2369, 3056, 8037, 5627, 2000, 2404, 4847, 8010, 4331, 2077, 6958, 2043, 2009, 3310, 2000, 1996, 2807, 4126, 3021, 1012, 2174, 1010, 2116, 1999, 1996, 7740, 2304, 13965, 2069, 5444, 2005, 2009, 2041, 1997, 1000, 3571, 1000, 1997, 1000, 4788, 1000, 8236, 1012, 100, 8299, 102], [101, 1030, 5310, 1030, 5310, 7672, 4177, 2024, 17328, 2015, 999, 999, 999, 1001, 3789, 5833, 10760, 7913, 3022, 17175, 2271, 3207, 5302, 23423, 1001, 8398, 11387, 11387, 102], [101, 1030, 5310, 2402, 3951, 2308, 2323, 5136, 7226, 2368, 1010, 2040, 2003, 1037, 8777, 1998, 1037, 15908, 2099, 2040, 2097, 4047, 2037, 2916, 1012, 8398, 2003, 1037, 11443, 2099, 1010, 1037, 28616, 15707, 26942, 1010, 1037, 22418, 6723, 5677, 1010, 1037, 22555, 14117, 26470, 2099, 1010, 1998, 1037, 8275, 3951, 1012, 1001, 15653, 24456, 2361, 1001, 10643, 29278, 17062, 2368, 102], [101, 1030, 5310, 2027, 3477, 2017, 1029, 1001, 8398, 11387, 11387, 8299, 102], [101, 3422, 1996, 1001, 4883, 3207, 20179, 11387, 11387, 2007, 2033, 2444, 3892, 2012, 1021, 1024, 3429, 7610, 20116, 2102, 1012, 2049, 2633, 2182, 1010, 15854, 999, 2040, 1005, 1055, 3201, 1029, 1001, 8398, 11387, 11387, 8299, 102], [101, 1030, 5310, 1045, 1521, 1049, 1037, 9421, 25423, 2860, 2266, 1012, 2017, 1521, 2128, 4637, 2330, 6645, 2029, 1027, 2896, 12678, 1012, 4637, 4491, 2006, 10558, 1012, 2026, 3428, 3233, 2005, 2256, 5210, 1004, 23713, 1025, 2005, 1001, 8398, 11387, 11387, 1012, 9245, 11344, 16717, 21519, 6293, 2545, 2035, 2058, 2256, 4497, 1012, 25423, 2860, 2003, 1037, 29591, 2000, 1996, 3915, 1012, 102], [101, 1030, 5310, 8398, 3530, 2000, 2031, 2010, 5551, 7039, 1012, 1012, 1998, 7226, 2368, 2038, 4188, 1012, 2061, 2521, 1010, 7226, 2368, 2038, 7303, 1037, 3338, 2296, 2382, 8117, 2015, 1010, 2038, 4188, 1037, 4319, 3231, 1998, 2085, 2038, 4188, 2000, 2022, 7039, 2005, 2151, 4816, 5733, 2008, 2089, 4681, 2032, 1999, 2010, 5981, 1058, 8398, 1012, 2017, 2079, 1996, 13847, 8785, 999, 102], [101, 1030, 5310, 1523, 1037, 3842, 2008, 5300, 2049, 14310, 2682, 2049, 6481, 2574, 12386, 2119, 1524, 16551, 1015, 1013, 2322, 1013, 4052, 1012, 2057, 2085, 2031, 1037, 2343, 2007, 2053, 6481, 1010, 2053, 5300, 1010, 2053, 26452, 1012, 2002, 10220, 2000, 3932, 1996, 4552, 1012, 2057, 2442, 3789, 8398, 1004, 23713, 1025, 2035, 2040, 2490, 2032, 2041, 1001, 3789, 16558, 23361, 8820, 3726, 14074, 14735, 102], [101, 1030, 5310, 1030, 5310, 1030, 5310, 10699, 6689, 2030, 2041, 1998, 2041, 4688, 1010, 2030, 2119, 999, 1001, 8398, 11387, 11387, 102], [101, 1030, 5310, 1030, 5310, 2008, 1005, 1055, 2138, 2002, 1005, 1055, 1037, 4167, 27283, 2304, 1001, 7672, 999, 2002, 1005, 1055, 2037, 9413, 13033, 2879, 1012, 3475, 1005, 1056, 2054, 1996, 1001, 8037, 2515, 1010, 1998, 1010, 2224, 2068, 2004, 17395, 2015, 2000, 4608, 2060, 10823, 999, 10823, 2123, 1005, 1056, 5223, 2060, 2111, 1010, 4983, 2027, 2024, 2919, 1010, 2008, 1005, 1055, 2025, 14398, 999, 102], [101, 3533, 7226, 2368, 2071, 4530, 2757, 2006, 1996, 2754, 3892, 1999, 2392, 1997, 1996, 2878, 2088, 1998, 1045, 2052, 2145, 3789, 2005, 2032, 2138, 1001, 6616, 24456, 2361, 102], [101, 2017, 2657, 2009, 2034, 2013, 1996, 1001, 17183, 3207, 20179, 1010, 1030, 5310, 2001, 1523, 2046, 1524, 1030, 5310, 2077, 2009, 2001, 4658, 1012, 2134, 1521, 1056, 2113, 15941, 7671, 2008, 2126, 1012, 102], [101, 2061, 4166, 2098, 2000, 2131, 2000, 3113, 2925, 1057, 1012, 1055, 1012, 16360, 1030, 5310, 1004, 23713, 1025, 1057, 1012, 1055, 1012, 12411, 1012, 1030, 5310, 2012, 2340, 1024, 2382, 2651, 2012, 2962, 6374, 2380, 1999, 19243, 4674, 3077, 1012, 1001, 12609, 12260, 7542, 102], [101, 1030, 5310, 1030, 5310, 2065, 2122, 2024, 1996, 9804, 1010, 1045, 4060, 21911, 2050, 1012, 1045, 1005, 1049, 7704, 2000, 1030, 5310, 2138, 1045, 1005, 1049, 1001, 3516, 1012, 2021, 2428, 1045, 2074, 2215, 9444, 7126, 2149, 2663, 1012, 1001, 3789, 5833, 10760, 3995, 2361, 1001, 3789, 16558, 23361, 8820, 3726, 14074, 14735, 102], [101, 3666, 1996, 29300, 2278, 1001, 29300, 2278, 11387, 11387, 102], [101, 1030, 5310, 1030, 5310, 1998, 2066, 2017, 1010, 2027, 2024, 2035, 5490, 27020, 2232, 1001, 2175, 2361, 29268, 2015, 2040, 2097, 2022, 1999, 4000, 2006, 2602, 2305, 1012, 102], [101, 1996, 4507, 22552, 2088, 1997, 1996, 29300, 2278, 2003, 3432, 14726, 1012, 2073, 2003, 2023, 2637, 2027, 2024, 3331, 2055, 1010, 1998, 2040, 2003, 2877, 2009, 1029, 1001, 29300, 2278, 11387, 11387, 102], [101, 1045, 2387, 1037, 1001, 23848, 2050, 2651, 2007, 1010, 1000, 2053, 15806, 999, 1000, 12509, 4993, 2006, 1996, 5725, 5867, 1997, 2010, 4744, 1012, 2017, 3191, 2008, 11178, 1012, 2002, 3158, 9305, 3550, 2010, 2219, 4744, 2000, 2191, 2008, 2022, 2124, 1012, 2023, 2003, 1996, 1030, 5310, 1001, 23848, 2050, 11387, 11387, 2929, 1999, 1037, 12264, 18223, 1012, 3531, 2393, 2149, 1012, 1001, 3789, 17062, 2368, 11387, 11387, 102], [101, 1030, 5310, 1030, 5310, 22905, 6732, 2016, 1005, 1055, 8550, 11071, 2011, 2478, 4003, 2072, 2000, 2562, 2841, 7882, 1012, 2035, 2016, 1005, 1055, 2428, 2725, 1999, 4852, 4003, 2072, 1005, 1055, 8771, 1012, 1045, 2347, 1005, 1056, 2206, 2077, 1010, 2021, 1045, 2572, 2085, 1012, 2562, 2039, 1996, 2204, 2147, 1030, 5310, 1010, 2057, 2288, 20760, 1012, 100, 1001, 3789, 16558, 5657, 102], [101, 1045, 2064, 1005, 1056, 3524, 2005, 1996, 2034, 1001, 5981, 1010, 2043, 1030, 5310, 1006, 2040, 1996, 1030, 5310, 2038, 6845, 3709, 1000, 17056, 3533, 1000, 2030, 1000, 28767, 3533, 1000, 1007, 14352, 2078, 20735, 1996, 2969, 10116, 1000, 6540, 11067, 1000, 1030, 5310, 1012, 2037, 2171, 4214, 2003, 6069, 2067, 10273, 1001, 8164, 9365, 5051, 1001, 7226, 2368, 11387, 11387, 102], [101, 1030, 5310, 2562, 2039, 1996, 2307, 2147, 999, 1001, 3789, 16558, 23361, 8820, 3726, 14074, 14735, 1001, 7226, 2368, 8167, 6935, 11387, 11387, 1001, 11238, 24138, 8189, 2618, 16558, 5657, 1001, 2630, 10422, 28987, 11387, 11387, 8299, 102], [101, 1030, 5310, 1030, 5310, 2292, 1005, 1055, 2404, 2009, 2035, 2006, 1996, 2795, 1012, 8398, 1005, 1055, 5651, 1012, 3533, 7226, 2368, 1005, 1055, 1012, 4477, 7226, 2368, 1005, 1055, 1012, 1998, 2216, 1997, 2010, 2567, 1998, 5727, 2004, 2092, 1011, 2035, 1997, 3183, 3711, 2000, 2031, 19727, 2013, 4822, 3533, 2218, 1012, 102], [101, 1996, 2069, 2518, 2008, 15508, 2033, 2055, 2023, 2878, 2518, 2003, 2008, 8398, 2003, 4760, 2068, 1996, 2995, 2373, 1997, 1996, 8798, 1011, 2057, 2064, 2196, 1010, 2412, 3499, 2019, 8112, 2030, 1037, 5747, 2030, 1037, 7207, 2828, 1999, 1996, 2160, 2153, 1012, 1001, 23848, 2050, 102], [101, 21911, 23278, 18752, 2015, 4227, 1023, 1010, 6079, 2575, 10474, 8771, 1999, 1996, 2197, 1020, 2847, 1010, 2005, 1037, 1014, 1012, 7449, 1003, 3623, 1010, 2007, 1037, 2783, 4175, 1997, 1019, 1010, 12963, 1010, 4185, 2575, 8771, 1012, 1001, 12609, 12260, 7542, 102], [101, 1030, 5310, 1030, 5310, 1012, 1001, 17727, 5243, 7690, 24456, 2361, 1030, 5310, 3849, 2000, 2191, 12667, 2903, 2027, 1521, 2128, 11311, 2000, 2151, 3423, 1013, 7191, 25928, 1012, 2064, 1521, 1056, 3524, 1019, 8398, 2000, 2022, 2041, 1997, 2436, 1004, 23713, 1025, 2002, 4152, 1996, 7750, 2002, 17210, 999, 1001, 3789, 16558, 23361, 8820, 3726, 14074, 14735, 102], [101, 1012, 1030, 5310, 2003, 24114, 1996, 4808, 1999, 17183, 2448, 3655, 2006, 8398, 5782, 2375, 1004, 23713, 1025, 2344, 1057, 3685, 2191, 2023, 2039, 17183, 2015, 2024, 1996, 2047, 3958, 3557, 1012, 2027, 2228, 2037, 8771, 2097, 2903, 1996, 3658, 2027, 11867, 7974, 1004, 23713, 1025, 4392, 2037, 12849, 4747, 4681, 4841, 2024, 8300, 999, 17183, 2015, 2031, 2668, 2006, 2037, 2398, 999, 1001, 100, 102], [101, 2065, 2017, 2064, 6186, 2017, 2064, 3789, 1999, 2711, 2017, 2175, 2833, 6023, 2017, 2064, 3789, 1999, 2711, 2017, 2064, 2175, 2000, 2377, 15213, 2017, 2064, 3789, 1999, 2711, 2017, 2064, 2175, 2000, 1996, 3509, 2017, 2064, 2175, 3789, 1999, 2711, 2017, 2064, 2147, 2017, 2064, 3789, 1999, 2711, 1001, 8398, 11387, 11387, 102], [101, 1030, 5310, 2156, 1001, 23409, 6793, 1010, 1996, 1001, 2175, 2361, 2003, 2746, 2005, 2115, 4409, 1012, 1000, 1008, 4278, 13780, 8243, 2011, 2012, 2546, 1012, 1000, 4125, 2039, 999, 6985, 1001, 23409, 999, 999, 999, 999, 999, 102], [101, 1030, 5310, 2735, 11514, 1005, 2222, 2022, 2152, 2004, 1996, 17666, 12059, 1012, 2292, 2032, 5823, 1998, 6402, 1012, 2021, 2035, 7226, 2368, 2038, 2000, 2079, 2003, 2298, 6047, 1998, 5681, 2360, 1000, 22017, 999, 1000, 102], [101, 1030, 5310, 1030, 5310, 1030, 5310, 1045, 1521, 1049, 2025, 1037, 2502, 7226, 2368, 5470, 1010, 2021, 1045, 1521, 1049, 2214, 2438, 2000, 3342, 2043, 16233, 1004, 23713, 1025, 12667, 2499, 2362, 1010, 2000, 2830, 1996, 2406, 1012, 1996, 2088, 9768, 2149, 1012, 8398, 2038, 4055, 2149, 9249, 1004, 23713, 1025, 1996, 2088, 2085, 2119, 11839, 2015, 2005, 2149, 1004, 23713, 1025, 2003, 4487, 12693, 6129, 2993, 2013, 2149, 1010, 10317, 1004, 23713, 1025, 6719, 1012, 8398, 2003, 2053, 2204, 1030, 2023, 1012, 8299, 102], [101, 2613, 5280, 19058, 24456, 2361, 1024, 2064, 2017, 2903, 2054, 1521, 1055, 6230, 999, 1029, 2027, 2507, 3533, 5342, 2078, 1521, 1996, 3980, 1010, 1998, 2002, 9631, 2068, 2019, 3437, 999, 8299, 102], [101, 1030, 5310, 1030, 5310, 2021, 1996, 2865, 2758, 2049, 9379, 1012, 1048, 2863, 2080, 1010, 5796, 2213, 2003, 1037, 8257, 1010, 4099, 1997, 1996, 2111, 1012, 1001, 8398, 11387, 11387, 102], [101, 5186, 4069, 2767, 1011, 1997, 1011, 7226, 2368, 6554, 20710, 4801, 11245, 5833, 2045, 1999, 1996, 1999, 24443, 18802, 2678, 1012, 1001, 17183, 8663, 15338, 3258, 102], [101, 1030, 5310, 1045, 2572, 1012, 2057, 3685, 5788, 2178, 1018, 2086, 1997, 1037, 8398, 8798, 1012, 2256, 7072, 2097, 3280, 3294, 1010, 1998, 2057, 2097, 2468, 2019, 8285, 17510, 2406, 1012, 2057, 2442, 3789, 2035, 1996, 9099, 25434, 4126, 16229, 2015, 2041, 2077, 2009, 2003, 2205, 2397, 1012, 1001, 3789, 16558, 23361, 8913, 26379, 24158, 15864, 24376, 1001, 3789, 16558, 23361, 8820, 3726, 14074, 14735, 102], [101, 1030, 5310, 1030, 5310, 5292, 3270, 1011, 2016, 2003, 2525, 4911, 2115, 2192, 1012, 2054, 2097, 2016, 3338, 2006, 2017, 2279, 1029, 1001, 8398, 11387, 11387, 1001, 28767, 5558, 2063, 8299, 102], [101, 2008, 2617, 2043, 2017, 5382, 1030, 5310, 2003, 2941, 2084, 2891, 1012, 1012, 1012, 1012, 1001, 4883, 3207, 20179, 1001, 5981, 11387, 11387, 1001, 12609, 3207, 20179, 1001, 14379, 1001, 7226, 2368, 8167, 6935, 11387, 11387, 1001, 8398, 11387, 11387, 1001, 7226, 2368, 11387, 11387, 8299, 102], [101, 2330, 2023, 1001, 17183, 8663, 15338, 3258, 27263, 2005, 1037, 8242, 4474, 999, 1008, 11307, 2716, 2000, 2017, 2011, 1030, 5310, 10245, 7507, 2102, 1001, 12536, 15185, 2290, 8299, 102], [101, 2023, 4613, 2011, 1001, 3533, 17062, 2368, 2003, 2035, 2008, 1045, 2359, 1012, 3488, 1010, 2054, 2027, 2024, 1010, 2129, 2027, 2097, 2191, 2068, 4148, 1012, 2023, 3475, 1521, 1056, 1996, 4613, 1997, 1037, 24726, 1010, 2021, 1037, 2655, 2000, 2895, 1998, 1037, 4872, 1012, 102], [101, 1001, 14303, 13699, 12083, 19341, 3619, 5833, 11387, 11387, 1001, 3789, 16558, 23361, 8820, 3726, 14074, 14735, 1030, 5310, 1030, 5310, 10643, 1010, 2017, 1005, 2128, 2589, 999, 8299, 102], [101, 1015, 1013, 9467, 2006, 1030, 5310, 4352, 1996, 24949, 3189, 2011, 1030, 5310, 2128, 1001, 14379, 11387, 11387, 4647, 2198, 2056, 2008, 1001, 8398, 5117, 1999, 2171, 4214, 2247, 1059, 1013, 3533, 18411, 1996, 1001, 3606, 1001, 2755, 2003, 2008, 2069, 7226, 2368, 2012, 2102, 2243, 2094, 1999, 2171, 4214, 1010, 7226, 2368, 2001, 2034, 2000, 2012, 2102, 2243, 7714, 1010, 2034, 2000, 2171, 4214, 1006, 8398, 102], [101, 1030, 5310, 1030, 5310, 11504, 2180, 1005, 1056, 2031, 2000, 5674, 2005, 2205, 2146, 1012, 1012, 1012, 1001, 3533, 17062, 2368, 11387, 11387, 1001, 3533, 17062, 2368, 1001, 8398, 23350, 102], [101, 1056, 2140, 2003, 2440, 1997, 1996, 4883, 5981, 1998, 1045, 2113, 6830, 2005, 8398, 2003, 6719, 1996, 5409, 2518, 2000, 2079, 2021, 2064, 2619, 2425, 2033, 1996, 3563, 2015, 1029, 1045, 1521, 2310, 2042, 2667, 2000, 2424, 2041, 1013, 3599, 1013, 2054, 2052, 4148, 2000, 2149, 2065, 2002, 1521, 1055, 2700, 1998, 1045, 1521, 1049, 15958, 5191, 1998, 5457, 1024, 1006, 102], [101, 1030, 5310, 1523, 1996, 2088, 1521, 1055, 2087, 4795, 2158, 1524, 1012, 2984, 8398, 1012, 2016, 26304, 2009, 1012, 1523, 2438, 2003, 2196, 2438, 1524, 1001, 15653, 24456, 2361, 1001, 8398, 14268, 29201, 8844, 1001, 7226, 2368, 11387, 11387, 1001, 23848, 2050, 2038, 2042, 25857, 102], [101, 2154, 1015, 1997, 1996, 1001, 17183, 8663, 15338, 3258, 2003, 2028, 1996, 2808, 1012, 2054, 2064, 2057, 5987, 2651, 1012, 1012, 1012, 2062, 3658, 1012, 1001, 10556, 2290, 11387, 11387, 102], [101, 1030, 5310, 7226, 2368, 2003, 20342, 2065, 2002, 2064, 1005, 1056, 5047, 1037, 13847, 3938, 3371, 5981, 1012, 1012, 1012, 2017, 5698, 8609, 2015, 1045, 8415, 1010, 2002, 2003, 2061, 18358, 2039, 1012, 102], [101, 1001, 8112, 10930, 1010, 1010, 1010, 1010, 2017, 2020, 2343, 2005, 1022, 2086, 2339, 2134, 1521, 1056, 2017, 8081, 2035, 2023, 1010, 2009, 2022, 2066, 2115, 2343, 2594, 2100, 2196, 3047, 1001, 21911, 23278, 18752, 2015, 1001, 21911, 23278, 18752, 22747, 2953, 2615, 2361, 1001, 8112, 17311, 20915, 14621, 2618, 27900, 20744, 1001, 1040, 12273, 8663, 15338, 3258, 1001, 12609, 12260, 7542, 102], [101, 1030, 5310, 1030, 5310, 2672, 2016, 5807, 1521, 1056, 1997, 13322, 2039, 2007, 1037, 16216, 4360, 12412, 2000, 2131, 2014, 2034, 3105, 1001, 8398, 11387, 11387, 8299, 102], [101, 2012, 1996, 8398, 2724, 8320, 2651, 1999, 2474, 1001, 8398, 11387, 11387, 2292, 1521, 1055, 2735, 2662, 2417, 999, 8299, 102], [101, 100, 1996, 4018, 1024, 16360, 1012, 6945, 100, 1517, 2783, 1057, 1012, 1055, 1012, 4387, 1010, 2449, 3954, 1010, 1998, 8291, 1997, 1996, 3416, 7450, 1517, 1030, 5310, 2003, 1996, 3951, 9773, 2005, 2167, 3792, 1521, 1055, 6122, 7740, 2212, 1001, 13316, 17134, 1001, 13316, 18155, 1001, 29300, 2278, 11387, 11387, 8299, 102], [101, 2130, 2065, 2017, 2123, 1005, 1056, 2066, 1001, 3533, 17062, 2368, 1010, 3789, 2005, 2032, 4312, 1012, 2065, 2002, 8289, 1999, 2436, 1012, 1012, 1012, 1001, 2343, 8167, 6935, 102], [101, 1000, 2052, 2017, 3844, 2039, 2158, 1029, 1000, 2008, 2741, 2033, 100, 1001, 3533, 17062, 2368, 11387, 11387, 1001, 21911, 23278, 18752, 2015, 2615, 2361, 1001, 4883, 3207, 20179, 11387, 11387, 102], [101, 3505, 7279, 3401, 2056, 1000, 22017, 10244, 2078, 5927, 2137, 4768, 1010, 2021, 1045, 2156, 2137, 2307, 2791, 1000, 10392, 3105, 2005, 1996, 1001, 29300, 2278, 11387, 11387, 1001, 2176, 5974, 29100, 2015, 1001, 3505, 11837, 3401, 1001, 10556, 2290, 11387, 11387, 1001, 2417, 16535, 11387, 11387, 1001, 23848, 2050, 1001, 11579, 19496, 3064, 102], [101, 1030, 5310, 1030, 5310, 2061, 2106, 2017, 15734, 2224, 13425, 1997, 1037, 3345, 1010, 5318, 3121, 1010, 1998, 25007, 7318, 1010, 2030, 2001, 2009, 2074, 1037, 16507, 1029, 2023, 2003, 1037, 2210, 17109, 6195, 1012, 1012, 1012, 2017, 2113, 1010, 8042, 1012, 2323, 2057, 4553, 2845, 2030, 2446, 2000, 7374, 2005, 1996, 15336, 1029, 1001, 14870, 24456, 2361, 1001, 8398, 11387, 11387, 102], [101, 1030, 5310, 1030, 5310, 1030, 5310, 1030, 5310, 2040, 4122, 2000, 3693, 2033, 1999, 24260, 3436, 1002, 2321, 2000, 1996, 7226, 2368, 1013, 5671, 3049, 2651, 1029, 1001, 5417, 3527, 17305, 27439, 22916, 1001, 7226, 2368, 8167, 6935, 11387, 11387, 1001, 7226, 2368, 11387, 11387, 8299, 102], [101, 1030, 5310, 1030, 5310, 1030, 5310, 2429, 1016, 2115, 3579, 2967, 2055, 1015, 1013, 1017, 1997, 8112, 1011, 8398, 7206, 2933, 1016, 3789, 1018, 7226, 2368, 1012, 2065, 2023, 2003, 6149, 7226, 2368, 5222, 1999, 1037, 20148, 1012, 2065, 2023, 1001, 2003, 2941, 2322, 1003, 1010, 7226, 2368, 5222, 1999, 1037, 20148, 1012, 2130, 1019, 1003, 2168, 20148, 1012, 1001, 12609, 12260, 7542, 1001, 7226, 2368, 1001, 8398, 102], [101, 1030, 5310, 2009, 2442, 2031, 2042, 6429, 3773, 2009, 1999, 2711, 1010, 6876, 2066, 2023, 2507, 2033, 13020, 8569, 25370, 999, 100, 1001, 2643, 13510, 21559, 22420, 2050, 1001, 8398, 11387, 11387, 102], [101, 2040, 2323, 2663, 1996, 1001, 3537, 6488, 2005, 1001, 8962, 2271, 1999, 12609, 1029, 1001, 12609, 12260, 7542, 1001, 3915, 1006, 1001, 2128, 2102, 28394, 2102, 2005, 7099, 2946, 1007, 1001, 7226, 2368, 11387, 11387, 1001, 15941, 11387, 11387, 1001, 6031, 11387, 11387, 1001, 6969, 11387, 11387, 102], [101, 2292, 1521, 1055, 2831, 2055, 1523, 10866, 2162, 1524, 8398, 2865, 6325, 2021, 2987, 1521, 1056, 5254, 2002, 1521, 1055, 7510, 9767, 2006, 14717, 1004, 23713, 1025, 4762, 2075, 1996, 2162, 1999, 13968, 2029, 2003, 4288, 2336, 2004, 2057, 3713, 999, 1030, 5310, 4282, 2023, 2021, 2003, 3432, 4688, 2004, 5156, 10866, 2162, 2003, 2200, 2172, 1999, 2440, 3466, 999, 1001, 29300, 2278, 11387, 11387, 102], [101, 1030, 5310, 1030, 5310, 2008, 1521, 1055, 2986, 1010, 2138, 2023, 2097, 2031, 2115, 3611, 2663, 1999, 1037, 20148, 1999, 2281, 1012, 1001, 8398, 11387, 11387, 102], [101, 100, 1024, 2305, 100, 4105, 5609, 100, 2444, 2085, 1024, 2053, 27932, 13867, 1001, 17183, 8663, 15338, 3258, 1001, 7226, 2368, 8167, 6935, 8299, 102], [101, 1030, 5310, 10643, 2667, 2000, 8954, 1996, 2602, 1029, 2053, 1010, 2008, 2003, 2054, 2017, 5698, 8609, 2015, 2024, 2667, 2000, 2079, 1012, 5653, 1011, 1999, 17069, 2024, 3811, 18002, 2000, 9861, 1998, 2017, 2113, 2023, 1012, 2017, 2064, 1005, 1056, 3786, 8398, 2302, 16789, 1010, 1998, 2057, 2113, 2023, 1012, 1001, 8398, 11387, 11387, 102], [101, 1001, 17183, 8663, 15338, 3258, 12654, 1024, 1000, 2317, 2111, 2024, 9643, 1010, 18904, 4221, 1029, 2292, 1005, 1055, 12324, 2438, 1997, 1996, 2717, 1997, 1996, 2088, 2000, 29454, 10421, 2122, 9643, 12461, 2041, 1997, 4598, 999, 1000, 102], [101, 1030, 5310, 2129, 2146, 2127, 2017, 5256, 2039, 1998, 2907, 1030, 5310, 1010, 1030, 5310, 1010, 1998, 1030, 5310, 26771, 1029, 2821, 3524, 2008, 1005, 1055, 2157, 1010, 2009, 1005, 1055, 1030, 5310, 6346, 1012, 1001, 1044, 22571, 10085, 6935, 2100, 1001, 12609, 12260, 7542, 1001, 2637, 8873, 12096, 1001, 8037, 5280, 13535, 12069, 1001, 2406, 7840, 19362, 3723, 102], [101, 1030, 5310, 1030, 5310, 2672, 2027, 2323, 2298, 2046, 2032, 2108, 2006, 26646, 1521, 1055, 4946, 2656, 2335, 1012, 1998, 2106, 2009, 2202, 7207, 2000, 10386, 2479, 1029, 1001, 8398, 11387, 11387, 1001, 5256, 6279, 14074, 14735, 102], [101, 1030, 5310, 2002, 1521, 1055, 2119, 999, 1001, 8398, 11387, 11387, 102], [101, 1030, 5310, 2343, 8398, 999, 2348, 2027, 1521, 2310, 2699, 2066, 3109, 999, 1001, 23848, 2050, 102], [101, 7226, 2368, 2074, 2056, 1000, 3071, 2842, 2106, 2009, 1000, 2000, 1996, 11804, 7450, 1012, 1001, 17183, 3207, 20179, 102], [101, 1030, 5310, 1030, 5310, 1030, 5310, 1030, 5310, 2009, 2515, 2342, 2000, 2022, 1037, 2304, 2450, 1012, 3422, 2054, 2097, 4148, 2065, 2009, 1521, 1055, 2025, 1012, 2123, 1521, 1056, 2202, 1996, 2304, 2450, 3789, 2005, 4379, 1012, 1037, 3308, 4060, 2071, 4315, 12502, 1001, 7226, 2368, 11387, 11387, 17057, 1998, 1996, 1001, 3537, 2283, 2005, 3864, 2000, 2272, 1012, 2017, 2064, 9231, 2023, 1056, 28394, 2102, 1012, 102], [101, 1030, 5310, 2027, 2024, 2667, 2673, 1998, 2059, 2070, 1012, 2057, 3685, 2022, 6247, 2091, 1012, 2057, 2097, 2035, 3789, 999, 1001, 3789, 16558, 23361, 8820, 3726, 14074, 14735, 1001, 3789, 16558, 24997, 9626, 12079, 2860, 6806, 102], [101, 2090, 1001, 6221, 24456, 2361, 9694, 1037, 3622, 3433, 1998, 1996, 1001, 8037, 12549, 23142, 3645, 1013, 5296, 2091, 3121, 1010, 2256, 2406, 2003, 3753, 2005, 7071, 1998, 2942, 2162, 1012, 2065, 2057, 4342, 2000, 4952, 2000, 2169, 2060, 1010, 2035, 1997, 2023, 2071, 2022, 9511, 1012, 102], [101, 1999, 2035, 27994, 1010, 1996, 1001, 4883, 3207, 20179, 2197, 2305, 2001, 6517, 1012, 6586, 1010, 2292, 1521, 1055, 3613, 2000, 2022, 1999, 7083, 2005, 2343, 8398, 1010, 7226, 2368, 1998, 2256, 2406, 1012, 102], [101, 1016, 1013, 2008, 2037, 7179, 2097, 1001, 5256, 6279, 10073, 1004, 23713, 1025, 1001, 3328, 9497, 19699, 5358, 3207, 5302, 23423, 1001, 3328, 9497, 19699, 5358, 3207, 5302, 23423, 29278, 22507, 1001, 3328, 9497, 1001, 3328, 3406, 23301, 9527, 24415, 2890, 14289, 16558, 5555, 3619, 1001, 3328, 3406, 23301, 9527, 24415, 3995, 2361, 1001, 3789, 24456, 2361, 1001, 3789, 27058, 19362, 3723, 1001, 3789, 27058, 19362, 3723, 1001, 10556, 2290, 1001, 23848, 2050, 1001, 10507, 4140, 1001, 29525, 2015, 1001, 2510, 1001, 3008, 1001, 2308, 1001, 3611, 2015, 1001, 3566, 2015, 1001, 4715, 5302, 2213, 1001, 8991, 2100, 1001, 8991, 2480, 102], [101, 1030, 5310, 7226, 2368, 2003, 15174, 8299, 102], [101, 1030, 5310, 100, 1001, 3789, 16558, 23361, 8820, 3726, 14074, 14735, 100, 1045, 2031, 2042, 2206, 1030, 5310, 2005, 1037, 2096, 1012, 2065, 2017, 2030, 1045, 7271, 2056, 1996, 4895, 6342, 5910, 5794, 10711, 3064, 4689, 2004, 7151, 100, 2477, 2002, 2758, 1010, 2057, 2052, 2022, 3844, 2091, 1999, 1037, 12251, 1012, 2027, 2024, 2041, 1004, 23713, 1025, 2041, 3658, 999, 2057, 2342, 1996, 3606, 1004, 23713, 1025, 1037, 17824, 3003, 1012, 1001, 3789, 16558, 5657, 11387, 11387, 102], [101, 1030, 5310, 21877, 10483, 2072, 2074, 2081, 2017, 3297, 1012, 1012, 1001, 8398, 11387, 11387, 102], [101, 1045, 2428, 3335, 1001, 3870, 9028, 7389, 2014, 3754, 2000, 4863, 3471, 1998, 2129, 2000, 9611, 2068, 2020, 2025, 2069, 9414, 2021, 2036, 18988, 1001, 17183, 3207, 20179, 8299, 102], [101, 1030, 5310, 6640, 2420, 1001, 2057, 29602, 2140, 22994, 2063, 1001, 3789, 16558, 23361, 8913, 26379, 24158, 15864, 24376, 1001, 3789, 16558, 5657, 11387, 11387, 1001, 3789, 16558, 23361, 8820, 3726, 14074, 14735, 11387, 11387, 1001, 7226, 2368, 8167, 6935, 11387, 11387, 1001, 7226, 2368, 8167, 6935, 11387, 11387, 8653, 24198, 8299, 102], [101, 1001, 3533, 27698, 5178, 2078, 2734, 5587, 21673, 2140, 2000, 2994, 18920, 1001, 5981, 11387, 11387, 1001, 4319, 2098, 6279, 17062, 2368, 102], [101, 6031, 3957, 24387, 2000, 1047, 4135, 25987, 2906, 2153, 1012, 2003, 2016, 2457, 2075, 2014, 2000, 7496, 2014, 2041, 1997, 1996, 3049, 1029, 1001, 17183, 3207, 20179, 1001, 3537, 3207, 20179, 102], [101, 2085, 2008, 1030, 5310, 2003, 3985, 1996, 3537, 9773, 1010, 2202, 1037, 2298, 2012, 2010, 1001, 7521, 4884, 18402, 1012, 1001, 17183, 8663, 15338, 3258, 1001, 3537, 8663, 15338, 3258, 8299, 102], [101, 1030, 5310, 2061, 6429, 1012, 2002, 2003, 2005, 28632, 5905, 1012, 1045, 2123, 1521, 1056, 2156, 2339, 2057, 2130, 2507, 2032, 2349, 2832, 1012, 2057, 2442, 12475, 1996, 11963, 1998, 7868, 2122, 2111, 2024, 5905, 2013, 1996, 2927, 2241, 2006, 2037, 2576, 12912, 2015, 1012, 2008, 3241, 2003, 2054, 3084, 2149, 2307, 1012, 1001, 7672, 1001, 7226, 2368, 8167, 6935, 11387, 11387, 13122, 10696, 14074, 14735, 102], [101, 1030, 5310, 2115, 2388, 2003, 2019, 4763, 21877, 3527, 1012, 1012, 1012, 25312, 2480, 3393, 27304, 1010, 2008, 19424, 2678, 2097, 2022, 17653, 2739, 2574, 1012, 2036, 2057, 2097, 2196, 5293, 12867, 1998, 3841, 5603, 16103, 1012, 1045, 4687, 2129, 2116, 16795, 2594, 13549, 2017, 1005, 2310, 2042, 2000, 9295, 999, 1996, 6548, 7425, 2121, 7207, 2155, 2024, 2183, 2091, 1012, 1001, 1059, 27767, 2487, 27767, 2050, 102], [101, 2123, 1521, 1056, 5293, 1012, 1030, 5310, 2038, 3271, 2216, 1999, 2342, 2011, 8402, 12163, 6666, 1010, 2025, 8037, 1012, 3342, 2008, 2043, 2017, 2175, 2000, 1996, 14592, 1999, 2281, 1012, 1996, 8037, 2052, 2738, 2377, 13593, 2576, 2399, 2084, 2490, 2172, 2342, 4335, 2005, 1996, 2137, 2111, 999, 1001, 8398, 11387, 11387, 102], [101, 3537, 3078, 6830, 6808, 1024, 2047, 7035, 12055, 1024, 2861, 1003, 1006, 1011, 1015, 1007, 10007, 8004, 28872, 1024, 2538, 1003, 1006, 1009, 1018, 1007, 7226, 2368, 1024, 2260, 1003, 1006, 1011, 1015, 1007, 6031, 1024, 2260, 1003, 1006, 1009, 1015, 1007, 1047, 4135, 25987, 2906, 1024, 2340, 1003, 1006, 1027, 1007, 11721, 22414, 4103, 1024, 1019, 1003, 1006, 1011, 1015, 1007, 8675, 1024, 1018, 1003, 1006, 1011, 1016, 1007, 26261, 10532, 1024, 1015, 1003, 1006, 1011, 1015, 1007, 1016, 1013, 1019, 1011, 1030, 5310, 3431, 2007, 1016, 1013, 1018, 1001, 12609, 12260, 7542, 1001, 2047, 3511, 4523, 20908, 102], [101, 6057, 2129, 15941, 2288, 4457, 2006, 1996, 10184, 3021, 1006, 2748, 1010, 2002, 2106, 2886, 7226, 2368, 2006, 1996, 5712, 2162, 1007, 1010, 2021, 2196, 2288, 1037, 3382, 2000, 3433, 2000, 2009, 1012, 1001, 17183, 3207, 20179, 102], [101, 1001, 6221, 3501, 6824, 2361, 1010, 1030, 5310, 1010, 1001, 23848, 2050, 1005, 10659, 2921, 1005, 1029, 2025, 4498, 1012, 2182, 2024, 2274, 4979, 8654, 16393, 2015, 8398, 3631, 1012, 8299, 102], [101, 3870, 6031, 2038, 2439, 2014, 3382, 2000, 2022, 1996, 2034, 6284, 2343, 1997, 1996, 2142, 2163, 1012, 1001, 17183, 3207, 20179, 102], [101, 8112, 4796, 2003, 2074, 2707, 2893, 4895, 22401, 2989, 999, 9296, 4812, 2074, 21801, 13593, 8495, 5160, 2040, 6904, 4877, 7810, 2592, 2000, 2131, 27424, 2050, 1012, 2085, 1996, 3160, 2003, 2339, 1037, 5160, 4682, 1029, 2006, 6852, 1997, 3183, 1029, 1996, 8495, 2323, 2022, 14984, 1997, 2993, 999, 1001, 9296, 1001, 8112, 3654, 3686, 1001, 3607, 6806, 8528, 1001, 8398, 11387, 11387, 102], [101, 2292, 1996, 2501, 2265, 8299, 1045, 2106, 2025, 3789, 2005, 8398, 999, 1045, 2097, 3789, 2005, 1030, 5310, 1998, 2035, 17183, 2015, 2000, 3828, 2149, 2013, 2023, 10103, 999, 1001, 3789, 16558, 23361, 8820, 3726, 14074, 14735, 102], [101, 4060, 1037, 4001, 2679, 999, 21357, 1012, 2655, 1012, 10683, 1012, 1001, 12609, 12260, 7542, 102], [101, 1030, 5310, 1030, 5310, 3464, 2000, 2022, 2464, 2000, 2054, 6698, 10643, 2097, 4681, 1998, 14863, 2102, 1998, 2292, 2023, 17328, 8398, 10083, 2637, 1998, 5853, 4221, 5762, 27865, 1997, 1996, 2627, 2000, 14033, 2637, 2046, 9535, 7747, 1012, 102], [101, 2079, 2057, 2228, 3870, 6031, 2097, 2022, 7226, 2368, 1005, 1055, 21210, 4060, 1029, 1001, 17183, 3207, 20179, 102], [101, 1030, 5310, 2003, 2002, 2183, 2000, 2079, 1996, 6128, 13987, 23233, 3917, 3328, 2044, 1996, 5981, 1029, 2057, 2097, 2074, 2031, 2000, 3422, 2343, 8398, 1521, 1055, 2227, 2000, 2113, 2043, 1996, 2617, 6433, 2008, 7226, 2368, 12386, 2009, 1998, 1996, 5437, 4269, 2000, 2566, 4168, 3686, 1012, 102], [101, 1030, 5310, 3198, 1001, 21864, 18927, 3217, 28940, 29147, 8913, 2055, 2010, 2125, 5370, 6115, 2007, 2859, 1010, 1001, 20934, 6935, 2863, 1010, 1998, 2500, 4283, 2000, 1001, 3477, 4842, 13068, 5679, 2005, 2032, 1998, 8112, 2083, 1001, 4477, 17062, 2368, 999, 1001, 10454, 17062, 2368, 4593, 2038, 2053, 3314, 2007, 5938, 2122, 17800, 4227, 12256, 3111, 999, 1001, 23848, 2050, 1001, 10556, 2290, 1001, 8398, 11387, 11387, 1001, 3533, 4783, 26100, 2078, 999, 102], [101, 3951, 2120, 4680, 1024, 14381, 8398, 18058, 2014, 4769, 1001, 29300, 2278, 11387, 11387, 8299, 102], [101, 1030, 5310, 2017, 1521, 2128, 7143, 2000, 21910, 999, 1001, 2167, 10010, 18861, 2050, 2022, 6047, 999, 1001, 3789, 16558, 23361, 8820, 3726, 14074, 14735, 2013, 1001, 8398, 999, 1011, 1012, 12609, 2602, 7206, 2031, 2363, 9962, 4402, 10428, 5227, 3596, 1999, 1996, 5653, 2007, 8398, 1005, 1055, 2227, 2006, 2068, 1011, 13229, 18155, 18291, 2015, 8299, 102], [101, 1001, 19387, 1030, 5310, 1024, 2064, 2017, 2425, 1996, 4489, 2090, 1037, 2576, 4680, 2008, 2003, 2019, 11322, 3089, 14116, 2724, 6431, 2028, 2008, 2001, 10468, 2019, 3668, 24095, 2655, 1029, 1001, 29300, 2278, 11387, 11387, 102], [101, 1030, 5310, 1030, 5310, 2017, 2024, 2025, 7966, 2075, 3087, 2021, 4426, 1012, 2017, 2113, 4415, 2054, 1053, 6761, 2078, 2003, 1012, 2045, 1005, 1055, 7564, 1997, 15800, 2090, 2017, 1998, 2068, 1012, 2122, 4127, 1997, 11703, 20175, 3993, 9887, 2003, 2339, 1996, 3484, 1997, 4841, 2024, 3788, 2185, 2013, 10643, 1010, 8398, 1998, 1001, 23848, 2050, 2000, 3533, 7226, 2368, 2040, 2027, 2113, 2000, 3404, 1012, 102], [101, 1030, 5310, 1030, 5310, 1030, 5310, 1030, 5310, 2002, 2987, 1005, 1056, 2507, 1037, 4485, 2055, 1061, 1005, 2035, 1012, 1045, 2228, 1045, 1005, 1049, 2074, 6517, 2055, 2023, 6302, 1012, 1001, 3789, 16558, 23361, 8820, 3726, 14074, 14735, 1001, 3789, 16558, 23361, 8913, 26379, 24158, 15864, 24376, 1001, 7226, 2368, 29278, 28994, 5178, 3372, 11387, 11387, 1001, 15653, 24456, 2361, 102], [101, 1030, 5310, 2129, 2515, 2008, 2735, 2046, 1037, 3789, 2005, 8398, 1029, 2023, 2711, 2038, 1037, 7961, 15074, 1998, 2347, 1521, 1056, 2412, 2183, 2000, 3789, 2005, 7226, 2368, 1012, 102], [101, 1045, 1005, 1049, 2025, 1037, 5470, 1997, 15941, 5472, 2121, 1005, 1055, 2021, 2296, 2051, 2002, 2758, 1000, 8529, 2319, 1000, 1010, 1045, 2064, 1005, 1056, 2393, 2021, 2868, 1012, 1001, 17183, 3207, 20179, 102], [101, 1000, 2004, 2019, 11265, 2050, 2586, 19012, 1010, 1045, 2097, 2954, 2000, 2191, 2469, 2008, 2009, 1521, 1055, 6529, 1010, 3008, 1010, 1998, 19156, 10561, 2043, 2009, 1521, 1055, 3647, 2000, 2175, 2067, 2000, 2082, 1517, 2025, 8801, 1012, 1524, 1011, 1030, 5310, 1012, 4067, 2017, 1010, 23787, 4747, 2005, 2115, 8426, 2000, 2493, 1998, 19156, 999, 1001, 19156, 29278, 5558, 2063, 1001, 17183, 8663, 15338, 3258, 102], [101, 1030, 5310, 1030, 5310, 2205, 2919, 2017, 2481, 1521, 1056, 2156, 2083, 1996, 1001, 1040, 12273, 5365, 14652, 1998, 2156, 2613, 17571, 999, 102], [101, 1030, 5310, 1030, 5310, 1030, 5310, 11425, 2190, 2001, 2725, 2014, 3105, 999, 999, 2242, 1523, 19156, 1524, 2066, 2017, 1010, 2323, 2022, 2725, 999, 999, 999, 2612, 1010, 2017, 6039, 2402, 9273, 2007, 6102, 2128, 29469, 2278, 1012, 1012, 1012, 1012, 2182, 1521, 1055, 2019, 2801, 1010, 6570, 8866, 1010, 2025, 3167, 8909, 8780, 21615, 1012, 1012, 1012, 1012, 6570, 2489, 3241, 1012, 1012, 1012, 2025, 11424, 6593, 11796, 3508, 999, 999, 999, 1001, 8398, 11387, 11387, 102], [101, 2274, 1997, 1996, 5221, 6270, 9021, 2015, 1997, 1996, 2034, 2305, 1997, 1996, 3951, 4680, 1001, 29300, 2278, 11387, 11387, 1001, 29300, 21408, 2078, 15338, 3258, 1001, 29300, 6593, 14287, 7245, 1001, 8398, 26775, 14428, 7011, 4328, 2135, 1001, 2175, 15042, 20026, 13290, 2015, 1001, 7226, 2368, 8167, 6935, 11387, 11387, 1001, 7226, 2368, 2890, 14289, 16558, 5555, 3619, 8299, 102], [101, 1996, 2034, 2382, 8117, 2015, 1997, 1996, 1001, 3537, 25434, 8663, 15338, 3258, 2031, 2042, 100, 3685, 3524, 2005, 18520, 1010, 3870, 1010, 13857, 1010, 1998, 21911, 2050, 1012, 1001, 17183, 8663, 15338, 3258, 1001, 3533, 17062, 2368, 27052, 7911, 8167, 6935, 11387, 11387, 1001, 3789, 17062, 2368, 8167, 15061, 8820, 3726, 14074, 14735, 102], [101, 2204, 2851, 999, 4931, 3146, 999, 1008, 4228, 2420, 1008, 2127, 2220, 1999, 1011, 2711, 6830, 999, 4868, 2420, 2000, 13292, 1017, 1012, 5653, 1011, 1999, 17069, 2323, 2022, 7194, 2151, 2154, 1012, 11245, 2041, 2043, 2017, 4374, 6737, 3531, 1012, 2057, 1521, 2128, 2183, 2000, 2079, 2023, 999, 1001, 3533, 17062, 2368, 1001, 21911, 23278, 18752, 2015, 8299, 102], [101, 1005, 21911, 2050, 5671, 2003, 1996, 2087, 4314, 2711, 1999, 3519, 1005, 1024, 8398, 19764, 2046, 7226, 2368, 1005, 1055, 21210, 4060, 1998, 2758, 2016, 1005, 1055, 2130, 2062, 4314, 2084, 15941, 12055, 1012, 1001, 8398, 1012, 1012, 1001, 2175, 2361, 1012, 1012, 1001, 3864, 102], [101, 1996, 2190, 2003, 2664, 2000, 2272, 1012, 1012, 1012, 1001, 8398, 11387, 11387, 1001, 8962, 2271, 1001, 8962, 2271, 19961, 1030, 5310, 1030, 5310, 1030, 5310, 8299, 102], [101, 2122, 1001, 3533, 17062, 2368, 14997, 2024, 9951, 1012, 2644, 2667, 2000, 2191, 2032, 2041, 2066, 1037, 3002, 1012, 2182, 1005, 1055, 2035, 2017, 2428, 2342, 2000, 2154, 2055, 2032, 1024, 1000, 2002, 1005, 1055, 2025, 8398, 1012, 1000, 2008, 1005, 1055, 10468, 2129, 1045, 2514, 2055, 2068, 1012, 7226, 2368, 19237, 1012, 8398, 19237, 4788, 1012, 2203, 1997, 2466, 1012, 102], [101, 1000, 1045, 3825, 8817, 1997, 6363, 1999, 7773, 1010, 1000, 2343, 8398, 2758, 1012, 1001, 4883, 3207, 20179, 11387, 11387, 1001, 14379, 11387, 11387, 1001, 3915, 1001, 23848, 2050, 11387, 11387, 1001, 10556, 2290, 1001, 2602, 11387, 11387, 1001, 5981, 15864, 1001, 4610, 102], [101, 1030, 5310, 7050, 100, 1030, 5310, 1030, 5310, 2435, 2033, 10720, 2015, 999, 100, 1001, 8294, 100, 1001, 2373, 15794, 5369, 5051, 27469, 1001, 17183, 8663, 15338, 3258, 1001, 8037, 1001, 17772, 1001, 7226, 2368, 8167, 6935, 11387, 11387, 13122, 10696, 14074, 14735, 1001, 7226, 2368, 8167, 6935, 8653, 24198, 11387, 11387, 102], [101, 1030, 5310, 1045, 2031, 6719, 2196, 2081, 1037, 7615, 2006, 2023, 2591, 2865, 4132, 1999, 2026, 2166, 1998, 2097, 2763, 2022, 25030, 2044, 2023, 2028, 1012, 7226, 2368, 2003, 2025, 17824, 2130, 2043, 3752, 1037, 3898, 1012, 1998, 2017, 1521, 2128, 24234, 2075, 8398, 3495, 1998, 17351, 2007, 2115, 2780, 3980, 1012, 9217, 7226, 2368, 1521, 1055, 11896, 1012, 102], [101, 2065, 2017, 2123, 1005, 1056, 3789, 2005, 1046, 1012, 7226, 2368, 1010, 2111, 6069, 7868, 2008, 2017, 2097, 3789, 2005, 8398, 2004, 2065, 2045, 2003, 2053, 2060, 3822, 5724, 8840, 2140, 102], [101, 1030, 5310, 1030, 5310, 8398, 2064, 1521, 1056, 2360, 6504, 1010, 2010, 6043, 2024, 2025, 2759, 102], [101, 1030, 5310, 2339, 2123, 1005, 1056, 2017, 13520, 2115, 3714, 12163, 2291, 1037, 2270, 2740, 5325, 1029, 1029, 1029, 1019, 5477, 2706, 1045, 1005, 2310, 2042, 3403, 2005, 2026, 2769, 999, 1045, 6655, 2017, 2030, 2151, 2372, 1997, 2115, 3095, 1004, 23713, 1025, 2155, 2038, 2025, 4771, 1037, 2309, 3477, 5403, 3600, 2076, 2023, 6090, 3207, 7712, 999, 999, 999, 1001, 6366, 2860, 16584, 5017, 1001, 1059, 16584, 16862, 12722, 2015, 1001, 2136, 24456, 2361, 1001, 1059, 27767, 2487, 27767, 2050, 102], [101, 1030, 5310, 2053, 2909, 2057, 2024, 5204, 1997, 2115, 12225, 1998, 2017, 2097, 2022, 5444, 2041, 1999, 2281, 20228, 2015, 12897, 2115, 27118, 7542, 2003, 22072, 2290, 4480, 2012, 3891, 1012, 2057, 1996, 2111, 2024, 3201, 2000, 3789, 2017, 2041, 1999, 1037, 20148, 1012, 1001, 3789, 16558, 23361, 8820, 3726, 14074, 14735, 102], [101, 2017, 2134, 2102, 2079, 2115, 3105, 1012, 2017, 3613, 2000, 2025, 2079, 2115, 3105, 1012, 2035, 6677, 2024, 2006, 2017, 2017, 10041, 1010, 16592, 100, 1012, 2637, 3791, 5604, 2005, 2035, 1012, 2085, 999, 1001, 8398, 11983, 14074, 14735, 3619, 10265, 1001, 5604, 22199, 2075, 22199, 2075, 1996, 2069, 2126, 2000, 5383, 1004, 23713, 1025, 2491, 999, 1001, 3789, 16558, 23361, 8820, 3726, 14074, 14735, 102], [101, 6396, 2102, 8368, 2604, 25967, 28514, 2005, 10851, 19220, 2007, 1520, 2576, 25869, 9648, 1521, 2004, 2002, 3632, 2006, 10885, 1011, 8299, 102], [101, 3793, 19988, 14142, 2000, 2131, 2592, 1997, 2129, 2000, 3789, 999, 1001, 1040, 12273, 11387, 11387, 102], [101, 2184, 2781, 2000, 2175, 1010, 3246, 2017, 2031, 2288, 2115, 24593, 3201, 1001, 8398, 1001, 7226, 2368, 102], [101, 2016, 2134, 1005, 1056, 9772, 2009, 1001, 17183, 3207, 20179, 102], [101, 15941, 2038, 2589, 1996, 2157, 2518, 1010, 2295, 2002, 2018, 2053, 2060, 5724, 1012, 2000, 3613, 1037, 2048, 1011, 16660, 5049, 1010, 2002, 1998, 7226, 2368, 2052, 2031, 2000, 7697, 2169, 2060, 4237, 1012, 2069, 8398, 2052, 5770, 1010, 2926, 2065, 2002, 2921, 2010, 5003, 2860, 3844, 1998, 2106, 2498, 999, 1001, 12609, 12260, 7542, 1001, 102], [101, 1030, 5310, 2017, 2614, 3294, 9951, 2157, 2085, 1012, 2175, 2000, 2793, 999, 1001, 17183, 8663, 15338, 3258, 102], [101, 2057, 2024, 1037, 2929, 9359, 1996, 2773, 1997, 1996, 2373, 1997, 1996, 2931, 3789, 2058, 1996, 1996, 2197, 2531, 2086, 1030, 5310, 1030, 5310, 1030, 5310, 1001, 3789, 1001, 4236, 26525, 12184, 11387, 11387, 1001, 12609, 12260, 7542, 8299, 102], [101, 1045, 2123, 1521, 1056, 2131, 2009, 1012, 2129, 2003, 2023, 2130, 1037, 5049, 1029, 1001, 3537, 8663, 15338, 3258, 1001, 17183, 8663, 15338, 3258, 1001, 2637, 14643, 15312, 4590, 24415, 17062, 2368, 1030, 5310, 102], [101, 1030, 5310, 1030, 5310, 1001, 8275, 2638, 9333, 1001, 1053, 6761, 2078, 9714, 3399, 102], [101, 3220, 18210, 1041, 24411, 2232, 2203, 5668, 2229, 3537, 4883, 9773, 3533, 7226, 2368, 3805, 1997, 2014, 2836, 2012, 1996, 12609, 3537, 2120, 4680, 1012, 8398, 2003, 1005, 9846, 2256, 2406, 1005, 1010, 2016, 2056, 1012, 2005, 2444, 14409, 1024, 8299, 1001, 1040, 12273, 11387, 11387, 1001, 17183, 8663, 15338, 3258, 1001, 2224, 2571, 22014, 1001, 6728, 6977, 2072, 102], [101, 1030, 5310, 7226, 2368, 2074, 4760, 2039, 1998, 5131, 5222, 999, 1996, 17626, 2003, 2275, 17111, 9541, 2659, 2008, 2002, 2097, 2663, 2074, 2011, 4760, 2039, 1012, 7539, 1997, 2054, 6433, 1012, 102], [101, 1030, 5310, 1030, 5310, 6160, 2000, 1001, 3533, 17062, 2368, 1521, 1055, 2544, 1997, 2637, 1011, 2746, 2000, 1037, 5101, 2379, 2017, 1012, 1001, 12609, 12155, 12096, 29100, 1001, 12609, 12260, 7542, 8299, 102], [101, 1030, 5310, 2065, 1045, 2020, 1996, 7226, 2368, 2136, 1010, 1045, 1005, 1040, 2224, 3892, 2000, 2404, 19817, 1008, 6131, 2006, 4446, 2005, 1996, 6677, 1997, 3263, 1010, 2199, 4841, 1012, 2191, 2032, 5490, 10179, 10867, 1012, 102], [101, 1045, 2572, 2667, 2000, 5674, 2040, 2052, 2298, 2062, 21934, 4842, 2075, 1998, 5003, 18696, 1999, 1037, 8398, 1013, 22950, 5981, 1012, 1001, 17183, 3207, 20179, 102], [101, 2023, 2694, 2208, 1011, 2265, 3677, 7551, 2001, 1037, 4945, 1001, 3533, 17062, 2368, 1001, 14379, 11387, 11387, 102], [101, 3173, 2026, 3052, 7188, 7226, 2368, 8847, 102], [101, 1030, 5310, 2489, 1996, 7980, 2094, 2336, 2040, 2020, 14177, 2013, 2308, 2040, 2024, 2017, 2139, 16467, 2023, 2154, 2205, 999, 2017, 2097, 2022, 5091, 2124, 2004, 1996, 2283, 2008, 10312, 2336, 2185, 2013, 2037, 10756, 999, 1001, 3789, 16558, 23361, 8820, 3726, 14074, 14735, 102], [101, 4593, 2017, 2134, 1521, 1056, 4952, 2000, 2010, 4613, 1012, 2002, 2106, 2025, 2091, 13068, 2505, 1012, 2009, 2001, 3819, 999, 1045, 2052, 2031, 3866, 2000, 2156, 7226, 2368, 3713, 2444, 2005, 2074, 2321, 2781, 999, 1001, 5263, 1030, 5310, 1001, 8398, 11387, 11387, 100, 8299, 102], [101, 1000, 3537, 2120, 4680, 1024, 4174, 18079, 1012, 21625, 1059, 16584, 5017, 18058, 2014, 4769, 1000, 1001, 17183, 8663, 15338, 3258, 8299, 102], [101, 1000, 2017, 2215, 2033, 2000, 2171, 2068, 1029, 1000, 1000, 2748, 1012, 1000, 15941, 2003, 2383, 3904, 1997, 2115, 4485, 1012, 1001, 17183, 3207, 20179, 102], [101, 2009, 1005, 1055, 2042, 1037, 3565, 10720, 2095, 1012, 1001, 3789, 16558, 23361, 8820, 3726, 14074, 14735, 1001, 7226, 2368, 11387, 11387, 8299, 102], [101, 1001, 21911, 23278, 18752, 2015, 8847, 2000, 1996, 2200, 2613, 7860, 5307, 4841, 1010, 1999, 1996, 2568, 1997, 23848, 2050, 1004, 23713, 1025, 1001, 29300, 2278, 11387, 11387, 7492, 1517, 2008, 2965, 1001, 7226, 2368, 8167, 6935, 11387, 11387, 1523, 16424, 3915, 1524, 1004, 23713, 1025, 2024, 1523, 2061, 4997, 1012, 1524, 2040, 2428, 16424, 2637, 1013, 24978, 1029, 1001, 8398, 11387, 11387, 2097, 2196, 2079, 1037, 4365, 2518, 2021, 1059, 14014, 1004, 23713, 1025, 7499, 1012, 1001, 8398, 15872, 21559, 22420, 6962, 102], [101, 1030, 5310, 1001, 2175, 2361, 1996, 2283, 1997, 2235, 2231, 1010, 2025, 999, 1030, 5310, 1001, 19220, 23947, 4270, 1001, 19220, 102], [101, 1030, 5310, 1030, 5310, 1030, 5310, 1996, 2606, 1004, 23713, 1025, 5789, 2003, 27503, 1001, 3537, 25434, 8663, 15338, 3258, 1001, 3537, 8663, 15338, 3258, 8299, 102], [101, 7791, 2000, 2119, 2122, 4364, 2005, 1016, 2847, 3892, 1012, 2172, 1997, 2054, 1045, 2657, 2003, 2062, 2502, 2231, 2008, 10659, 2489, 1013, 2489, 1013, 2489, 1010, 2330, 6645, 1010, 4288, 1996, 3514, 3068, 1998, 2062, 1012, 4283, 1010, 2021, 2057, 2253, 2083, 1022, 2086, 1997, 2008, 2007, 8112, 1998, 2116, 1997, 2149, 2079, 2025, 2215, 2000, 2175, 2067, 1012, 1012, 1012, 1012, 1001, 17183, 3207, 20179, 102], [101, 1030, 5310, 1030, 5310, 4480, 1997, 7672, 2448, 3655, 2008, 2024, 2108, 3908, 2342, 2000, 9790, 2037, 2334, 2231, 2005, 4352, 2023, 2375, 24913, 1010, 1998, 7694, 2009, 2004, 1001, 9379, 21572, 22199, 1012, 2202, 2037, 2769, 1010, 1001, 12475, 24138, 4213, 8737, 1998, 2486, 2334, 2231, 2000, 3499, 2976, 2231, 8830, 2000, 9239, 2344, 1012, 102], [101, 2663, 2054, 1029, 1996, 2971, 2005, 5409, 1998, 2087, 9596, 2398, 20459, 2063, 2412, 1029, 1030, 5310, 1030, 5310, 1001, 7226, 2368, 8167, 6935, 11387, 11387, 1001, 7226, 2368, 11387, 11387, 1001, 21911, 23278, 18752, 2015, 1001, 12609, 12260, 7542, 1001, 2398, 20459, 2063, 100, 8299, 102], [101, 1030, 5310, 2057, 2113, 2488, 3533, 1012, 1001, 8398, 11387, 11387, 1030, 5310, 1030, 5310, 100, 8299, 102], [101, 1030, 5310, 2748, 1010, 2057, 2097, 5454, 1037, 2488, 2126, 1010, 2720, 1012, 1004, 23713, 1025, 3680, 1012, 5747, 999, 999, 100, 1001, 3789, 16558, 23361, 8820, 3726, 14074, 14735, 102], [101, 1030, 5310, 1030, 5310, 1030, 5310, 1030, 5310, 2138, 1045, 2219, 5144, 1999, 3146, 1012, 1045, 5444, 1999, 1996, 2233, 3078, 1999, 3146, 1998, 2097, 2175, 2067, 2000, 3789, 1999, 2281, 1999, 3146, 999, 7359, 2003, 2574, 2000, 2022, 2026, 2047, 2188, 1010, 2021, 2127, 2059, 1045, 2097, 1001, 3789, 16558, 23361, 8820, 3726, 14074, 14735, 1998, 3146, 1012, 1001, 19067, 23115, 2063, 8299, 102], [101, 15699, 2232, 1996, 3178, 1010, 15699, 2232, 1996, 2158, 1004, 23713, 1025, 2450, 1001, 7226, 2368, 8167, 6935, 11387, 11387, 100, 1001, 7226, 2368, 29278, 14074, 14735, 100, 1001, 15653, 24456, 2361, 100, 1001, 15653, 26775, 14659, 2098, 24456, 2361, 100, 8299, 102], [101, 9393, 3331, 2055, 1523, 2183, 2152, 1524, 2003, 2074, 4895, 22852, 999, 2054, 2112, 1997, 6012, 2111, 2039, 1999, 1996, 2395, 1004, 23713, 1025, 3046, 2000, 4853, 1037, 25073, 2700, 2343, 2003, 1523, 2183, 2152, 1524, 1029, 1006, 2453, 10587, 13764, 2067, 1996, 8275, 7603, 2205, 1012, 2009, 1521, 1055, 1037, 2210, 2172, 1007, 1001, 17183, 8663, 15338, 3258, 102], [101, 1001, 3533, 17062, 2368, 27052, 7911, 8167, 6935, 11387, 11387, 1001, 3533, 17062, 2368, 1001, 7672, 26243, 3111, 13337, 2265, 2039, 2012, 2474, 2902, 12318, 22168, 10558, 1010, 14315, 1005, 1045, 3246, 2027, 1042, 1011, 1011, 1011, 1011, 1011, 1011, 3280, 1005, 8299, 102], [101, 1037, 3232, 7760, 2013, 7483, 1005, 1055, 4013, 1011, 2166, 13965, 2007, 1030, 5310, 1012, 2057, 2024, 7098, 2000, 4256, 2007, 8037, 2005, 2166, 1999, 2344, 2000, 14323, 11324, 4654, 7913, 26725, 2306, 1996, 1040, 12273, 999, 1001, 14008, 14406, 6279, 1001, 17183, 8663, 15338, 3258, 8299, 102], [101, 1030, 5310, 2057, 5632, 2296, 3371, 1997, 2009, 999, 999, 999, 1001, 23848, 2050, 1001, 8398, 11387, 11387, 102], [101, 1001, 2162, 2239, 12155, 3549, 4247, 1001, 28616, 15707, 4890, 3040, 1001, 6335, 4819, 9626, 2099, 12308, 2126, 1001, 7672, 1018, 3423, 1001, 1042, 21693, 1001, 7354, 5856, 3207, 2239, 13945, 6580, 1997, 1009, 3156, 2243, 2210, 3057, 1999, 3915, 1012, 1001, 10958, 20952, 6633, 1030, 5310, 19424, 1012, 1001, 2304, 3669, 6961, 18900, 3334, 1001, 2775, 7875, 8557, 1030, 5310, 1030, 5310, 1030, 5310, 1030, 5310, 8299, 102], [101, 2104, 1030, 5310, 1520, 1055, 3768, 1997, 4105, 1004, 23713, 1025, 2010, 2342, 2000, 11443, 2149, 2612, 1997, 15908, 2149, 1010, 2097, 2991, 1013, 8246, 1012, 1037, 2160, 4055, 3685, 3233, 1012, 1037, 2160, 2142, 2097, 2004, 2009, 2038, 2307, 2490, 1004, 23713, 1025, 1037, 2204, 3192, 1012, 1001, 3789, 16558, 23361, 8820, 3726, 14074, 14735, 8299, 102], [101, 2064, 2017, 3305, 1030, 5310, 2043, 2002, 8847, 1029, 1001, 3533, 17062, 2368, 1001, 7226, 2368, 11387, 11387, 1001, 2184, 29100, 6499, 14876, 7228, 7442, 7542, 102], [101, 1045, 1005, 1040, 5454, 26261, 10532, 2058, 7226, 2368, 2151, 2154, 1012, 1001, 3537, 3207, 20179, 1001, 17183, 3207, 20179, 1001, 17183, 3207, 20179, 11387, 11387, 102]]\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '2', '0', '2', '2', '0', '0', '0', '1', '0', '1', '2', '0', '1', '1', '0', '2', '0', '0', '1', '2', '0', '0', '0', '0', '1', '2', '2', '2', '1', '2', '0', '0', '1', '2', '2', '0', '0', '0', '0', '2', '0', '1', '0', '0', '2', '0', '0', '0', '1', '1', '1', '0', '1', '0', '2', '1', '1', '2', '1', '0', '0', '1', '1', '0', '2', '1', '2', '0', '1', '0', '0', '1', '1', '1', '1', '2', '0', '0', '1', '1', '1', '0', '2', '2', '0', '0', '2', '1', '1', '0', '2', '2', '0', '2', '0', '0', '1', '0', '0', '0', '2', '2', '0', '2', '2', '1', '1', '1', '0', '2', '2', '2', '2', '0', '2', '2', '2', '1', '0', '0', '2', '0', '2', '2', '0', '0', '2', '2', '2', '1', '0', '0', '0', '1', '2', '0', '1', '2', '0', '1', '0', '1', '1', '2', '2', '2', '0', '1', '0', '1', '1', '0', '2', '0', '0', '0', '1', '0', '0', '0', '1', '0', '1', '0', '1', '0', '2', '2', '0', '0', '1', '0', '2', '2', '0', '0', '0', '1', '0', '2', '0', '1', '1', '0', '2', '1', '1', '2', '1', '2', '1', '0', '2', '2', '0', '1', '0', '0', '1', '1', '1', '1', '2', '1', '0', '0', '0', '1', '0', '1', '2', '1', '0', '0', '1', '1', '2', '2', '2', '1', '2', '0', '2', '2', '1', '2', '2', '1', '0', '0', '1', '0', '0', '1', '1', '0', '1', '1', '2', '2', '0', '1', '2', '2', '0', '1', '0', '2', '2', '0', '1', '2', '0', '0', '1', '1', '2', '1', '0', '1', '0', '2', '0', '2', '2', '0', '1', '0', '2', '0', '1', '0', '2', '0', '0', '2', '0', '2', '0', '2', '2', '0', '2', '0', '0', '1', '0', '0', '2', '2', '1', '2', '0', '1', '2', '0', '0', '1', '0', '2', '0', '0', '0', '0', '2', '1', '0', '0', '0', '0', '2', '0', '1', '0', '1', '1', '0', '0', '1', '0', '2', '1', '1', '1', '2', '2', '2', '2', '1', '2', '0', '0', '2', '1', '1', '1', '0', '0', '0', '0', '0', '0', '1', '0', '0', '1', '2', '1', '0', '1', '0', '1', '2', '0', '0', '1', '1', '0', '0', '0', '2', '0', '2', '0', '1', '1', '0', '1', '0', '2', '0', '1', '1', '2']\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 225\n",
      "Validation set size: 75\n",
      "Test set size: 75\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming you have already loaded your data into X (features) and y (labels)\n",
    "\n",
    "# Splitting the dataset into training and temporary sets (combining validation and test)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "\n",
    "# Further splitting the temporary set into validation and test sets\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f\"Training set size: {len(X_train)}\")\n",
    "print(f\"Validation set size: {len(X_val)}\")\n",
    "print(f\"Test set size: {len(X_test)}\")\n",
    "\n",
    "# Now, X_train, y_train are your training data\n",
    "# X_val, y_val are your validation data\n",
    "# X_test, y_test are your test data\n",
    "\n",
    "# You can proceed with using these datasets for training and evaluating your model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Initialize the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('C:\\\\Users\\\\LENOVO\\\\Desktop\\\\bert-election2020-twitter-stance-biden')\n",
    "\n",
    "# Encoding labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# BERT Input Formatting\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "for sent in X:\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                      # Sentence to encode\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 64,           # Pad & truncate all sentences\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors\n",
    "                   )\n",
    "    \n",
    "    # Add the encoded sentence to the list\n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    \n",
    "    # And its attention mask (simply differentiates padding from non-padding)\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "# Convert the lists into tensors\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(y_encoded)\n",
    "\n",
    "# Use train_test_split to split our data into train and validation sets\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, random_state=2018, test_size=0.1)\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks, labels, random_state=2018, test_size=0.1)\n",
    "\n",
    "# Create the DataLoader for our training set\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = torch.utils.data.RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=32)\n",
    "\n",
    "# Create the DataLoader for our validation set\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = torch.utils.data.SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=32)\n",
    "\n",
    "# Initialize BertForSequenceClassification\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"C:\\\\Users\\\\LENOVO\\\\Desktop\\\\bert-election2020-twitter-stance-biden\",\n",
    "    num_labels = len(label_encoder.classes_) # The number of output labels\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(len(label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 10 ========\n",
      "Training...\n",
      "  Average training loss: 0.60\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.74\n",
      "  Validation Loss: 1.02\n",
      "======== Epoch 2 / 10 ========\n",
      "Training...\n",
      "  Average training loss: 0.47\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.64\n",
      "  Validation Loss: 1.02\n",
      "======== Epoch 3 / 10 ========\n",
      "Training...\n",
      "  Average training loss: 0.41\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.74\n",
      "  Validation Loss: 0.92\n",
      "======== Epoch 4 / 10 ========\n",
      "Training...\n",
      "  Average training loss: 0.33\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.66\n",
      "  Validation Loss: 0.99\n",
      "======== Epoch 5 / 10 ========\n",
      "Training...\n",
      "  Average training loss: 0.31\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.77\n",
      "  Validation Loss: 0.97\n",
      "======== Epoch 6 / 10 ========\n",
      "Training...\n",
      "  Average training loss: 0.25\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.76\n",
      "  Validation Loss: 0.98\n",
      "======== Epoch 7 / 10 ========\n",
      "Training...\n",
      "  Average training loss: 0.22\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.77\n",
      "  Validation Loss: 0.96\n",
      "======== Epoch 8 / 10 ========\n",
      "Training...\n",
      "  Average training loss: 0.21\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.77\n",
      "  Validation Loss: 0.98\n",
      "======== Epoch 9 / 10 ========\n",
      "Training...\n",
      "  Average training loss: 0.18\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.77\n",
      "  Validation Loss: 0.98\n",
      "======== Epoch 10 / 10 ========\n",
      "Training...\n",
      "  Average training loss: 0.19\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.77\n",
      "  Validation Loss: 0.97\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "\n",
    "# Set the seed value all over the place to make this reproducible\n",
    "seed_val = 42\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# BERT requires a special optimizer and a learning rate scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
    "epochs = 10\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "# Function for formatting elapsed times\n",
    "def format_time(elapsed):\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "\n",
    "# Training Loop\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # Training\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "        model.zero_grad()        \n",
    "\n",
    "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_dataloader)            \n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "\n",
    "    # Validation\n",
    "    print(\"\\nRunning Validation...\")\n",
    "\n",
    "    model.eval()\n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "\n",
    "    for batch in validation_dataloader:\n",
    "        \n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        with torch.no_grad():        \n",
    "            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        total_eval_loss += loss.item()\n",
    "        logits = outputs.logits\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "\n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.91      0.87        11\n",
      "           1       0.89      0.73      0.80        11\n",
      "           2       0.82      0.88      0.85        16\n",
      "\n",
      "    accuracy                           0.84        38\n",
      "   macro avg       0.85      0.84      0.84        38\n",
      "weighted avg       0.85      0.84      0.84        38\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have a function to get predictions from your model\n",
    "def get_predictions(model, dataloader):\n",
    "    model.eval()\n",
    "    predictions, true_labels = [], []\n",
    "\n",
    "    for batch in dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "\n",
    "        logits = outputs.logits\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        predictions.append(logits)\n",
    "        true_labels.append(label_ids)\n",
    "\n",
    "    return np.concatenate(predictions, axis=0), np.concatenate(true_labels, axis=0)\n",
    "\n",
    "# Get predictions for the validation set\n",
    "predictions, true_labels = get_predictions(model, validation_dataloader)\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "print(classification_report(true_labels, predicted_labels))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(true_labels, predicted_labels)\n",
    "sns.heatmap(cm, annot=True, fmt='d')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "token_lengths = data['tokens'].apply(len)\n",
    "sns.histplot(token_lengths)\n",
    "plt.title('Token Length Distribution')\n",
    "plt.xlabel('Token Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class 0 (Precision: 0.60, Recall: 0.82, F1-Score: 0.69)\n",
    "\n",
    "Precision: 60% of the instances predicted as class 0 are correct.\n",
    "Recall: The model correctly identifies 82% of the actual class 0 instances.\n",
    "F1-Score: The balance between precision and recall for class 0 is represented by an F1-score of 0.69, which is relatively high and indicates a good balance, albeit with a slightly higher emphasis on recall.\n",
    "\n",
    "Class 1 (Precision: 0.82, Recall: 0.82, F1-Score: 0.82)\n",
    "\n",
    "This class shows high and balanced precision and recall, both at 82%.\n",
    "The F1-score of 0.82 indicates a very good balance between precision and recall, suggesting that the model performs effectively for this class.\n",
    "\n",
    "Class 2 (Precision: 0.83, Recall: 0.62, F1-Score: 0.71)\n",
    "\n",
    "Precision: 83% precision indicates a high likelihood that predictions for class 2 are correct.\n",
    "Recall: The recall of 62% is the lowest among the classes but still respectable, suggesting that some class 2 instances are missed.\n",
    "F1-Score: An F1-score of 0.71 shows a good balance between precision and recall, leaning more towards precision.\n",
    "\n",
    "Overall Performance\n",
    "\n",
    "Accuracy: The overall accuracy of 74% shows that the model correctly predicts the class of an instance 74% of the time across all classes.\n",
    "Macro Average: The macro average for precision, recall, and F1-score are all 0.75. This indicates a balanced performance across all classes without bias towards any particular class.\n",
    "Weighted Average: The weighted average for precision, recall, and F1-score are slightly higher at 0.76, considering the support (number of instances) for each class. This suggests a slightly better performance for the classes with more instances.\n",
    "Interpretation and Insights\n",
    "The model shows a well-balanced performance across all classes, with particularly strong results in class 1.\n",
    "While the model performs well in precision for class 2, it could benefit from improvements in recall, indicating it might be missing some class 2 instances.\n",
    "The balanced macro and weighted averages suggest that the model is overall effective and doesn't suffer from significant bias towards any class.\n",
    "Further improvements might still be possible, particularly in increasing the recall for class 2 and balancing the precision-recall trade-off for class 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Another Training Process with other hyperparameters and other parameters in general"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>tokens</th>\n",
       "      <th>data_type</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tweet_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1298098337946783745</th>\n",
       "      <td>@USER @USER and like you, they are all squish #gop weasels who will be in tears on election night.</td>\n",
       "      <td>0</td>\n",
       "      <td>[101, 1030, 5310, 1030, 5310, 1998, 2066, 2017, 1010, 2027, 2024, 2035, 5490, 27020, 2232, 1001, 2175, 2361, 29268, 2015, 2040, 2097, 2022, 1999, 4000, 2006, 2602, 2305, 1012, 102]</td>\n",
       "      <td>not_set</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1296643755022397440</th>\n",
       "      <td>.@USER on covid-19: we'll put the politics aside. we'll take the muzzle off the experts so the the public gets the information they need and deserve. honest unvarnished truth. they can handle it. #demconvention @USER HTTP</td>\n",
       "      <td>1</td>\n",
       "      <td>[101, 1012, 1030, 5310, 2006, 2522, 17258, 1011, 2539, 1024, 2057, 1005, 2222, 2404, 1996, 4331, 4998, 1012, 2057, 1005, 2222, 2202, 1996, 17750, 2125, 1996, 8519, 2061, 1996, 1996, 2270, 4152, 1996, 2592, 2027, 2342, 1998, 10107, 1012, 7481, 4895, 10755, 28357, 3606, 1012, 2027, 2064, 5047, 2009, 1012, 1001, 17183, 8663, 15338, 3258, 1030, 5310, 8299, 102]</td>\n",
       "      <td>not_set</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1293907151501459462</th>\n",
       "      <td>@USER google: how to vote early in \"your state\" and get it done as soon as possible. we have a duty to help the u.s. postal service process these ballots and reduce the stress donald trump is intentionally placing on these public servants. \\n\\n#voteearly #voteblue #bidenharris2020</td>\n",
       "      <td>1</td>\n",
       "      <td>[101, 1030, 5310, 8224, 1024, 2129, 2000, 3789, 2220, 1999, 1000, 2115, 2110, 1000, 1998, 2131, 2009, 2589, 2004, 2574, 2004, 2825, 1012, 2057, 2031, 1037, 4611, 2000, 2393, 1996, 1057, 1012, 1055, 1012, 10690, 2326, 2832, 2122, 17069, 1998, 5547, 1996, 6911, 6221, 8398, 2003, 15734, 6885, 2006, 2122, 2270, 8858, 1012, 1001, 3789, 14644, 2135, 1001, 3789, 16558, 5657, 1001, 7226, 2368, 8167, 6935, 11387, 11387, 102]</td>\n",
       "      <td>not_set</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1239356229803225088</th>\n",
       "      <td>am i the only one that pictures biden this way all the time? #demdebate HTTP</td>\n",
       "      <td>0</td>\n",
       "      <td>[101, 2572, 1045, 1996, 2069, 2028, 2008, 4620, 7226, 2368, 2023, 2126, 2035, 1996, 2051, 1029, 1001, 17183, 3207, 20179, 8299, 102]</td>\n",
       "      <td>not_set</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1298801129984266241</th>\n",
       "      <td>@USER was it the infanticide lie the nun dropped on us that was helpful?\\n#rnc2020</td>\n",
       "      <td>0</td>\n",
       "      <td>[101, 1030, 5310, 2001, 2009, 1996, 10527, 21752, 4682, 1996, 16634, 3333, 2006, 2149, 2008, 2001, 14044, 1029, 1001, 29300, 2278, 11387, 11387, 102]</td>\n",
       "      <td>not_set</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                          text  \\\n",
       "tweet_id                                                                                                                                                                                                                                                                                                         \n",
       "1298098337946783745                                                                                                                                                                                         @USER @USER and like you, they are all squish #gop weasels who will be in tears on election night.   \n",
       "1296643755022397440                                                              .@USER on covid-19: we'll put the politics aside. we'll take the muzzle off the experts so the the public gets the information they need and deserve. honest unvarnished truth. they can handle it. #demconvention @USER HTTP   \n",
       "1293907151501459462  @USER google: how to vote early in \"your state\" and get it done as soon as possible. we have a duty to help the u.s. postal service process these ballots and reduce the stress donald trump is intentionally placing on these public servants. \\n\\n#voteearly #voteblue #bidenharris2020   \n",
       "1239356229803225088                                                                                                                                                                                                               am i the only one that pictures biden this way all the time? #demdebate HTTP   \n",
       "1298801129984266241                                                                                                                                                                                                         @USER was it the infanticide lie the nun dropped on us that was helpful?\\n#rnc2020   \n",
       "\n",
       "                     label  \\\n",
       "tweet_id                     \n",
       "1298098337946783745      0   \n",
       "1296643755022397440      1   \n",
       "1293907151501459462      1   \n",
       "1239356229803225088      0   \n",
       "1298801129984266241      0   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                  tokens  \\\n",
       "tweet_id                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
       "1298098337946783745                                                                                                                                                                                                                                                 [101, 1030, 5310, 1030, 5310, 1998, 2066, 2017, 1010, 2027, 2024, 2035, 5490, 27020, 2232, 1001, 2175, 2361, 29268, 2015, 2040, 2097, 2022, 1999, 4000, 2006, 2602, 2305, 1012, 102]   \n",
       "1296643755022397440                                                              [101, 1012, 1030, 5310, 2006, 2522, 17258, 1011, 2539, 1024, 2057, 1005, 2222, 2404, 1996, 4331, 4998, 1012, 2057, 1005, 2222, 2202, 1996, 17750, 2125, 1996, 8519, 2061, 1996, 1996, 2270, 4152, 1996, 2592, 2027, 2342, 1998, 10107, 1012, 7481, 4895, 10755, 28357, 3606, 1012, 2027, 2064, 5047, 2009, 1012, 1001, 17183, 8663, 15338, 3258, 1030, 5310, 8299, 102]   \n",
       "1293907151501459462  [101, 1030, 5310, 8224, 1024, 2129, 2000, 3789, 2220, 1999, 1000, 2115, 2110, 1000, 1998, 2131, 2009, 2589, 2004, 2574, 2004, 2825, 1012, 2057, 2031, 1037, 4611, 2000, 2393, 1996, 1057, 1012, 1055, 1012, 10690, 2326, 2832, 2122, 17069, 1998, 5547, 1996, 6911, 6221, 8398, 2003, 15734, 6885, 2006, 2122, 2270, 8858, 1012, 1001, 3789, 14644, 2135, 1001, 3789, 16558, 5657, 1001, 7226, 2368, 8167, 6935, 11387, 11387, 102]   \n",
       "1239356229803225088                                                                                                                                                                                                                                                                                                 [101, 2572, 1045, 1996, 2069, 2028, 2008, 4620, 7226, 2368, 2023, 2126, 2035, 1996, 2051, 1029, 1001, 17183, 3207, 20179, 8299, 102]   \n",
       "1298801129984266241                                                                                                                                                                                                                                                                                [101, 1030, 5310, 2001, 2009, 1996, 10527, 21752, 4682, 1996, 16634, 3333, 2006, 2149, 2008, 2001, 14044, 1029, 1001, 29300, 2278, 11387, 11387, 102]   \n",
       "\n",
       "                    data_type  \n",
       "tweet_id                       \n",
       "1298098337946783745   not_set  \n",
       "1296643755022397440   not_set  \n",
       "1293907151501459462   not_set  \n",
       "1239356229803225088   not_set  \n",
       "1298801129984266241   not_set  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['data_type'] = ['not_set'] * data.shape[0]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(data.index.values, data.label.values,test_size = 0.15,random_state = 17,stratify = data.label.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[X_train, 'data_type'] = 'train'\n",
    "data.loc[X_val, 'data_type'] = 'val'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th>data_type</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">0</th>\n",
       "      <th>train</th>\n",
       "      <td>133</td>\n",
       "      <td>133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
       "      <th>train</th>\n",
       "      <td>95</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">2</th>\n",
       "      <th>train</th>\n",
       "      <td>90</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 text  tokens\n",
       "label data_type              \n",
       "0     train       133     133\n",
       "      val          24      24\n",
       "1     train        95      95\n",
       "      val          17      17\n",
       "2     train        90      90\n",
       "      val          16      16"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby(['label', 'data_type']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    }
   ],
   "source": [
    "#Tokenize train set\n",
    "encoded_data_train = tokenizer.batch_encode_plus(data[data.data_type == 'train'].text.values,\n",
    "                                                add_special_tokens = True,\n",
    "                                                return_attention_mask = True,\n",
    "                                                pad_to_max_length = True,\n",
    "                                                max_length = 150,\n",
    "                                                return_tensors = 'pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizer val set\n",
    "encoded_data_val = tokenizer.batch_encode_plus(data[data.data_type == 'val'].text.values,\n",
    "                                                #add_special_tokens = True,\n",
    "                                                return_attention_mask = True,\n",
    "                                                pad_to_max_length = True,\n",
    "                                                max_length = 150,\n",
    "                                                return_tensors = 'pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 1030, 5310,  ...,    0,    0,    0],\n",
       "        [ 101, 1012, 1030,  ...,    0,    0,    0],\n",
       "        [ 101, 2572, 1045,  ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [ 101, 2065, 2057,  ...,    0,    0,    0],\n",
       "        [ 101, 1045, 2066,  ...,    0,    0,    0],\n",
       "        [ 101, 1030, 5310,  ...,    0,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]])}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encode train set\n",
    "input_ids_train = encoded_data_train['input_ids']\n",
    "attention_masks_train = encoded_data_train['attention_mask']\n",
    "labels_train = torch.tensor(data[data.data_type == 'train'].label.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encode val set\n",
    "input_ids_val = encoded_data_val['input_ids']\n",
    "attention_masks_val = encoded_data_val['attention_mask']\n",
    "\n",
    "#Convert data type to torch.tensor\n",
    "labels_val = torch.tensor(data[data.data_type == 'val'].label.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 101, 1030, 5310,  ...,    0,    0,    0],\n",
       "        [ 101, 1012, 1030,  ...,    0,    0,    0],\n",
       "        [ 101, 2572, 1045,  ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [ 101, 2065, 2057,  ...,    0,    0,    0],\n",
       "        [ 101, 1045, 2066,  ...,    0,    0,    0],\n",
       "        [ 101, 1030, 5310,  ...,    0,    0,    0]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_masks_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 0, 2, 0, 0, 1, 0, 2, 1, 2, 2, 2, 0, 0, 1, 2, 2, 1, 2, 1, 2, 0, 2,\n",
       "        0, 2, 2, 0, 0, 1, 0, 0, 2, 1, 0, 0, 1, 1, 0, 2, 0, 1, 2, 0, 0, 1, 2, 0,\n",
       "        2, 1, 0, 1, 1, 0, 1, 2, 2, 0, 2, 0, 1, 1, 1, 2, 1, 1, 2, 0, 2, 1, 0, 2,\n",
       "        0, 1, 0, 2, 0, 1, 0, 2, 2, 2, 0, 2, 2, 0, 1, 2, 1, 1, 0, 0, 2, 0, 0, 1,\n",
       "        0, 1, 1, 1, 2, 2, 1, 2, 1, 0, 1, 2, 1, 1, 0, 0, 0, 0, 0, 2, 0, 2, 1, 1,\n",
       "        0, 0, 1, 0, 1, 0, 1, 0, 1, 2, 0, 2, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 2, 2,\n",
       "        1, 0, 0, 0, 0, 0, 1, 0, 2, 2, 0, 0, 0, 1, 2, 0, 1, 1, 0, 0, 0, 2, 2, 0,\n",
       "        2, 0, 2, 0, 2, 1, 1, 0, 0, 0, 2, 1, 0, 2, 2, 0, 2, 1, 0, 2, 1, 1, 0, 0,\n",
       "        1, 2, 1, 0, 0, 1, 2, 2, 2, 1, 1, 0, 0, 1, 1, 0, 0, 2, 1, 0, 2, 2, 0, 2,\n",
       "        0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 2, 2, 1, 1, 2, 0, 0,\n",
       "        1, 2, 1, 0, 0, 1, 2, 0, 0, 0, 0, 0, 2, 2, 2, 0, 1, 1, 2, 0, 1, 2, 0, 0,\n",
       "        0, 0, 2, 0, 0, 2, 2, 0, 2, 0, 0, 0, 1, 2, 0, 2, 1, 0, 1, 1, 0, 1, 0, 1,\n",
       "        2, 0, 0, 0, 2, 0, 2, 2, 0, 2, 1, 1, 2, 0, 2, 1, 1, 1, 1, 0, 0, 2, 2, 1,\n",
       "        0, 0, 2, 0, 0, 2])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dataloader\n",
    "dataset_train = TensorDataset(input_ids_train, \n",
    "                              attention_masks_train,\n",
    "                              labels_train)\n",
    "\n",
    "dataset_val = TensorDataset(input_ids_val, \n",
    "                             attention_masks_val, \n",
    "                             labels_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "318\n",
      "57\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset_train))\n",
    "print(len(dataset_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"_name_or_path\": \"C:\\\\Users\\\\LENOVO\\\\Desktop\\\\bert-election2020-twitter-stance-biden\",\n",
       "  \"architectures\": [\n",
       "    \"BertForSequenceClassification\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"id2label\": {\n",
       "    \"0\": \"LABEL_0\",\n",
       "    \"1\": \"LABEL_1\",\n",
       "    \"2\": \"LABEL_2\"\n",
       "  },\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"label2id\": {\n",
       "    \"LABEL_0\": 0,\n",
       "    \"LABEL_1\": 1,\n",
       "    \"LABEL_2\": 2\n",
       "  },\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"torch_dtype\": \"float32\",\n",
       "  \"transformers_version\": \"4.38.2\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 30722\n",
       "}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(PRETRAINED_LM_PATH, num_labels=3, ignore_mismatched_sizes=True)\n",
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "batch_size = 4 #Since we have limited resource\n",
    "\n",
    "#Load train set\n",
    "dataloader_train = DataLoader(dataset_train,sampler = RandomSampler(dataset_train),batch_size = batch_size)\n",
    "\n",
    "#Load val set\n",
    "dataloader_val = DataLoader(dataset_val,sampler = RandomSampler(dataset_val),batch_size = 32) #since we don't have to do backpropagation for this step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "epochs = 10\n",
    "\n",
    "#Load optimizer\n",
    "optimizer = AdamW(model.parameters(),lr = 1e-5,eps = 1e-8) #2e-5 > 5e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load scheduler\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,num_warmup_steps = 0,num_training_steps = len(dataloader_train)*epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "#F1 score\n",
    "def f1_score_func(preds, labels):\n",
    "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return f1_score(labels_flat, preds_flat, average = 'weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accuracy score\n",
    "def accuracy_per_class(preds, labels):\n",
    "    label_dict_inverse = {v: k for k, v in label_dict.items()}\n",
    "    \n",
    "    #Make prediction\n",
    "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    \n",
    "    for label in np.unique(labels_flat):\n",
    "        y_preds = preds_flat[labels_flat==label]\n",
    "        y_true = labels_flat[labels_flat==label]\n",
    "        print(f'Class: {label_dict_inverse[label]}')\n",
    "        print(f'Accuracy:{len(y_preds[y_preds==label])}/{len(y_true)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataloader_val):\n",
    "\n",
    "    #Evaluation mode disables the dropout layer \n",
    "    model.eval()\n",
    "    \n",
    "    #Tracking variables\n",
    "    loss_val_total = 0\n",
    "    predictions, true_vals = [], []\n",
    "    \n",
    "    for batch in tqdm(dataloader_val):\n",
    "        \n",
    "        #Load into GPU\n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        \n",
    "        #Define inputs\n",
    "        inputs = {'input_ids':      batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                  'labels':         batch[2]}\n",
    "\n",
    "        #Compute logits\n",
    "        with torch.no_grad():        \n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        #Compute loss\n",
    "        loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "        loss_val_total += loss.item()\n",
    "\n",
    "        #Compute accuracy\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = inputs['labels'].cpu().numpy()\n",
    "        predictions.append(logits)\n",
    "        true_vals.append(label_ids)\n",
    "    \n",
    "    #Compute average loss\n",
    "    loss_val_avg = loss_val_total/len(dataloader_val) \n",
    "    \n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    true_vals = np.concatenate(true_vals, axis=0)\n",
    "            \n",
    "    return loss_val_avg, predictions, true_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "seed_val = 17\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b0f9feb5dc04634ba89c6835e58040f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ff2b56801834084842aef933b4cadc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch {epoch}\n",
      "Training loss: 0.9696608632802963\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "418ad68c8417446692592b7d4be661a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.5666688978672028\n",
      "F1 Score (weighted): 0.7894736842105263\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34d588b84c6d48a388dedc8b76beb0a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch {epoch}\n",
      "Training loss: 0.654443123191595\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "beaf183ffac2405e92de3d741ca77b52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.5616437941789627\n",
      "F1 Score (weighted): 0.7487461787846896\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e71553558b494be99e6e5b239f1561e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch {epoch}\n",
      "Training loss: 0.5593753306195139\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac756844678c451591679d22fcafea89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.5146887898445129\n",
      "F1 Score (weighted): 0.7855474453501671\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "968cae4895d4467b82a4953187886eb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch {epoch}\n",
      "Training loss: 0.4058077627792954\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da5579aeea724efeaf3db802c63f2ee3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.4769679456949234\n",
      "F1 Score (weighted): 0.8236044657097289\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "924b71876d144d2bb6b78a3997c00876",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch {epoch}\n",
      "Training loss: 0.3326746561797336\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a1252258f5c4239b4d5fdd9ce09dbf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.517719179391861\n",
      "F1 Score (weighted): 0.8226652489810383\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cea44a99115741d29528113bcdf969a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch {epoch}\n",
      "Training loss: 0.26701633711345496\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a25eb0c20704ae2a5acf95c0b6e4fc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.7225965559482574\n",
      "F1 Score (weighted): 0.7448029747198722\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd2afa7cc8cd48f9a0bb59f935fbb2f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\"> </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> </span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">28</span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #808000; text-decoration-color: #808000\">c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module</span> <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #808000; text-decoration-color: #808000\">.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1511</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_wrapped_call_impl</span>                                                                   <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1508 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._compiled_call_impl <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>:                                          <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1509 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._compiled_call_impl(*args, **kwargs)  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># type: ignore[misc]</span>        <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1510 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                             <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span>1511 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._call_impl(*args, **kwargs)                                       <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1512 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">   </span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1513 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, *args, **kwargs):                                                <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1514 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span>forward_call = (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._slow_forward <span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> torch._C._get_tracing_state() <span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.fo  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #808000; text-decoration-color: #808000\">c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module</span> <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #808000; text-decoration-color: #808000\">.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1520</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>                                                                           <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1517 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks   <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1518 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks                   <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1519 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span>1520 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*args, **kwargs)                                          <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1521 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1522 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">try</span>:                                                                              <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1523 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span>result = <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>                                                                 <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #808000; text-decoration-color: #808000\">c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\ber</span> <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #808000; text-decoration-color: #808000\">t\\modeling_bert.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1564</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>                                                               <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1561 </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">      </span><span style=\"color: #808000; text-decoration-color: #808000\">\"\"\"</span>                                                                               <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1562 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span>return_dict = return_dict <span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> return_dict <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.config.use_return  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1563 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span>1564 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span>outputs = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.bert(                                                              <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1565 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span>input_ids,                                                                    <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1566 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span>attention_mask=attention_mask,                                                <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1567 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span>token_type_ids=token_type_ids,                                                <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #808000; text-decoration-color: #808000\">c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module</span> <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #808000; text-decoration-color: #808000\">.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1511</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_wrapped_call_impl</span>                                                                   <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1508 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._compiled_call_impl <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>:                                          <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1509 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._compiled_call_impl(*args, **kwargs)  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># type: ignore[misc]</span>        <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1510 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                             <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span>1511 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._call_impl(*args, **kwargs)                                       <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1512 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">   </span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1513 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, *args, **kwargs):                                                <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1514 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span>forward_call = (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._slow_forward <span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> torch._C._get_tracing_state() <span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.fo  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #808000; text-decoration-color: #808000\">c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module</span> <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #808000; text-decoration-color: #808000\">.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1520</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>                                                                           <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1517 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks   <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1518 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks                   <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1519 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span>1520 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*args, **kwargs)                                          <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1521 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1522 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">try</span>:                                                                              <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1523 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span>result = <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>                                                                 <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #808000; text-decoration-color: #808000\">c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\ber</span> <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #808000; text-decoration-color: #808000\">t\\modeling_bert.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1013</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>                                                               <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1010 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span>inputs_embeds=inputs_embeds,                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1011 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span>past_key_values_length=past_key_values_length,                                <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1012 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span>)                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span>1013 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span>encoder_outputs = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.encoder(                                                   <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1014 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span>embedding_output,                                                             <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1015 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span>attention_mask=extended_attention_mask,                                       <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1016 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span>head_mask=head_mask,                                                          <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #808000; text-decoration-color: #808000\">c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module</span> <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #808000; text-decoration-color: #808000\">.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1511</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_wrapped_call_impl</span>                                                                   <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1508 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._compiled_call_impl <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>:                                          <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1509 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._compiled_call_impl(*args, **kwargs)  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># type: ignore[misc]</span>        <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1510 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                             <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span>1511 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._call_impl(*args, **kwargs)                                       <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1512 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">   </span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1513 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, *args, **kwargs):                                                <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1514 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span>forward_call = (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._slow_forward <span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> torch._C._get_tracing_state() <span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.fo  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #808000; text-decoration-color: #808000\">c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module</span> <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #808000; text-decoration-color: #808000\">.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1520</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>                                                                           <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1517 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks   <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1518 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks                   <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1519 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span>1520 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*args, **kwargs)                                          <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1521 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1522 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">try</span>:                                                                              <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1523 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span>result = <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>                                                                 <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #808000; text-decoration-color: #808000\">c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\ber</span> <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #808000; text-decoration-color: #808000\">t\\modeling_bert.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">607</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>                                                                <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 604 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>output_attentions,                                                    <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 605 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>)                                                                         <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 606 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                         <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span> 607 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>layer_outputs = layer_module(                                             <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 608 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>hidden_states,                                                        <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 609 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>attention_mask,                                                       <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 610 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>layer_head_mask,                                                      <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #808000; text-decoration-color: #808000\">c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module</span> <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #808000; text-decoration-color: #808000\">.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1511</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_wrapped_call_impl</span>                                                                   <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1508 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._compiled_call_impl <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>:                                          <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1509 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._compiled_call_impl(*args, **kwargs)  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># type: ignore[misc]</span>        <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1510 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                             <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span>1511 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._call_impl(*args, **kwargs)                                       <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1512 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">   </span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1513 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, *args, **kwargs):                                                <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1514 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span>forward_call = (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._slow_forward <span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> torch._C._get_tracing_state() <span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.fo  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #808000; text-decoration-color: #808000\">c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module</span> <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #808000; text-decoration-color: #808000\">.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1520</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>                                                                           <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1517 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks   <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1518 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks                   <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1519 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span>1520 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*args, **kwargs)                                          <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1521 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1522 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">try</span>:                                                                              <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1523 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span>result = <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>                                                                 <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #808000; text-decoration-color: #808000\">c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\ber</span> <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #808000; text-decoration-color: #808000\">t\\modeling_bert.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">539</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>                                                                <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 536 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span>cross_attn_present_key_value = cross_attention_outputs[-<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>]                    <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 537 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span>present_key_value = present_key_value + cross_attn_present_key_value          <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 538 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span> 539 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span>layer_output = apply_chunking_to_forward(                                         <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 540 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.feed_forward_chunk, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.chunk_size_feed_forward, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.seq_len_dim, att  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 541 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span>)                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 542 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span>outputs = (layer_output,) + outputs                                               <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #808000; text-decoration-color: #808000\">c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\pytorch_ut</span> <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #808000; text-decoration-color: #808000\">ils.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">237</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">apply_chunking_to_forward</span>                                                          <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">234 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># concatenate output at same dimension</span>                                             <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">235 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> torch.cat(output_chunks, dim=chunk_dim)                                     <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">236 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span>237 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_fn(*input_tensors)                                                      <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">238 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">239 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">240 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">find_pruneable_heads_and_indices</span>(                                                      <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #808000; text-decoration-color: #808000\">c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\ber</span> <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #808000; text-decoration-color: #808000\">t\\modeling_bert.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">552</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">feed_forward_chunk</span>                                                     <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 549 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">   </span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 550 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">feed_forward_chunk</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, attention_output):                                       <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 551 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span>intermediate_output = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.intermediate(attention_output)                         <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span> 552 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span>layer_output = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.output(intermediate_output, attention_output)                 <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 553 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> layer_output                                                               <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 554 </span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 555 </span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #808000; text-decoration-color: #808000\">c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module</span> <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #808000; text-decoration-color: #808000\">.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1511</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_wrapped_call_impl</span>                                                                   <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1508 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._compiled_call_impl <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>:                                          <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1509 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._compiled_call_impl(*args, **kwargs)  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># type: ignore[misc]</span>        <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1510 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                             <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span>1511 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._call_impl(*args, **kwargs)                                       <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1512 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">   </span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1513 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, *args, **kwargs):                                                <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1514 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span>forward_call = (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._slow_forward <span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> torch._C._get_tracing_state() <span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.fo  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #808000; text-decoration-color: #808000\">c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module</span> <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #808000; text-decoration-color: #808000\">.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1520</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>                                                                           <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1517 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks   <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1518 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks                   <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1519 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span>1520 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*args, **kwargs)                                          <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1521 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1522 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">try</span>:                                                                              <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1523 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span>result = <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>                                                                 <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #808000; text-decoration-color: #808000\">c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\ber</span> <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #808000; text-decoration-color: #808000\">t\\modeling_bert.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">464</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>                                                                <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 461 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.dropout = nn.Dropout(config.hidden_dropout_prob)                             <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 462 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">   </span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 463 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -&gt; torch.  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span> 464 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span>hidden_states = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.dense(hidden_states)                                         <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 465 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span>hidden_states = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.dropout(hidden_states)                                       <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 466 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span>hidden_states = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.LayerNorm(hidden_states + input_tensor)                      <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 467 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> hidden_states                                                              <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #808000; text-decoration-color: #808000\">c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module</span> <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #808000; text-decoration-color: #808000\">.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1511</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_wrapped_call_impl</span>                                                                   <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1508 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._compiled_call_impl <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>:                                          <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1509 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._compiled_call_impl(*args, **kwargs)  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># type: ignore[misc]</span>        <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1510 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                             <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span>1511 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._call_impl(*args, **kwargs)                                       <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1512 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">   </span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1513 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, *args, **kwargs):                                                <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1514 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span>forward_call = (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._slow_forward <span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> torch._C._get_tracing_state() <span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.fo  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #808000; text-decoration-color: #808000\">c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module</span> <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #808000; text-decoration-color: #808000\">.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1520</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>                                                                           <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1517 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks   <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1518 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks                   <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1519 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span>1520 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*args, **kwargs)                                          <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1521 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1522 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">try</span>:                                                                              <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1523 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span>result = <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>                                                                 <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #808000; text-decoration-color: #808000\">c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\linear</span> <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #808000; text-decoration-color: #808000\">.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">116</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>                                                                               <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">113 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span>init.uniform_(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.bias, -bound, bound)                                        <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">114 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">115 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>: Tensor) -&gt; Tensor:                                            <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span>116 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> F.linear(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.weight, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.bias)                                     <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">117 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">118 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">extra_repr</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>) -&gt; <span style=\"color: #00ffff; text-decoration-color: #00ffff\">str</span>:                                                           <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">119 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #808000; text-decoration-color: #808000\">f'in_features={</span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.in_features<span style=\"color: #808000; text-decoration-color: #808000\">}, out_features={</span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.out_features<span style=\"color: #808000; text-decoration-color: #808000\">}, bias=</span>   <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">KeyboardInterrupt</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m\u001b[0m\u001b[31m\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m\u001b[0m\u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m28\u001b[0m                                                                                   \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[33mc:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module\u001b[0m \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[33m.py\u001b[0m:\u001b[94m1511\u001b[0m in \u001b[92m_wrapped_call_impl\u001b[0m                                                                   \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1508 \u001b[0m\u001b[2m      \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m._compiled_call_impl \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m:                                          \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1509 \u001b[0m\u001b[2m         \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m._compiled_call_impl(*args, **kwargs)  \u001b[2m# type: ignore[misc]\u001b[0m        \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1510 \u001b[0m\u001b[2m      \u001b[0m\u001b[94melse\u001b[0m:                                                                             \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[31m \u001b[0m1511 \u001b[2m         \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m._call_impl(*args, **kwargs)                                       \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1512 \u001b[0m\u001b[2m   \u001b[0m                                                                                      \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1513 \u001b[0m\u001b[2m   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92m_call_impl\u001b[0m(\u001b[96mself\u001b[0m, *args, **kwargs):                                                \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1514 \u001b[0m\u001b[2m      \u001b[0mforward_call = (\u001b[96mself\u001b[0m._slow_forward \u001b[94mif\u001b[0m torch._C._get_tracing_state() \u001b[94melse\u001b[0m \u001b[96mself\u001b[0m.fo  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[33mc:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module\u001b[0m \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[33m.py\u001b[0m:\u001b[94m1520\u001b[0m in \u001b[92m_call_impl\u001b[0m                                                                           \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1517 \u001b[0m\u001b[2m      \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._backward_pre_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks   \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1518 \u001b[0m\u001b[2m            \u001b[0m\u001b[95mor\u001b[0m _global_backward_pre_hooks \u001b[95mor\u001b[0m _global_backward_hooks                   \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1519 \u001b[0m\u001b[2m            \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[31m \u001b[0m1520 \u001b[2m         \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*args, **kwargs)                                          \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1521 \u001b[0m\u001b[2m      \u001b[0m                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1522 \u001b[0m\u001b[2m      \u001b[0m\u001b[94mtry\u001b[0m:                                                                              \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1523 \u001b[0m\u001b[2m         \u001b[0mresult = \u001b[94mNone\u001b[0m                                                                 \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[33mc:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\ber\u001b[0m \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[33mt\\modeling_bert.py\u001b[0m:\u001b[94m1564\u001b[0m in \u001b[92mforward\u001b[0m                                                               \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1561 \u001b[0m\u001b[2;33m      \u001b[0m\u001b[33m\"\"\"\u001b[0m                                                                               \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1562 \u001b[0m\u001b[2m      \u001b[0mreturn_dict = return_dict \u001b[94mif\u001b[0m return_dict \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m \u001b[94melse\u001b[0m \u001b[96mself\u001b[0m.config.use_return  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1563 \u001b[0m\u001b[2m      \u001b[0m                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[31m \u001b[0m1564 \u001b[2m      \u001b[0moutputs = \u001b[96mself\u001b[0m.bert(                                                              \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1565 \u001b[0m\u001b[2m         \u001b[0minput_ids,                                                                    \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1566 \u001b[0m\u001b[2m         \u001b[0mattention_mask=attention_mask,                                                \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1567 \u001b[0m\u001b[2m         \u001b[0mtoken_type_ids=token_type_ids,                                                \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[33mc:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module\u001b[0m \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[33m.py\u001b[0m:\u001b[94m1511\u001b[0m in \u001b[92m_wrapped_call_impl\u001b[0m                                                                   \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1508 \u001b[0m\u001b[2m      \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m._compiled_call_impl \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m:                                          \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1509 \u001b[0m\u001b[2m         \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m._compiled_call_impl(*args, **kwargs)  \u001b[2m# type: ignore[misc]\u001b[0m        \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1510 \u001b[0m\u001b[2m      \u001b[0m\u001b[94melse\u001b[0m:                                                                             \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[31m \u001b[0m1511 \u001b[2m         \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m._call_impl(*args, **kwargs)                                       \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1512 \u001b[0m\u001b[2m   \u001b[0m                                                                                      \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1513 \u001b[0m\u001b[2m   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92m_call_impl\u001b[0m(\u001b[96mself\u001b[0m, *args, **kwargs):                                                \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1514 \u001b[0m\u001b[2m      \u001b[0mforward_call = (\u001b[96mself\u001b[0m._slow_forward \u001b[94mif\u001b[0m torch._C._get_tracing_state() \u001b[94melse\u001b[0m \u001b[96mself\u001b[0m.fo  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[33mc:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module\u001b[0m \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[33m.py\u001b[0m:\u001b[94m1520\u001b[0m in \u001b[92m_call_impl\u001b[0m                                                                           \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1517 \u001b[0m\u001b[2m      \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._backward_pre_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks   \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1518 \u001b[0m\u001b[2m            \u001b[0m\u001b[95mor\u001b[0m _global_backward_pre_hooks \u001b[95mor\u001b[0m _global_backward_hooks                   \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1519 \u001b[0m\u001b[2m            \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[31m \u001b[0m1520 \u001b[2m         \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*args, **kwargs)                                          \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1521 \u001b[0m\u001b[2m      \u001b[0m                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1522 \u001b[0m\u001b[2m      \u001b[0m\u001b[94mtry\u001b[0m:                                                                              \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1523 \u001b[0m\u001b[2m         \u001b[0mresult = \u001b[94mNone\u001b[0m                                                                 \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[33mc:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\ber\u001b[0m \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[33mt\\modeling_bert.py\u001b[0m:\u001b[94m1013\u001b[0m in \u001b[92mforward\u001b[0m                                                               \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1010 \u001b[0m\u001b[2m         \u001b[0minputs_embeds=inputs_embeds,                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1011 \u001b[0m\u001b[2m         \u001b[0mpast_key_values_length=past_key_values_length,                                \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1012 \u001b[0m\u001b[2m      \u001b[0m)                                                                                 \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[31m \u001b[0m1013 \u001b[2m      \u001b[0mencoder_outputs = \u001b[96mself\u001b[0m.encoder(                                                   \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1014 \u001b[0m\u001b[2m         \u001b[0membedding_output,                                                             \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1015 \u001b[0m\u001b[2m         \u001b[0mattention_mask=extended_attention_mask,                                       \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1016 \u001b[0m\u001b[2m         \u001b[0mhead_mask=head_mask,                                                          \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[33mc:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module\u001b[0m \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[33m.py\u001b[0m:\u001b[94m1511\u001b[0m in \u001b[92m_wrapped_call_impl\u001b[0m                                                                   \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1508 \u001b[0m\u001b[2m      \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m._compiled_call_impl \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m:                                          \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1509 \u001b[0m\u001b[2m         \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m._compiled_call_impl(*args, **kwargs)  \u001b[2m# type: ignore[misc]\u001b[0m        \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1510 \u001b[0m\u001b[2m      \u001b[0m\u001b[94melse\u001b[0m:                                                                             \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[31m \u001b[0m1511 \u001b[2m         \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m._call_impl(*args, **kwargs)                                       \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1512 \u001b[0m\u001b[2m   \u001b[0m                                                                                      \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1513 \u001b[0m\u001b[2m   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92m_call_impl\u001b[0m(\u001b[96mself\u001b[0m, *args, **kwargs):                                                \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1514 \u001b[0m\u001b[2m      \u001b[0mforward_call = (\u001b[96mself\u001b[0m._slow_forward \u001b[94mif\u001b[0m torch._C._get_tracing_state() \u001b[94melse\u001b[0m \u001b[96mself\u001b[0m.fo  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[33mc:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module\u001b[0m \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[33m.py\u001b[0m:\u001b[94m1520\u001b[0m in \u001b[92m_call_impl\u001b[0m                                                                           \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1517 \u001b[0m\u001b[2m      \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._backward_pre_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks   \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1518 \u001b[0m\u001b[2m            \u001b[0m\u001b[95mor\u001b[0m _global_backward_pre_hooks \u001b[95mor\u001b[0m _global_backward_hooks                   \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1519 \u001b[0m\u001b[2m            \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[31m \u001b[0m1520 \u001b[2m         \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*args, **kwargs)                                          \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1521 \u001b[0m\u001b[2m      \u001b[0m                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1522 \u001b[0m\u001b[2m      \u001b[0m\u001b[94mtry\u001b[0m:                                                                              \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1523 \u001b[0m\u001b[2m         \u001b[0mresult = \u001b[94mNone\u001b[0m                                                                 \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[33mc:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\ber\u001b[0m \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[33mt\\modeling_bert.py\u001b[0m:\u001b[94m607\u001b[0m in \u001b[92mforward\u001b[0m                                                                \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 604 \u001b[0m\u001b[2m               \u001b[0moutput_attentions,                                                    \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 605 \u001b[0m\u001b[2m            \u001b[0m)                                                                         \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 606 \u001b[0m\u001b[2m         \u001b[0m\u001b[94melse\u001b[0m:                                                                         \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[31m \u001b[0m 607 \u001b[2m            \u001b[0mlayer_outputs = layer_module(                                             \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 608 \u001b[0m\u001b[2m               \u001b[0mhidden_states,                                                        \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 609 \u001b[0m\u001b[2m               \u001b[0mattention_mask,                                                       \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 610 \u001b[0m\u001b[2m               \u001b[0mlayer_head_mask,                                                      \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[33mc:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module\u001b[0m \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[33m.py\u001b[0m:\u001b[94m1511\u001b[0m in \u001b[92m_wrapped_call_impl\u001b[0m                                                                   \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1508 \u001b[0m\u001b[2m      \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m._compiled_call_impl \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m:                                          \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1509 \u001b[0m\u001b[2m         \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m._compiled_call_impl(*args, **kwargs)  \u001b[2m# type: ignore[misc]\u001b[0m        \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1510 \u001b[0m\u001b[2m      \u001b[0m\u001b[94melse\u001b[0m:                                                                             \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[31m \u001b[0m1511 \u001b[2m         \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m._call_impl(*args, **kwargs)                                       \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1512 \u001b[0m\u001b[2m   \u001b[0m                                                                                      \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1513 \u001b[0m\u001b[2m   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92m_call_impl\u001b[0m(\u001b[96mself\u001b[0m, *args, **kwargs):                                                \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1514 \u001b[0m\u001b[2m      \u001b[0mforward_call = (\u001b[96mself\u001b[0m._slow_forward \u001b[94mif\u001b[0m torch._C._get_tracing_state() \u001b[94melse\u001b[0m \u001b[96mself\u001b[0m.fo  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[33mc:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module\u001b[0m \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[33m.py\u001b[0m:\u001b[94m1520\u001b[0m in \u001b[92m_call_impl\u001b[0m                                                                           \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1517 \u001b[0m\u001b[2m      \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._backward_pre_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks   \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1518 \u001b[0m\u001b[2m            \u001b[0m\u001b[95mor\u001b[0m _global_backward_pre_hooks \u001b[95mor\u001b[0m _global_backward_hooks                   \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1519 \u001b[0m\u001b[2m            \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[31m \u001b[0m1520 \u001b[2m         \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*args, **kwargs)                                          \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1521 \u001b[0m\u001b[2m      \u001b[0m                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1522 \u001b[0m\u001b[2m      \u001b[0m\u001b[94mtry\u001b[0m:                                                                              \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1523 \u001b[0m\u001b[2m         \u001b[0mresult = \u001b[94mNone\u001b[0m                                                                 \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[33mc:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\ber\u001b[0m \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[33mt\\modeling_bert.py\u001b[0m:\u001b[94m539\u001b[0m in \u001b[92mforward\u001b[0m                                                                \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 536 \u001b[0m\u001b[2m         \u001b[0mcross_attn_present_key_value = cross_attention_outputs[-\u001b[94m1\u001b[0m]                    \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 537 \u001b[0m\u001b[2m         \u001b[0mpresent_key_value = present_key_value + cross_attn_present_key_value          \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 538 \u001b[0m\u001b[2m      \u001b[0m                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[31m \u001b[0m 539 \u001b[2m      \u001b[0mlayer_output = apply_chunking_to_forward(                                         \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 540 \u001b[0m\u001b[2m         \u001b[0m\u001b[96mself\u001b[0m.feed_forward_chunk, \u001b[96mself\u001b[0m.chunk_size_feed_forward, \u001b[96mself\u001b[0m.seq_len_dim, att  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 541 \u001b[0m\u001b[2m      \u001b[0m)                                                                                 \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 542 \u001b[0m\u001b[2m      \u001b[0moutputs = (layer_output,) + outputs                                               \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[33mc:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\pytorch_ut\u001b[0m \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[33mils.py\u001b[0m:\u001b[94m237\u001b[0m in \u001b[92mapply_chunking_to_forward\u001b[0m                                                          \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m234 \u001b[0m\u001b[2m      \u001b[0m\u001b[2m# concatenate output at same dimension\u001b[0m                                             \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m235 \u001b[0m\u001b[2m      \u001b[0m\u001b[94mreturn\u001b[0m torch.cat(output_chunks, dim=chunk_dim)                                     \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m236 \u001b[0m\u001b[2m   \u001b[0m                                                                                       \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[31m \u001b[0m237 \u001b[2m   \u001b[0m\u001b[94mreturn\u001b[0m forward_fn(*input_tensors)                                                      \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m238 \u001b[0m                                                                                           \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m239 \u001b[0m                                                                                           \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m240 \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mfind_pruneable_heads_and_indices\u001b[0m(                                                      \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[33mc:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\ber\u001b[0m \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[33mt\\modeling_bert.py\u001b[0m:\u001b[94m552\u001b[0m in \u001b[92mfeed_forward_chunk\u001b[0m                                                     \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 549 \u001b[0m\u001b[2m   \u001b[0m                                                                                      \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 550 \u001b[0m\u001b[2m   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mfeed_forward_chunk\u001b[0m(\u001b[96mself\u001b[0m, attention_output):                                       \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 551 \u001b[0m\u001b[2m      \u001b[0mintermediate_output = \u001b[96mself\u001b[0m.intermediate(attention_output)                         \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[31m \u001b[0m 552 \u001b[2m      \u001b[0mlayer_output = \u001b[96mself\u001b[0m.output(intermediate_output, attention_output)                 \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 553 \u001b[0m\u001b[2m      \u001b[0m\u001b[94mreturn\u001b[0m layer_output                                                               \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 554 \u001b[0m                                                                                          \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 555 \u001b[0m                                                                                          \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[33mc:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module\u001b[0m \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[33m.py\u001b[0m:\u001b[94m1511\u001b[0m in \u001b[92m_wrapped_call_impl\u001b[0m                                                                   \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1508 \u001b[0m\u001b[2m      \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m._compiled_call_impl \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m:                                          \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1509 \u001b[0m\u001b[2m         \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m._compiled_call_impl(*args, **kwargs)  \u001b[2m# type: ignore[misc]\u001b[0m        \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1510 \u001b[0m\u001b[2m      \u001b[0m\u001b[94melse\u001b[0m:                                                                             \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[31m \u001b[0m1511 \u001b[2m         \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m._call_impl(*args, **kwargs)                                       \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1512 \u001b[0m\u001b[2m   \u001b[0m                                                                                      \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1513 \u001b[0m\u001b[2m   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92m_call_impl\u001b[0m(\u001b[96mself\u001b[0m, *args, **kwargs):                                                \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1514 \u001b[0m\u001b[2m      \u001b[0mforward_call = (\u001b[96mself\u001b[0m._slow_forward \u001b[94mif\u001b[0m torch._C._get_tracing_state() \u001b[94melse\u001b[0m \u001b[96mself\u001b[0m.fo  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[33mc:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module\u001b[0m \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[33m.py\u001b[0m:\u001b[94m1520\u001b[0m in \u001b[92m_call_impl\u001b[0m                                                                           \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1517 \u001b[0m\u001b[2m      \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._backward_pre_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks   \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1518 \u001b[0m\u001b[2m            \u001b[0m\u001b[95mor\u001b[0m _global_backward_pre_hooks \u001b[95mor\u001b[0m _global_backward_hooks                   \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1519 \u001b[0m\u001b[2m            \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[31m \u001b[0m1520 \u001b[2m         \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*args, **kwargs)                                          \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1521 \u001b[0m\u001b[2m      \u001b[0m                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1522 \u001b[0m\u001b[2m      \u001b[0m\u001b[94mtry\u001b[0m:                                                                              \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1523 \u001b[0m\u001b[2m         \u001b[0mresult = \u001b[94mNone\u001b[0m                                                                 \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[33mc:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\ber\u001b[0m \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[33mt\\modeling_bert.py\u001b[0m:\u001b[94m464\u001b[0m in \u001b[92mforward\u001b[0m                                                                \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 461 \u001b[0m\u001b[2m      \u001b[0m\u001b[96mself\u001b[0m.dropout = nn.Dropout(config.hidden_dropout_prob)                             \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 462 \u001b[0m\u001b[2m   \u001b[0m                                                                                      \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 463 \u001b[0m\u001b[2m   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mforward\u001b[0m(\u001b[96mself\u001b[0m, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[31m \u001b[0m 464 \u001b[2m      \u001b[0mhidden_states = \u001b[96mself\u001b[0m.dense(hidden_states)                                         \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 465 \u001b[0m\u001b[2m      \u001b[0mhidden_states = \u001b[96mself\u001b[0m.dropout(hidden_states)                                       \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 466 \u001b[0m\u001b[2m      \u001b[0mhidden_states = \u001b[96mself\u001b[0m.LayerNorm(hidden_states + input_tensor)                      \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 467 \u001b[0m\u001b[2m      \u001b[0m\u001b[94mreturn\u001b[0m hidden_states                                                              \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[33mc:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module\u001b[0m \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[33m.py\u001b[0m:\u001b[94m1511\u001b[0m in \u001b[92m_wrapped_call_impl\u001b[0m                                                                   \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1508 \u001b[0m\u001b[2m      \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m._compiled_call_impl \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m:                                          \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1509 \u001b[0m\u001b[2m         \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m._compiled_call_impl(*args, **kwargs)  \u001b[2m# type: ignore[misc]\u001b[0m        \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1510 \u001b[0m\u001b[2m      \u001b[0m\u001b[94melse\u001b[0m:                                                                             \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[31m \u001b[0m1511 \u001b[2m         \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m._call_impl(*args, **kwargs)                                       \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1512 \u001b[0m\u001b[2m   \u001b[0m                                                                                      \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1513 \u001b[0m\u001b[2m   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92m_call_impl\u001b[0m(\u001b[96mself\u001b[0m, *args, **kwargs):                                                \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1514 \u001b[0m\u001b[2m      \u001b[0mforward_call = (\u001b[96mself\u001b[0m._slow_forward \u001b[94mif\u001b[0m torch._C._get_tracing_state() \u001b[94melse\u001b[0m \u001b[96mself\u001b[0m.fo  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[33mc:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module\u001b[0m \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[33m.py\u001b[0m:\u001b[94m1520\u001b[0m in \u001b[92m_call_impl\u001b[0m                                                                           \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1517 \u001b[0m\u001b[2m      \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._backward_pre_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks   \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1518 \u001b[0m\u001b[2m            \u001b[0m\u001b[95mor\u001b[0m _global_backward_pre_hooks \u001b[95mor\u001b[0m _global_backward_hooks                   \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1519 \u001b[0m\u001b[2m            \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[31m \u001b[0m1520 \u001b[2m         \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*args, **kwargs)                                          \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1521 \u001b[0m\u001b[2m      \u001b[0m                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1522 \u001b[0m\u001b[2m      \u001b[0m\u001b[94mtry\u001b[0m:                                                                              \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1523 \u001b[0m\u001b[2m         \u001b[0mresult = \u001b[94mNone\u001b[0m                                                                 \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[33mc:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\linear\u001b[0m \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[33m.py\u001b[0m:\u001b[94m116\u001b[0m in \u001b[92mforward\u001b[0m                                                                               \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m113 \u001b[0m\u001b[2m         \u001b[0minit.uniform_(\u001b[96mself\u001b[0m.bias, -bound, bound)                                        \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m114 \u001b[0m\u001b[2m   \u001b[0m                                                                                       \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m115 \u001b[0m\u001b[2m   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mforward\u001b[0m(\u001b[96mself\u001b[0m, \u001b[96minput\u001b[0m: Tensor) -> Tensor:                                            \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[31m \u001b[0m116 \u001b[2m      \u001b[0m\u001b[94mreturn\u001b[0m F.linear(\u001b[96minput\u001b[0m, \u001b[96mself\u001b[0m.weight, \u001b[96mself\u001b[0m.bias)                                     \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m117 \u001b[0m\u001b[2m   \u001b[0m                                                                                       \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m118 \u001b[0m\u001b[2m   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mextra_repr\u001b[0m(\u001b[96mself\u001b[0m) -> \u001b[96mstr\u001b[0m:                                                           \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m119 \u001b[0m\u001b[2m      \u001b[0m\u001b[94mreturn\u001b[0m \u001b[33mf\u001b[0m\u001b[33m'\u001b[0m\u001b[33min_features=\u001b[0m\u001b[33m{\u001b[0m\u001b[96mself\u001b[0m.in_features\u001b[33m}\u001b[0m\u001b[33m, out_features=\u001b[0m\u001b[33m{\u001b[0m\u001b[96mself\u001b[0m.out_features\u001b[33m}\u001b[0m\u001b[33m, bias=\u001b[0m   \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m\n",
       "\u001b[1;91mKeyboardInterrupt\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "for epoch in tqdm(range(1, epochs+1)):\n",
    "\n",
    "    #Set model in train mode\n",
    "    model.train()\n",
    "\n",
    "    #Tracking variable\n",
    "    loss_train_total = 0\n",
    "    \n",
    "    #Set up progress bar\n",
    "    progress_bar = tqdm(dataloader_train, \n",
    "                        desc='Epoch {:1d}'.format(epoch), \n",
    "                        leave=False, \n",
    "                        disable=False)\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        #Set gradient to 0\n",
    "        model.zero_grad()\n",
    "\n",
    "        #load into GPU\n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "\n",
    "        #define inputs\n",
    "        inputs = {'input_ids': batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                  'labels': batch[2]}\n",
    "        \n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs[0] #output.loss\n",
    "        loss_train_total +=loss.item()\n",
    "\n",
    "        #Backward pass to get gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        #Clip the norm of the gradients to 1.0 to prevent exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        \n",
    "        #Update optimizer\n",
    "        optimizer.step()\n",
    "\n",
    "        #Update scheduler\n",
    "        scheduler.step()\n",
    "        \n",
    "        progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(batch))})     \n",
    "    \n",
    "    tqdm.write('\\nEpoch {epoch}')\n",
    "    \n",
    "    #Print training result\n",
    "    loss_train_avg = loss_train_total/len(dataloader_train)\n",
    "    tqdm.write(f'Training loss: {loss_train_avg}')\n",
    "    \n",
    "    #Evaluate\n",
    "    val_loss, predictions, true_vals = evaluate(dataloader_val)\n",
    "    #F1 score\n",
    "    val_f1 = f1_score_func(predictions, true_vals)\n",
    "    tqdm.write(f'Validation loss: {val_loss}')\n",
    "    tqdm.write(f'F1 Score (weighted): {val_f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0096, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.0184,  3.8596, -1.9231],\n",
       "        [-1.0758, -2.0951,  3.5465]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2/2 [00:17<00:00,  8.58s/it]\n"
     ]
    }
   ],
   "source": [
    "#Evaluate\n",
    "_, predictions, true_vals = evaluate(dataloader_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\"> </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> </span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1 #Get accuracy score</span>                                                                          <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span>2 accuracy_per_class(predictions, true_vals)                                                   <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3 </span>                                                                                             <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">accuracy_per_class</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">3</span>                                                                          <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 #Accuracy score</span>                                                                             <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">accuracy_per_class</span>(preds, labels):                                                      <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span> 3 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">   </span>label_dict_inverse = {v: k <span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> k, v <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> label_dict.items()}                              <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 4 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">   </span>                                                                                        <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 5 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">#Make prediction</span>                                                                        <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 6 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">   </span>preds_flat = np.argmax(preds, axis=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>).flatten()                                         <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">NameError: </span>name <span style=\"color: #008000; text-decoration-color: #008000\">'label_dict'</span> is not defined\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m\u001b[0m\u001b[31m\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m\u001b[0m\u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m2\u001b[0m                                                                                    \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1 \u001b[0m\u001b[2m#Get accuracy score\u001b[0m                                                                          \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[31m \u001b[0m2 accuracy_per_class(predictions, true_vals)                                                   \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m3 \u001b[0m                                                                                             \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m in \u001b[92maccuracy_per_class\u001b[0m:\u001b[94m3\u001b[0m                                                                          \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 1 \u001b[0m\u001b[2m#Accuracy score\u001b[0m                                                                             \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 2 \u001b[0m\u001b[94mdef\u001b[0m \u001b[92maccuracy_per_class\u001b[0m(preds, labels):                                                      \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[31m \u001b[0m 3 \u001b[2m   \u001b[0mlabel_dict_inverse = {v: k \u001b[94mfor\u001b[0m k, v \u001b[95min\u001b[0m label_dict.items()}                              \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 4 \u001b[0m\u001b[2m   \u001b[0m                                                                                        \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 5 \u001b[0m\u001b[2m   \u001b[0m\u001b[2m#Make prediction\u001b[0m                                                                        \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 6 \u001b[0m\u001b[2m   \u001b[0mpreds_flat = np.argmax(preds, axis=\u001b[94m1\u001b[0m).flatten()                                         \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m\n",
       "\u001b[1;91mNameError: \u001b[0mname \u001b[32m'label_dict'\u001b[0m is not defined\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Get accuracy score\n",
    "accuracy_per_class(predictions, true_vals)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "5c1f741a4f83aa020b4b2a4d7353a073a4e5e4a855a3258a20da40294ddbf005"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
